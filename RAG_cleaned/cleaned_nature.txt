cs CV Jul Adaptation Multi modal Representation Models Multi task Surgical Computer Vision Soham Walimbe Britty Vinkle Nicolas University Strasbourg CNRS INSERM ICube Strasbourg France Institute Image Guided Surgery IHU Strasbourg Strasbourg France Abstract Surgical AI involves multiple tasks single cedure like phase recognition assessing Critical View Safety laparoscopic cholecystectomy Traditional models built task time lack flexibility requiring separate model dress introduce MML SurgAdapt unified multi task work Vision Language Models VLMs specifically CLIP handle diverse surgical tasks natural language supervision key lenge multi task learning presence partial annotations integrating different tasks overcome employ Single Positive Multi Label SPML learning
traditionally reduces annotation burden training models positive label instance framework extends approach integrate data multiple cal tasks single procedure enabling effective learning despite incomplete noisy annotations demonstrate effectiveness model combined dataset consisting utilizing custom prompts Extensive evaluation shows MML SurgAdapt performs comparably task specific benchmarks added advantage handling noisy annotations forms existing SPML frameworks task reducing quired labels approach proposes scalable efficient labeling process significantly easing annotation burden clinicians knowledge application SPML integrate data multiple surgical tasks presenting novel generalizable tion multi task learning surgical computer vision Implementation available Keywords Vision Language Models Single Positive Multi Label agnostic models Multi
Task Learning Surgical Data science Introduction AI models capable handling diverse downstream tasks gained increasing attention computer vision research Traditional task specific models designed author contributed internship hosting laboratory Corresponding author Walimbe et al single tasks like multi class classification lack versatility require separate architectures task making non scalable resource intensive limitation spurred interest scalable task agnostic models capable generalizing multiple tasks including unseen classes Vision Language Models VLMs like CLIP uses natural language supervision align visual textual representations shared semantic space Trained vast text datasets VLMs excel general tasks object scene tion textual prompts adapted techniques like prompt learning
network adaptation VLMs thrive everyday vision tasks application surgical data science limited Surgical AI involves analyzing complex video data procedures tasks ranging phase recognition tion domain specific challenges like Critical View Safety CVS assessment action triplet identification forceps grasp gallbladder Traditionally datasets curated specific tasks leading specialized models address phase recognition CVS prediction action triplet recognition independently task specific approach effective isolation results fragmented solutions struggle scale integrate complementary tasks inherent surgical procedure address introduce MML SurgAdapt work adapts pretrained VLM CLIP natural language sion pairing surgical images discriminative text multiple tasks create unified task agnostic model merging enable cohesive
approach tackles interconnected tasks responding need scalable integrated solutions gical AI reflect procedure holistic nature key difficulty kind multi task learning handling partial tations dataset labels target task leaving unlabeled creates multi label problem potential false negatives address turn Single Positive Multi Label learning SPML traditionally designed streamline annotation multi label classification settings SPML image assigned positive label potential labels initially assumed negative introduce false negatives mitigate false negatives Hill loss weights negatives outperforms SPML losses experiments SPML based approach enables multi task integration plete data enhance resilience noisy inconsistent labels ingly implementation SPML reduced number required labels reduction particularly
significant surgical tasks tion complexity varies widely labels instance tasks like CVS ment typically depend time intensive clinical reviews costly resource heavy streamlining annotation demands framework enhances practicality multi task learning real world surgical applications contributions include MML SurgAdapt unified task agnostic based model multi task surgical analysis SPML adaptation cuts MML SurgAdapt HQFRGHU PDJH HQFRGHU SN N FUHDWLRQ Fig Proposed model architecture MML SurgAdapt labeling needs enhancing robustness addressing false negatives evaluation loss functions benchmarks task specific models scalable framework surgical broader multimodal applications Methodology address multi task learning surgical domain use popular Language Model VLM CLIP multi label
classification setup convert task specific labels textual prompts calculate cosine ity image embedding label corresponding text embedding similarities converted probabilities sigmoid function obtain label predictions validate approach laparoscopic cholecystectomy procedure focusing interrelated tasks Phase Recognition Critical View Safety CVS Assessment Action Triplet Recognition Prompt label creation create Partial Positive setup combining datasets ing partial annotations specific tasks phase CVS triplet setup image contains annotations respective task labels tasks unobserved set negative default resulting false negatives ground truth contains noise false negatives tasks combined creating union phase labels CVS labels triplet Walimbe et al labels resulting labels treated multi label classification
problem missing unknown data creation SPML setup randomly select positive label image set rest zero setup encourages model generalize despite incomplete supervision learning meaningful representations minimal annotations image prompt creation include task label information form simple prompt general prompt look like photo text TASK LABEL Examples include Phase photo phase Calot Triangle Dissection CVS photo lower gallbladder divided liver bed expose cystic plate Triplet photo tool grasper performing action grasp target gallbladder Model architecture model MML SurgAdapt makes use CLIP framework image text encoders additional components adapt multi task SPML classification setup CLIP image encoder fi text encoder ft tialized
pre trained weights extract feature representations input image text prompts respectively given image X pre defined set K text prompts encoders generate following representations Tf image feature vector Tf set text feature vectors K labels K text prompts mains fixed use label image backpropagation training aligning specific requirements SPML setup addition capture inter label relationships apply Graph tion Network GCN text features Tf GCN module refine text features utilizing label dependency graph represent form adjacency matrix K total number labels Following create matrix similarities text encoder embeddings label prompts sparsify remove noise retaining K connections Finally address potential smoothing introduce
scaling factor adjust weights assigned neighboring label nodes Let Tf input GCN GCN propagates label information follows ρ learnable weights l th layer ρ non linear activation function L layers final text embeddings Tg computed residual connection added combining GCN output original text embeddings Tfinal Tg Tf MML SurgAdapt multi label classification similarity image text embedding calculated cosine similarity sk sk similarity score k th label probabilities label computed pk σ sk τ sigmoid activation function τ temperature parameter Finally appropriate loss function L applied optimize model loss computed ground truth labels yik predicted probabilities pik corresponds instance dataset architecture
proposed model shown Fig evaluate framework different SPML loss functions capable handling single positive ground truth WAN SPLC Hill Loss outperformed loss tasks adapted final framework Hill Loss designed insensitive missing labels ing loss negative labels assigning weight predictions closer likely indicate false negatives positive labels semi hard mining strategy utilized modified Mean Squared Error MSE loss applied negatives offering better robustness binary cross entropy naturally downweighting high prediction samples loss functions positive ative labels Hill loss follows pkm σ τ m logit margin Hill Hill k λ hyperparameter reweighting Experimental Setup Dataset splits task Phase recognition CVS assessment tion
Triplet Recognition use publicly available datasets respectively combine datasets multi task evaluation SPML setup detailed description datasets including number images training validation testing provided Tab ensure fair evaluation enforce mutual exclusivity test splits datasets Specifically overlap training testing images datasets like prevent data leakage overlapping videos removed dataset result final split consists videos training validation testing dataset split performed according dataset uses Rendezvous split Walimbe et al Table Overview datasets tasks evaluation criteria T V t represents Train Val Test V T triplet recognition represent instrument verb target respectively Dataset Task Videos T V t Images T V t Metrics
Phase recognition score CVS assessment AP mAP Triplet recognition mAPs V T IV IVT Evaluation Criteria testing evaluate images labels correspond dataset task images originate task evaluated separately ensuring appropriate labels considered assessment Phase recognition score puted according CVS assessment average precision class mean average precision mAP classes evaluated frame level Action Triplet recognition average precision computed configurations V T IV IVT described Implementation details use ViT based pretrained CLIP model experiments train models epochs batch size Optimization Adam learning rate experiments conducted single Nvidia GPU Input images resized normalized best model saved based validation criteria highest mAP lowest loss
partial positive validation ground truths lowest loss single positive validation ground truths single positive labels fixed dataset initialization set hyperparameter reweighting λ Hill loss equal Results Discussions experimental results MML SurgAdapt model presented setups compared multiple baselines Comparision Architectures Vision Models dataset benchmark CVS assessment model considers CVS labels use bounding boxes segmentation masks dataset referenced Rendezvous model uses aligned data splits trained subset dataset avoid overlap direct comparisons state art SOTA methods possible Task specific Models compared multi task model task specific models trained separately labels dataset cross entropy loss baselines include CLIP variant multi task model uses feature
extractor separate MML SurgAdapt Table Comparison baselines Phase CVS Assessment Action Triplet Recognition TS stands Task specific Model Phase CVS Assessment Action Triplet Recognition mAP AP AP V AP T AP IV AP AP IVT Vision Models Rendezvous Task Specific Models TS CLIPViT TS Multi task SPML Models DualCoOp VLPL HSPNet MML SurgAdapt classification heads Despite training fewer annotations model formed comparable task specific models established benchmarks SPML Models evaluated MML SurgAdapt model SPML setup image positive label fixed initialization negative benchmarked MML SurgAdapt state art SPML frameworks including DualCoOp HSPNet use default loss functions specified original papers model MML
SurgAdapt outperforms existing SPML frameworks multi task integration showing robustness model summary comparison baselines provided Table Overall MML SurgAdapt achieves competitive performance significantly reducing labels compared task specific models vision baselines Loss Function Analysis conducted studies different loss functions training setups tested WAN SPLC Hill Loss SPML setup conducting runs experiment report standard deviations extended evaluation Partial Positive setup labels partially annotated given dataset ground truth contains false negatives known labels marked positive Results Single Partial Positive setups Phase Recognition CVS Assessment Action Triplet Recognition shown Table Notably WAN slightly outperformed phase triplet tion underperformed complex CVS assessment task indicating
effectiveness simpler tasks limitations handling complexity formance inconsistencies observed SPLC stem reliance pseudolabels generated initial predictions curate surgical domain divergence CLIP pretraining data Walimbe et al Table Combined Performance comparison objective functions SPML tial Positive setups Phase CVS Assessment Action Triplet Recognition Loss Phase CVS Action Triplet Recognition mAP AP AP V AP T AP IV AP AP IVT SPML Setup WAN SPLC Hill Partial Positive Setup WAN SPLC Hill Findings indicate loss functions performed comparably tasks Hill Loss emerging best performer particularly CVS ment Partial Positive setup additional positive labels included observed slight performance gain suggesting model maintains strong
performance extreme positive annotations tatively Single Positive setup reduced labels compared Partial Positive scenario maintaining competitive performance Limitations Future Work ablation study removing Graph Convolutional Network GCN showed performance improvement included model quires end end fine tuning use pretrained weights limits performance improvement GCN Differences arise changes initialization work CLIP weights domain specific trained model particularly trained surgical data enable fective adaptation GCN fine tuning basic initial tests conducted exploring remains future work current setup explicitly address class imbalance inary experiments class weighting yielded inconclusive results omitted targeted strategies handle imbalance especially rare triplets remain explored Lastly evaluation limited single
cedure validating framework broader range surgeries important direction future work Conclusion work addressed limitations traditional approaches Surgical AI rely task specific datasets models lack scalability struggle integrate diverse tasks introduce MML SurgAdapt unified MML SurgAdapt task agnostic framework uses natural language supervision align surgical images text prompts multiple tasks incorporating Single Positive Multi Label SPML learning surgical domain tackle challenges combining datasets interrelated tasks mitigating false negatives abling label efficient learning partial annotations extensive periments compare model existing task specific baselines SPML frameworks analyzing impact different loss functions handling false negatives findings demonstrate single AI model generalize diverse complementary surgical
tasks procedure paving way future advancements surgical AI annotation efficient learning Acknowledgments work received funding European Union ERC CompSURG Views opinions expressed thors necessarily reflect European Union European Research Council European Union granting authority held responsible work partially supported French state funds managed ANR Grant authors like knowledge High Performance Computing Center University Strasbourg supporting work providing scientific support access computing sources computing resources funded Equipex project Programme Investissements CPER Alsacalcul Big Data Disclosure Interests authors competing interests declare relevant article References Alapatt Murali Srivastav Consortium Mascagni Padoy Jumpstarting surgical computer vision International Conference Medical Image Computing Computer Assisted Intervention
pp Springer Cole Mac Aodha Lorieul Perona Morris Jojic Multi label learning single positive labels Proceedings IEEE CVF Conference Computer Vision Pattern Recognition pp Zhang Ren Sun Deep residual learning image recognition Proceedings IEEE conference computer vision pattern recognition pp Kipf Welling Semi supervised classification graph convolutional networks arXiv preprint Murali Alapatt Mascagni Vardazaryan Garcia Okamoto Costamagna Mutter Marescaux Dallemagne et al endoscapes dataset surgical scene segmentation object detection critical view safety assessment official splits benchmark arXiv preprint Murali Alapatt Mascagni Vardazaryan Garcia Okamoto Mutter Padoy Encoding surgical videos latent spatiotemporal graphs object anatomy driven reasoning International Conference cal Image
Computing Computer Assisted Intervention pp Springer Walimbe et al Nwoye Yu Gonzalez Seeliger Mascagni Mutter Marescaux Padoy Rendezvous Attention mechanisms recognition surgical action triplets endoscopic videos Medical Image Analysis Radford Kim Hallacy Ramesh Goh Agarwal Sastry Askell Mishkin Clark et al Learning transferable visual models natural language supervision International conference machine learning pp PMLR Ramesh Gonzalez Yu Mascagni Mutter Marescaux Fiorini Padoy Multi task temporal convolutional networks joint recognition surgical phases steps gastric bypass procedures International journal computer assisted radiology surgery Sun Hu Saenko Dualcoop Fast adaptation multi label recognition limited annotations Advances Neural Information Processing Systems Twinanda Shehata Mutter
Marescaux De Mathelin Padoy Endonet deep architecture recognition tasks laparoscopic videos IEEE transactions medical imaging Wang Chen Lin Ding Liu Bao Yan Ding chical prompt learning clip multi label classification single positive labels Proceedings ACM International Conference Multimedia pp Xing Xiong Stylianou Sastry Gong Jacobs language pseudo labels single positive multi label learning Proceedings IEEE CVF Conference Computer Vision Pattern Recognition pp Yang Zhang Wang Xie Mma Multi modal adapter language models Proceedings IEEE CVF Conference Computer sion Pattern Recognition pp Yuan Srivastav Navab Padoy Hecvl Hierarchical video language pretraining zero shot surgical phase recognition International Conference Medical Image Computing
Computer Assisted Intervention pp Springer Zhang Cheng Huang Wen Feng Li Guo Simple robust loss design multi label learning missing labels arXiv preprint Zhou Yang Loy Liu Learning prompt vision language models International Journal Computer Vision
cs CL Jul Evaluating Memory LLM Agents Incremental Multi Turn Interactions Yuanzhe Yu Julian San Diego Datasets Source Code Abstract Recent benchmarks Large Language Model LLM agents primarily focus evaluating reasoning planning execution capabilities critical component memory encompassing agents memorize update retrieve long term information evaluated lack benchmarks term agents memory mechanisms memory agents paper identify core competencies essential memory agents accurate retrieval test time learning long range understanding conflict resolution Existing datasets rely limited context lengths tailored static context settings like book based QA reflect interactive multi turn nature memory agents incrementally accumulate information Furthermore existing benchmarks cover competencies
introduce MemoryAgentBench new benchmark specifically designed memory agents benchmark combines reformulated existing datasets newly constructed ones covering memory competencies providing systematic challenging testbed assessing memory quality evaluate diverse set memory agents ranging simple context based retrieval augmented generation RAG systems advanced agents external memory modules tool integration Empirical results reveal current methods fall short mastering competencies underscoring need research comprehensive memory mechanisms LLM agents Introduction Large Language Model LLM agents rapidly transitioned proof concept chatbots end end systems write software control browsers reason multi modal inputs Frameworks MANUS OWL OPENHANDS CODEX routinely solve complex tool rich tasks achieve state art
results agentic benchmarks like GAIA SWE Bench evaluations focus exclusively reasoning planning tool code synthesis leave equally important question memorization abstraction storing updating retrieving largely explored Recent memory centric architectures ranging parametric memory systems like ryLLM SELF PARAM commercial token level memory solutions COGNEE diverse strategies storing retrieving past information Despite growing interest real world effectiveness remains largely anecdotal currently unified benchmark systematically evaluating quality memory agents paper refer agents equipped memory mechanisms Memory Agents memory forms including parameters vectors textual histories external databases paper primarily focus memory agents utilize textual histories Hu Wang contribute equally Preprint review Figure complementary
competencies memory agents external databases approaches commonly deployed real world applications contrast memory encoded model parameters remains largely academic research typically capable proprietary memory systems equipped closed sourced API models evaluate memory agents identify complementary competencies Examples shown Figure Accurate Retrieval ability extract correct snippet response query involve hop multi hop retrieval long relevant information accessed single query Test Time Learning capacity incorporate new behaviors acquire new skills deployment additional training Long Range Understanding ability integrate information distributed extended contexts tokens answer questions requiring global understanding entire sequence Conflict Resolution skill revise overwrite remove previously stored information faced contradictory evidence
aligning goals model editing knowledge unlearning tasks Previous datasets developed evaluate memory language models notable limitations Early benchmarks LOCOMO tokens tokens tokens feature relatively short contexts longer challenge current models recent datasets like tokens tokens tokens tokens extend context length evaluate global reasoning retrieval capabilities datasets primarily designed evaluating long context language models memory agents reason long context benchmarks directly evaluate memory agents follows fundamental distinction memory long context memory serves compressed distilled representation past information storing historical content verbatim memory selectively extracts salient details removes irrelevant information incorporates new inferences derived prior experiences Consequently memory agents designed process context
incrementally absorbing input piece piece abstracting consolidating information time generating new inferences learning novel rules accumulated history reason datasets provide entire context single block directly applicable evaluating memory agents recent effort LONGMEMEVAL seeks address limitation synthetic long form conversations injected memory gradually session session Nonetheless evaluation framework remains constrained limited topical diversity realistic interaction patterns reducing applicability real world memory agent scenarios address limitations introduce unified benchmark framework MemoryAgentBench specifically designed evaluate broad spectrum memory mechanisms agent systems provide framework memory agent evaluation framework agents presented sequences textual inputs simulate multi turn interactions users repurpose existing datasets originally developed long
context LLM evaluation segmenting inputs multiple chunks feeding incrementally agent datasets fully capture targeted memory competencies introduce new datasets EventQA FactConsolidation designed evaluate accurate retrieval conflict resolution respectively benchmark includes evaluations state art commercial memory agents MemGPT long context agents treat input memory RAG agents extend memory retrieval methods examine techniques developed long context models RAG transfer memory agent setting commercial memory agents perform challenging competency specific tests providing consistent evaluation protocol diverse agent architectures datasets MemoryAgentBench delivers comprehensive insights agent performance core memory competencies contributions summarized follows Datasets structure existing datasets create new datasets construct sive benchmark covering distinct
memory competencies Framework provide unified evaluation framework open source codebase datasets encourage reproducibility research Empirical Study implement simple agents diverse memory mechanisms adopt commercial agents evaluate agents proposed benchmark results existing memory agents effective tasks face significant challenges aspects Related Work Benchmarks Long Input section review prior work long context benchmarks Early benchmarks designed long context evaluation include average input lengths approximately tokens respectively recent benchmarks NoLiMa LongBench context lengths tokens primarily intended evaluate capabilities long context models despite scale benchmarks designed assess memory agents prior work repurposed goal recently LOCOMO MemEval proposed specifically evaluating memory agents promising LOCOMO features
relatively short conversations LongMemEval uses synthetic conversations limited topical diversity making dialogues realistic potentially representative real world memory use cases Agents Memory Mechanisms Memory mechanisms attracting attention lately Recent advancements LLMs demonstrated capability process extended context lengths ranging K million tokens instance models Claude handle inputs approximately K K tokens models like Gemini Pro series extend capacity million tokens strong long context capabilities enable simple effective form memory storing information directly context window approach inherently constrained hard limit context window exceeded earlier information discarded parallel RAG continues serve dominant paradigm managing excessive context retrieving relevant information earlier context feeding LLM
RAG allows systems overcome context length limitations example OpenAI recent memory combines explicit user preference tracking retrieval based methods reference prior interactions RAG methods broadly classified categories Simple RAG methods rely matching techniques TF IDF BMX entirely non neural operate string level similarity Embedding based RAG class leverages neural encoders primarily transformers map text dense vector representations Early methods like DPR Contriever based BERT recent models NV utilize decoder backbones achieve significantly improved retrieval performance Structure Augmented RAG approaches enhance retrieval structural representations graphs trees Representative systems include GraphRAG RAPTOR Cognee Zep offers graph augmented variant g built structured factual
knowledge Despite effectiveness RAG based methods face challenges ambiguous queries multi hop reasoning long range comprehension questions require integrating knowledge entire session learning long skill encoding inputs retrieval mechanism limited k relevant passages fail surface necessary information address limitations Agentic Memory Agents introduce iterative decision driven framework relying single pass retrieval agents dynamically process query retrieve evidence reflect iterate multiple retrieval reasoning cycles Examples include MemGPT Self RAG Auto RAG agentic design particularly effective resolving ambiguous multi step queries Nonetheless methods remain fundamentally constrained limitations RAG inability fully understand learn long range context inaccessible retrieval MemoryAgentBench Aspects Evaluation evaluation memory
agents encompasses following key dimensions Accurate Retrieval AR task accurately retrieving information extensively explored prior work domain long context modeling Needle Haystack NIAH task widely evaluate model ability locate specific value based given key lengthy input Extensions multi value NIAH test model capacity retrieve multiple values scattered input context RAG setting corresponds document based QA model identify extract relevant snippets documents answer query snippets reside single location distributed multiple documents paper focus agentic settings long context multiple documents long form conversations define Accurate Retrieval AR ability agent identify retrieve important information dispersed long dialogue history Test Time Learning TTL essential
capability real world agents ability acquire new skills dynamically interaction users mirrors concept Context Learning ICL LLMs model learns prompt containing small number examples framed shot classification tasks Ideally performance improves additional examples prompt conversational agent setting prompts replaced dialogue histories define Test Time Learning TTL agent ability learn perform new tasks directly conversation property crucial enabling self evolving agents continuously adapt improve real world deployments Long Range Understanding LRU Long range understanding refers agent ability form abstract high level comprehension extended conversations example user narrates long story agent retain content derive holistic understanding recall isolated facts define Long Range
Understanding LRU ability reason long form inputs answer high level questions require understanding overall content detailed recall example question Summarize main experiences Harry Potter Conflict Resolution CR long term interactions agents face evolving conflicting information external world changes political leadership user specific facts new occupation challenge closely related model editing knowledge unlearning focus modifying removing factual knowledge language models define Conflict Resolution CR agent ability detect resolve contradictions existing knowledge newly acquired information ensuring agent remains aligned current realities user states CR distinct Abstractive Retrieval AR key ways Certain questions requiring CR answered solely AR illustrated Figure agent retrieves facts
related pears fail identify updated information Table Datasets categorized specific aspects evaluation Capability Benchmarks Tasks Sequences QAs Avg Len Accurate Retrieval RULER QA K RULER NIAH MQ K QA K LongMemEval S K EventQA K Test Time Learning NLU K TREC Coarse TREC Fine Movie Rec Redial M Long Range Understanding Sum K Conflict Resolution FactConsolidation SH K FactConsolidation MH second message AR earlier messages remain relevant retained multiple pieces evidence required contrast CR involves identifying outdated incorrect information discarding AR requires preservation related content CR requires overwriting prior facts reflect date truth Dataset Preperation section describe adopt existing datasets
construct new ones evaluating aspect introduced Section datasets categories shown Table introduce details datasets curation Appendix Datasets Accurate Retrieval AR adopt datasets evaluate accurate retrieval pability memory agents adapted existing benchmarks newly constructed RULER QA NIAH style QA task long passage contains single multiple snippets answering input question agent identify extract relevant snippets extended context NIAH MQ use multiple query MQ version NIAH dataset RULER query seeks different numeric value embedded long passage agent accurately retrieve multiple distinct answers En QA task presents free form QA questions based entire books entities replaced fictitious names avoid contamination model pretraining Compared synthetic
datasets like RULER QA benchmark realistic challenging natural narrative structure books LongMemEval benchmark evaluates memory agents long dialogue histories task types like information extraction IE multi session reasoning included tasks reformulated single retrieval problems requiring agents retrieve correct segments spanning long multi turn conversation LongMemEval formatted agent based evaluation session separation use original dataset K tokens reformulated chat history long dialogues K tokens questions LongMemEval S Table create LongMemEval S specifically increasing number questions context mitigating exhaustive needs reconstructing memory quesiton EventQA introduce EventQA reasoning style NIAH task evaluate agents ability recall reason temporal sequences long form narratives dataset agent
required read novel select correct event series candidates receiving previous events datasets originally designed long context modeling split documents chunks sequentially inject agent Datasets Test Time Learning TTL evaluate TTL task categories Class Classification MCC adopt classification datasets prior TTL work TREC Coarse TREC Fine NLU task requires agent map sentences class labels leveraging previously seen labeled examples context Recommendation Recom use Redial dataset evaluate movie recommendation dialogue Following setup et al agent exposed thousands movie related dialogue turns asked recommend relevant movies based long interaction history Datasets Long Range Understanding LRU adopt Summarization task En Sum agent required analyze
organize plot characters novel compose summary words Datasets Conflict Resolution CR assess agent consolidate conflicting tual updates reason construct new dataset called FactConsolidation Specifically build benchmark counterfactual edit pairs MQUAKE pair contains true fact rewritten contradictory version ordered rewritten new fact appears original simulating realistic update scenario concatenate multiple edit pairs create long contexts length K K adpot MQUAKE original questions categorize FactConsolidation SH SH means Single Hop requiring direct factual recall country tool created FactConsolidation MH MH refers Multi Hop requiring inference multiple facts location death spouse person B Agents prompted prioritize later information case conflict reason based final
memory state setup directly evaluates strength consistency conflict resolution long sequences Different Categories Memory Agents evaluate major types memory agents reflect common strategies handling long term information Long Context Agents RAG Agents Agentic Memory Agents approaches differ store retrieve reason past inputs Long Context Agents Modern language models support extended context windows ranging K M tokens straightforward strategy memory maintain context buffer recent tokens example model token limit agent concatenates incoming chunks total exceeds window size limit reached earliest chunks evicted FIFO manner agent design relies solely positional recency assumes model attend effectively current context window RAG Agents RAG based
agents address context limitations storing past information external memory pool retrieving relevant content needed consider RAG variants Simple RAG Agents input chunks stored raw text inference keyword rule based string matching mechanism retrieves relevant passages Embedding based RAG Agents input chunk embedded saved query time agent embeds query performs retrieval cosine similarity embeddings Structure Augmented RAG Agents ingesting input chunks agent constructs structured representation knowledge graph event timeline Subsequent queries answered based structured memory Agentic Memory Agents Agentic memory agents extend static memory stores employing agentic loops iterative reasoning cycles agent reformulate questions perform memory lookups update working memory agents
designed simulate human like process recalling verifying integrating knowledge Datasets Agents Formulation Datasets Formulation standardize datasets format cn chunks qm questions answers ci denotes th chunk wrapped construct user message instructions memorizing content sequential input cn represents single conversation chunk accompanied instructions prompting agent memorize contents Example prompts provided Appendix curating datasets like EventQA FactConsolidation deliberately design scenarios multiple questions follow single context allows probe model memory multiple times sequential injection example LME S contexts paired questions shown Table design choice reflects key trend LLMs support increasingly long context windows memory agents capable handling extended inputs evaluation datasets scale accordingly
Injecting M tokens question resource inefficient associating input questions provides significantly higher utility Table Overall Performance Comparison RAG agents commercial memory agents use mini backbone highlight performance mini reference FactCon SH FactCon MH mean FactConsolidation Single Hop FactConsolidation Multi Hop respectively use NV Embed dense retriever based open source code HippoRAG AR TTL LRU CR Agent Type RULER QA NIAH MQ QA EventQA MCC Recom Sum FactCon SH FactCon MH Long Context Agents mini mini Flash Sonnet mini Simple RAG Agents Embedding RAG Agents Contriever Text Small Text Large NV Embed Structure Augmented RAG Agents RAPTOR GraphRAG HippoRAG Cognee Agentic
Memory Agents Self RAG MemGPT Agents Formulation framework agents required chunks absorb memory incrementally update memory seeing chunks ask agent answer related questions Experiments Experimental Setup datasets split categories statistics datasets shown Table evaluation metrics datasets shown Table Appendix dataset details agents described Section consider categories agents Long Context Agents RAG agents Agentic Memory Agents RAG Agents split Simple RAG Agents Embedding based RAG Agents Structure Augmented RAG Agents chunk size settings choose chunk size RULER QA NIAH MQ tasks AR tasks CR mainly tasks composed long texts synthesized multiple short texts tasks use chunk size Considering computational overhead uniformly
use chunk size Cognee methods report settings chunk size Table Appendix Overall Performance Comparison Table presents overall performance different benchmarks summarize key findings follows Superiority RAG methods Accurate Retrieval Tasks RAG Agents better backbone model mini tasks Accurate Retrieval Category matches intuition RAG agents typically excel extracting small snippet text crucial answering question Superiority Long Context Models Test Time Learning Long Range Understanding Long context models achieve best performance TTL LRU highlights fundamental limitation RAG methods commercial memory agents follow agentic RAG paradigm systems retrieve partial information past context lacking ability capture holistic understanding input let perform learning Limitation Existing
Methods Conflict Resolution discussed task model editing community resolving conflict poses significant challenge memory agents observe methods fail multi hop situation achieving accuracy long context agents Chunk Size Accuracy NVEmbed HippoRAG MemGPT RULER QA performance Chunk Size Model Based NVEmbed HippoRAG MemGPT b Sum performance Figure Performances RULER QA different chunk sizes achieve fairly reasonable results single hop scenarios Section current reasoning models better performance change conclusion Conflict Resolution poses significant challenge memory mechanisms Limited Performance Commercial Memory Agents Commercial memory agents MemGPT exhibit limited performance broad range benchmarks shortfall attributed primary factors systems frequently fail capture preserve sufficient information
storing inputs memory example depends extracting factual knowledge inputs approach inherently discards substantial portion original content result reconstructing inputs supporting downstream tasks question answering significantly challenging demonstrated relatively strong performance conversational tasks LOCOMO information density comparatively low tends perform poorly benchmarks containing dense informational content including RULER tasks emphasizing Time Live TTL Recently LRU retrieval limitations pronounced Second MemGPT rely retrieval mechanisms access subset stored information case retrieval typically performed single time similar conventional RAG methods constraining breadth information available reasoning MemGPT adopting agentic framework permits multiple retrieval iterations maintain temporal structural metadata stored content Consequently agent unable reconstruct longer
documents original form adversely affects performance LRU tasks requiring structured memory retrieval Finally methods MemGPT depend heavily embedding based retrieval mechanisms insufficient fine grained tasks like NIAH locating specific precise information needle haystack essential embedding based approaches struggle distinguish subtle contextual nuances critical accurate retrieval settings Ablation Study Section conduct experiments result analysis dimensions Input Chunk Size Retrieval TopK Validation Dataset Computational Latency detailed experimental results provided Appendix Ablation Study Input Chunk Size understand chunk size impacts performance particularly RAG methods agentic memory agents conduct additional analysis vary chunk size fixing number retrieved chunks results presented Figure figure observe following
RULER QA task reducing chunk size little effect performance expected relies term frequency based scoring level ranking inherently benefit finer grained segmentation impact term distributions contrast embedding based methods including MemGPT uses text small retriever consistently perform better smaller chunks suggests finer segmentation improves granularity relevance retrieved results models rely dense semantic representations Sum smaller chunk sizes lead worse performance task requires agent summarize entire conversation smaller K Accuracy RULER QA K Multi Class Classification K Bench QA NV Embed HippoRAG Figure accuracies different benchmarks varying retrieval k chunks correspond fewer available tokens retrieval result agent access context degrades summarization
quality results suggest resources permit smaller chunk sizes increasing number retrieval calls memory construction improve performance Accurate Retrieval AR tasks Finer grained segmentation enhances relevance retrieved information particularly embedding based methods tasks requiring Long Range Understanding LRU varying chunk size hurts performance likely RAG methods inherently suited tasks demand integration information large coherent context Ablation Study Retrieval TopK experiments report results number retrieved chunks set Table conducted ablation studies varying retrieval sizes subset results visualized Figure results provided Table results indicate increasing number retrieved chunks generally improves performance tasks worth noting chunk size tokens retrieving chunks yields input approximately tokens
places significant demands model capacity high token volume evaluate settings retrieved chunks Validation Dataset FactConsolidation Table Performances reasoning models dataset FactConsolidation FactCon SH FactCon MH K K K K mini performance different models dataset remains drastically low turn stronger ing model mini validate dataset ing performance mini smaller version dataset results shown Table Analysis Computational Latency Table Computational Latency seconds Memory Construction Query ecution mini Contriever Text Large NV Embed RAPTOR GraphRAG HippoRAG Cognee Self RAG MemGPT illustrate latency memory agents terms Memory Construction Query Execution domly choose examples RULER LME S report latency ous memory agents experiments server NVDIA
GPU AMD EPYC Core CPU use NV Embed embedding model HippoRAG summarized sults Table results Table table find smaller chunk size requires significantly time memory construction especially methods HippoRAG Cognee MemGPT methods Cognee need extremely high resources constructing memory pose challenges real world applications Conclusion Future Work paper introduce MemoryAgentBench unified benchmark designed evaluate ory agents essential competencies accurate retrieval test time learning long range understanding conflict resolution prior benchmarks focus largely skill execution long context reasoning MemoryAgentBench fills critical gap assessing agents store update utilize long term information multi turn interactions build benchmark restructure existing datasets propose new ones
EventQA FactConsolidation tailored stress specific memory behaviors overlooked prior work evaluate wide spectrum agents including long context models RAG based systems commercial memory agents consistent evaluation protocol results reveal despite recent advances current memory agents exhibit substantial limitations faced tasks requiring dynamic memory updates long range consistency limitation work datasets MemoryAgentBench primarily synthetic fully reflect characteristics real world user conversations future work aim collect curate realistic real world datasets aligned competencies enrich diversify benchmark provide comprehensive evaluations memory agents Acknowledgment thank Kevin Lin engaging thoughtful discussions overall idea latency evaluation input played role shaping evaluation pipeline memory agents References Anthropic
Claude sonnet URL sonnet announcement introduces Claude Sonnet described thropic intelligent model date hybrid reasoning model generally available market Akari Asai Zeqiu Wu Yizhong Wang Avirup Sil Hannaneh Hajishirzi Self rag ing retrieve generate critique self reflection Twelfth International Conference Learning Representations Yushi Bai Xin Lv Jiajie Zhang Hongchang Lyu Jiankai Tang Zhidian Huang Zhengxiao Du Xiao Liu Aohan Zeng Lei Hou et al Longbench bilingual multitask benchmark long context understanding arXiv preprint Yushi Bai Shangqing Tu Jiajie Zhang Hao Peng Xiaozhi Wang Xin Lv Shulin Cao Jiazheng Xu Lei Hou Yuxiao Dong et al Longbench deeper understanding reasoning realistic
long context multitasks arXiv preprint Amanda Bertsch Maor Ivgi Emily Xiao Uri Alon Jonathan Berant Matthew R Gormley Graham Neubig context learning long context models depth exploration arXiv preprint Iñigo Casanueva Tadas Temˇcinas Daniela Gerz Matthew Henderson Ivan Efficient intent detection dual sentence encoders Tsung Hsien Wen Asli Celikyilmaz Zhou Yu Alexandros Papangelis Mihail Eric Anuj Kumar Iñigo Casanueva Rushin Shah editors Proceedings Workshop Natural Language Processing Conversational AI pages Online July Association Computational Linguistics doi URL Prateek Chhikara Dev Khant Saket Aryan Taranjeet Singh Deshraj Yadav Building production ready ai agents scalable long term memory arXiv preprint DeepMind Gemini
pro URL page provides overview Gemini Pro highlighting advanced capabilities applications fields Jacob Devlin Ming Wei Chang Kenton Lee Kristina Toutanova Bert Pre training deep bidirectional transformers language understanding Proceedings conference North American chapter association computational linguistics human language technologies volume long short papers pages Darren Edge Ha Trinh Newman Cheng Joshua Bradley Alex Chao Apurva Mody Steven Truitt Dasha Metropolitansky Robert Osazuwa Ness Jonathan Larson local global graph rag approach query focused summarization arXiv preprint Junfeng Fang Houcheng Jiang Kun Wang Yunshan Ma Shi Jie Xiang Wang Xiangnan Tat Seng Chua Alphaedit Null space constrained knowledge editing language models
arXiv preprint Bernal Jiménez Gutiérrez Yiheng Shu Weijian Qi Sizhe Zhou Yu Su rag memory Non parametric continual learning large language models arXiv preprint Zhankui Zhouhang Xie Rahul Jha Harald Steck Dawen Liang Yesu Feng hisattwa Prasad Majumder Nathan Kallus Julian McAuley Large language models zero shot conversational recommenders Proceedings ACM international conference information knowledge management pages Zhankui Zhouhang Xie Rahul Jha Harald Steck Dawen Liang Yesu Feng hisattwa Prasad Majumder Nathan Kallus Julian McAuley Large language models zero shot conversational recommenders Proceedings ACM international conference information knowledge management pages Cheng Ping Hsieh Simeng Sun Samuel Kriman Shantanu Acharya Dima
Rekesh Fei Jia Yang Zhang Boris Ginsburg RULER Real Context Size Context Language Models August URL cs Mengkang Hu Yuhang Zhou Wendong Fan Yuzhou Nie Bowei Xia Tao Sun Ziyu Ye Zhaoxuan Jin Yingru Li Zeyu Zhang Yifeng Wang Qianshuo Ye Ping Luo Guohao Li Owl Optimized workforce learning general multi agent assistance real world task automation URL Gautier Izacard Mathilde Caron Lucas Hosseini Sebastian Riedel Piotr Bojanowski Armand Joulin Edouard Grave Unsupervised dense information retrieval contrastive learning arXiv preprint Carlos E Jimenez John Yang Alexander Wettig Shunyu Yao Kexin Pei Ofir Press Karthik Narasimhan Swe bench language models resolve
real world github issues arXiv preprint Marzena Karpinska Katherine Thai Kyle Lo Tanya Goyal Mohit Iyyer sand pairs novel challenge long context language models arXiv preprint Vladimir Karpukhin Barlas Oguz Sewon Min Patrick SH Lewis Ledell Wu Sergey Edunov Danqi Chen Wen tau Yih Dense passage retrieval open domain question answering EMNLP pages Stefan Larson Anish Mahendran Joseph Peper Christopher Clarke Andrew Lee Parker Hill Jonathan Kummerfeld Kevin Leach Michael Laurenzano Lingjia Tang Jason Mars evaluation dataset intent classification scope prediction Kentaro Inui Jing Jiang Vincent Ng Xiaojun Wan editors Proceedings Conference Empirical Methods Natural Language Processing International Joint Conference
Natural Language Processing EMNLP IJCNLP pages Hong Kong China November Association Computational Linguistics doi URL Chankyu Lee Rajarshi Roy Mengyao Xu Jonathan Raiman Mohammad Shoeybi Bryan Catanzaro Wei Ping Nv embed Improved techniques training llms generalist embedding models arXiv preprint Jiaqi Li Mengmeng Wang Zilong Zheng Muhan Zhang Loogle long context language models understand long contexts arXiv preprint Raymond Li Samira Ebrahimi Kahou Hannes Schulz Vincent Michalski Laurent Charlin Chris Pal deep conversational recommendations Advances neural information processing systems Xianming Li Julius Lipp Aamir Shakir Rui Huang Jing Li Bmx Entropy weighted similarity semantic enhanced lexical search arXiv preprint Xin
Li Dan Roth Learning question classifiers COLING tional Conference Computational Linguistics URL Kevin Lin Charlie Snell Yu Wang Charles Packer Sarah Wooders Ion Stoica Joseph E Gonzalez Sleep time compute inference scaling test time arXiv preprint Xingkun Liu Arash Eshghi Pawel Swietojanski Verena Rieser Benchmarking natural language understanding services building conversational agents URL Adyasha Maharana Dong Ho Lee Sergey Tulyakov Mohit Bansal Francesco Barbieri Yuwei Fang Evaluating long term conversational memory llm agents arXiv preprint Kevin Meng Arnab Sen Sharma Alex Andonian Yonatan Belinkov David Bau editing memory transformer ICLR Grégoire Mialon Clémentine Fourrier Thomas Wolf Yann LeCun Thomas Scialom
Gaia benchmark general ai assistants Twelfth International Conference Learning Representations Eric Mitchell Charles Lin Antoine Bosselut Christopher Manning Chelsea Finn Memory based model editing scale ICML volume Proceedings Machine Learning Research pages PMLR Ali Modarressi Hanieh Deilamsalehy Franck Dernoncourt Trung Bui Ryan Rossi unghyun Yoon Hinrich Schütze Nolima Long context evaluation literal matching arXiv preprint Magnus Müller Gregor Žuniˇc Browser use Enable ai control browser URL OpenAI system card URL system report outlines safety work carried prior leasing including external red teaming frontier risk evaluations according Preparedness Framework overview mitigations built address key risk areas Charles Packer Vivian Fang Patil
Kevin Lin Sarah Wooders lez Memgpt llms operating systems Preston Rasmussen Pavlo Paliychuk Travis Beauvais Jack Ryan Daniel Chalef Zep temporal knowledge graph architecture agent memory arXiv preprint Stephen E Robertson Steve Walker simple effective approximations poisson model probabilistic weighted retrieval Proceedings Seventeenth Annual International ACM SIGIR Conference Research Development Information Retrieval organised Dublin City University pages Springer Parth Sarthi Salman Abdullah Aditi Tuli Shubh Khanna Anna Goldie Christopher D Manning Raptor Recursive abstractive processing tree organized retrieval Twelfth International Conference Learning Representations Cunxiang Wang Ruoxi Ning Boqi Pan Tonghui Wu Qipeng Guo Cheng Deng Guangsheng Bao Qian Wang Yue
Zhang Novelqa benchmark long range novel question answering arXiv preprint Minzheng Wang Longze Chen Fu Cheng Shengyi Liao Xinghua Zhang Bingli Wu Haiyang Yu Nan Xu Lei Zhang Run Luo et al Leave document Benchmarking long context llms extended multi doc qa Proceedings Conference Empirical Methods Natural Language Processing pages Xingyao Wang Boxuan Li Yufan Song Frank F Xu Xiangru Tang Mingchen Zhuge Jiayi Pan Yueqi Song Bowen Li Jaskirat Singh et al Openhands open platform ai software developers generalist agents Thirteenth International Conference Learning Representations Yu Wang Xinshuang Liu Xiusi Chen Sean Junda Wu Julian McAuley updatable large language
models integrating context model parameters Thirteenth International Conference Learning Representations Yu Wang Yifan Gao Xiusi Chen Haoming Jiang Shiyang Li Jingfeng Yang Qingyu Yin Zheng Li Xian Li Bing Yin et al Memoryllm self updatable large language models arXiv preprint Yu Wang Ruihan Wu Zexue Xiusi Chen Julian McAuley Large scale knowledge washing arXiv preprint Yu Wang Dmitry Krotov Yuanzhe Hu Yifan Gao Wangchunshu Zhou Julian McAuley Dan Gutfreund Rogerio Feris Zexue Extending memoryLLM scalable term memory second International Conference Machine Learning URL Yu Wang Chi Han Tongtong Wu Xiaoxin Wangchunshu Zhou Nafis Sadeq Xiusi Chen Zexue Wei Wang Gholamreza
Haffari Heng Ji Julian McAuley lifespan cognitive systems TMLR Di Wu Hongwei Wang Wenhao Yu Yuwei Zhang Kai Wei Chang Dong Yu memeval Benchmarking chat assistants long term interactive memory arXiv preprint Qiyu Wu Chongyang Tao Tao Shen Xu Xiubo Geng Daxin Jiang Pcl contrastive learning diverse augmentations unsupervised sentence embeddings arXiv preprint Howard Yen Tianyu Gao Minmin Hou Ke Ding Daniel Fleischer Peter Izsak Moshe Wasserblat Danqi Chen Helmet evaluate long context language models effectively thoroughly arXiv preprint Zhangyue Yin Qiushi Sun Qipeng Guo Zhiyuan Zeng Qinyuan Cheng Xipeng Qiu Jing Huang Explicit memory learning expectation maximization Proceedings Conference
Empirical Methods Natural Language Processing pages Tian Yu Shaolei Zhang Yang Feng Auto rag Autonomous retrieval augmented generation large language models arXiv preprint Xinrong Zhang Yingfa Chen Shengding Hu Zihang Xu Junhao Chen Moo Hao Xu Han Zhen Thai Shuo Wang Zhiyuan Liu et al Extending long context evaluation tokens Proceedings Annual Meeting Association Computational Linguistics Volume Long Papers pages Zexuan Zhong Zhengxuan Wu Christopher D Manning Christopher Potts Danqi Chen Mquake Assessing knowledge editing language models multi hop questions arXiv preprint Details Dataset provide detailed introduction datasets evaluating core competencies including dataset curation corresponding metrics average context length brief
description Details shown Table Accurate Retrieval AR use datasets evaluate accurate retrieval capability memory agents RULER QA adopt QA datasets datasets provide multiple synthetic contexts varying lengths ranging K K tokens select questions datasets shorter contexts questions collect documents removed duplicate content shuffled concatenated create new long contexts K K tokens making sure new contexts containing gold passage answers short informational entities years names yes responses use substring exact match SubEM evaluation metric SubEM measures predicted answer exactly matches gold answer substring common standard question answering systems NIAH MQ choose context length K tokens containing groups total queries check queries
appeared context randomly shuffl queries corresponding numbers evenly prevent clustering long context primary evaluation criterion agent successfully retrieve correct numbers use average recall evaluation metric En QA borrow dataset QA task novels character names replaced makes content coherent closer real world scenarios use ROUGE dataset answers entity names LongMemEval dialogue based QA dataset use multiple historical conversation data segments arrange chronological order finally concatenate create long conversation histories length approximately K tokens questions open ended answers adopt approach previous work employ model assess agent responses meet requirements response deemed satisfactory marked True Finally calculate proportion satisfactory responses evaluation metric EventQA
books K tokens counted mini tokenizer identify frequently mentioned characters SpaCy NER extract events experienced key characters event construct way multiple choice question pairing true event distractors generated agent receives previous events identify correct continuation report mean accuracy questions book ultimately present average accuracy books Test time Learning TTL evaluate TTL task categories Multi Class Classification MCC adopt classification datasets prior TTL work dataset curation use thousands sentence samples different categories type sample assigned number label Following format sentence Label label concatenate sentences long context shuffle prevent samples type concentrated task agent needs refer long context correctly classify input content
use average accuracy evaluation metric Recommendation Recom concatenate multiple short dialogues movie recommendations original dataset remove duplicate dialogues create long context containing thousand recommendation instances task agent required recommend movies based content dialogue evaluate recommendations calculating measures overlap recommended movies ground truth Table Overview evaluation datasets select datasets cover important context capabilities SubEM substring exact match table underline datasets constructed Avg Length Average Context Length measured mini model tokenizer Category Dataset Metrics Avg Length Description Accurate Retrieval RULER SubEM K Gold passage retrieval QA RULER K RULER NIAH MQ Recall K Retrieve multiple needles haystack QA ROUGE K Novel QA
entity replacement LongMemEval S Model Based Acc K Dialogues based QA LongMemEval S K EventQA Accuracy K Novel multiple choice QA characters events Test time Learning Accuracy K Banking intent classification labels Intent classification labels NLU Task intent classification labels TREC Coarse Question type classification labels TREC Fine Question type classification labels Movie Recommendation M Recommend movies based provided dialogues examples Long Range Understanding Sum Model Based K Novel summarization entity replacement Conflict Resolving FactConsolidation SH SubEM K Conflict solving single hop reasoning FactConsolidation MH Conflict solving multiple hop reasoning Long Range Understanding LRU evaluate LRU Summarization task En Sum
follow settings use model evaluating summarized text process assess fluency input text scored use dot product score score final evaluation metric Conflict Resolution CR use counterfactual edit pairs MQUAKE dataset sentence containing tion assigned number edit pair sentence representing outdated information distractor given smaller number sentence representing recent information containing answer given larger number concatenate sentences long context order according assigned numbers evaluate CR datasets Hop Editing Multi Hop Editing tasks agent responses informational entities use SubEM Substring Exact Match evaluation metric B Prompts introduce example prompts section Instructions Memory Construction processing long context inputs split content chunks specified size
feed chunks agent memory agent extract relevant information memory based query assist query execution chunking approach helps organize manage large amounts contextual information making retrieval reasoning efficient Figure provide example instructions require agent memorize corresponding context Instructions Long Context Agents Figure provide examples instructions different datasets existing datasets adjust prompt settings previous work example dataset QA Sum insert answer examples demo prompt help agent better understand questions standardize outputs Prompts Memory Construction Tasks LongMemEval Memorize following conversation user assistant chunk ELIF Movie Recommendation Memorize following dialogues user recommender system chunk ELIF Fact Consolidation Memorize following facts chunk Memorize following content
chunk Figure prompts use agents create memory Instructions RAG Agents provide examples prompts RAG based Agents Figure type agent storing input long context memory use question memory retrieval query tasks task RULER NIAH MQ use entire question special magic numbers question mentioned memorized text query Sum use entire query demo memory retrieval C Detailed Experimental Results section provide detailed versions results presented main text Detailed Results AR Table present detailed results agent dataset AR tasks Simple RAG Agents equipped retrievers like significantly improve performance compared backbone model mini limited K context length restricts information process overall performance Embedding RAG Agents
surpasses Structure Augmented RAG Agents Agentic Memory Agents advantage primarily attributed use dense retrieval Embedding RAG Agents enables extraction longer contextual information memory result Embedding RAG Agents able provide richer comprehensive context tasks Detailed Results TTL LRU CR detailed results dataset Table types tasks RAG based agents generally underperform compared respective mini backbones observation highlights certain limitations inherent RAG approach instance TTL tasks RAG based methods struggle accurately retrieve context memory closely associated input LRU tasks methods face challenges achieving comprehensive understanding long contexts Furthermore CR tasks especially multi hop variants effective handling requires strong reasoning information extraction capabilities remain
reach current agents Detailed Results Ablation Study section introduce detailed results ablation study different chunk sizes retrieve number context length computation latency Table compare RAG based Agents different chunk sizes datasets selected chunk sizes sets datasets composed synthetic text RULER QA smaller chunk size generally helps RAG Agents Agentic Memory Agents achieve better test performance datasets composed continuous text QA retrieval number k remains unchanged reducing chunk size lead performance improvement Prompts Long Context Agents Tasks RULER QA context given memory memorize Answer question based memorized documents answer output words Answer Question question Answer RULER NIAH MQ context given memory
memorize special magic numbers hidden memorized text sure memorize quiz numbers Answer Question special magic numbers question mentioned memorized text special magic numbers question mentioned memorize text QA context given memory memorize Based context memorized answer question concisely single phrase possible Answer Question Answer LongMemEval history chats user memory memorize history chats user Based relevant chat history answer question concisely single phrase Current Date Answer Question question Answer EventQA context given memory memorize Based context memorized complete task events occurred list possible subsequent question task choose events event happens based book excerpt response include answer event happens Label Matching etc
context given memory memorize Use provided mapping context numerical label assign numerical label context output label label Question question label Movie Recommendation dialogues user recommender system memory memorize Pretend movie recommender system need recommend movies based dialogues memorized new conversation user recommender system Based conversation reply recommendations extra sentences Conversation recommendations conversation question recommendations Sum book given memory memorize given book tasked summarize Write summary words write plot characters story discuss themes background book provide analysis commentary demo summarize book Fact Consolidation knowledge pool lots new facts memory memorize Pretend knowledge management system fact knowledge pool provided serial number beginning
newer fact larger serial number need solve conflicts facts knowledge pool finding newest fact need answer question based rule concise answer saying words question knowledge pool memorized real facts real world example Knowledge Pool Question Based provided Knowledge Pool current president Country R Answer Person Answer Question Based provided Knowledge Pool question Answer Figure prompts use Long Context Agents Table memory refers accumulated text sequential inputs Table evaluate selected RAG based Agents datasets choose different TopK ranging find AR series tasks increasing retrieve number TopK leads significant improvement performance TTL series tasks performance gains increasing TopK pronounced Prompts RAG Based
Agents Tasks RULER QA context retrieved memory Answer question based retrieved context answer output words Answer Question question Answer RULER NIAH MQ context retrieved memory special magic numbers hidden retrieved text sure memorize quiz numbers Answer Question special magic numbers question mentioned memorized text special magic numbers question mentioned memorize text QA context retrieved memory Based context retrieved answer question concisely single phrase possible Answer Question Answer LongMemEval retrieved history chats user memory memory retrieved history chats user Based relevant chat history answer question concisely single phrase Current Date Answer Question question Answer EventQA context retrieved memory Based context retrieved
complete task events occurred list possible subsequent question task choose events event happens based book excerpt response include answer event happens Label Matching etc examples retrieved memory memory Use retrieved mapping context numerical label assign numerical label context output label label Question question label Movie Recommendation retrieved dialogues user recommender system memory memory Pretend movie recommender system need recommend movies based example dialogues retrieved new conversation user recommender system Based conversation reply recommendations extra sentences Conversation recommendations conversation question recommendations Sum book context retrieved memory given memory given retrieved context tasked summarize Write summary words write plot characters story discuss
themes background book provide analysis commentary demo summarize book Fact Consolidation list knowledge retrieved memory memory Pretend knowledge management system fact retrieved knowledge pool provided serial number beginning newer fact larger serial number need solve conflicts facts retrieved knowledge pool finding newest fact need answer question based rule concise answer saying words question retrieved knowledge pool memorized real facts real world example Knowledge Pool Question Based provided Knowledge Pool current president Country R Answer Person Answer Question Based provided Knowledge Pool question Answer Figure prompts use Simple RAG Agents Embedding RAG Agents Augmented RAG Agents Agentic Memory RAG Agents Table
memory refers retrieved text sequential inputs MemGPT method add phrase Search Archival Memory prompt task Table report performances different agents scaling input length measure average context length tokenizer mini K Long Context Agents tasks AR series generally achieve satisfactory performance relatively Table Overall Performance Comparison datasets AR RAG agents commercial memory agents use mini backbone highlight performance mini reference Agent Type RULER RULER NIAH MQ QA EventQA Long Context Agents mini mini Flash Sonnet mini Simple RAG Agents Embedding RAG Agents Contriever Text Small Text Large NV Embed Structure Augmented RAG Agents RAPTOR GraphRAG HippoRAG Cognee Agentic Memory Agents Self
RAG MemGPT Table Overall performance comparison datasets TTL LRU CR RAG agents commercial memory agents use mini backbone Agent Type BANKING CLINC NLU TREC C TREC F Recom Summ FactCon SH FactCon MH Long Context Agents mini mini Flash Sonnet mini Simple RAG Agents Embedding RAG Agents Contriever Text Small Text Large NV Embed Structure Augmented RAG Agents RAPTOR GraphRAG HippoRAG Cognee Agentic Memory Agents Self RAG MemGPT small context lengths K tokens context length increases performance agents declines accordingly contrast RAG based agents Cognee performance significantly lower backbone mini context length relatively small Table Table provide computational latency different
agents choose chunk sizes datasets RULER experimental results demonstrate agents selecting smaller chunk size leads significantly higher computational latency example case Cognee computational latency chunk size nearly times greater chunk size Table Performance comparison different datasets chunk sizes choose different chunk sizes use RAG based methods NIAH MQ QA Event QA FactCon SH FactCon MH Simple RAG Agents Embedding RAG Agents Contriever Text Small Text Large NV Embed Structure Augmented RAG Agents RAPTOR GraphRAG HippoRAG Agentic Memory Agents Self RAG MemGPT Table Performance comparison different datasets chunk sizes choose chunk sizes use RAG based methods RULER RULER Sum NVEmbed HippoRAG
MemGPT Table Performance comparison different retrieve number RULER NIAH QA EventQA TTL MCC Contriever Text Large NV Embed RAPTOR HippoRAG Self RAG Table Performance comparison different context length RULER NIAH EventQA FactCon SH FactCon MH K K K K K K K K K K K K K K K mini mini Flash Sonnet Cognee D Experimental Settings section present experimental settings Max Output Tokens provide token number limitation task Table Table Computational latency seconds comparison Long Context Agents RULER LME S mini mini Flash Sonnet Table Computational latency seconds comparison RAG based agents means Memory Construction means Query Execution
RULER LME S Contriever Text Large NV Embed RAPTOR GraphRAG HippoRAG Cognee Self RAG MemGPT Table Maximum output token limits tasks Task Max Output Tokens RULER QA RULER NIAH MQ QA LongMemEval EventQA Movie Recommendation Sum FactConsolidation Settings RAG Agents embedding model selection Structure Augmented RAG Agents Agentic Memory Agents approaches utilize OpenAI embedding models Text Small HippoRAG method follow experimental setting Gutiérrez et al employing NV Embed model implement open sourced memory agents main experiments use function add message content context chunk agent memory repository memory consolidation query execution relevant memory elements retrieved function retrieved memories integrated query processed
mini backbone model complete requested tasks MemGPT employ function memory consolidation phase inject long context chunks Archival Memory structure query execution agent processes requests function generates appropriate responses based archived information Cognee utilize functions construct memory graph input chunks memory consolidation phase query execution function retrieve contextually relevant information memory graph based input query Settings Chunk Size use smaller chunk size synthetic context AR CR tasks based continuous text EventQA larger chunk size tasks MCC Recom considering characteristics tasks computational cost chose larger chunk size memory construction methods time consuming Cognee uniformly chunk size datasets Chunk Size RULER QA NIAH
MQ QA Sum Dataset FactCon SH FactCon MH MCC Recom EventQA Table choice chunk size different datasets
cs AI Jul Rules Represent Causal Knowledge Causal Modeling Abductive Logic Programs Kilian Felix Eberhard Karls Universität Tübingen Auf der Morgenstelle Tübingen German University Digital Science Marlene Dietrich Allee Potsdam Ludwig Maximilians Universität München Oettingenstr München Abstract Pearl observes causal knowledge enables predicting effects interventions actions descriptive knowledge permits drawing conclusions observation paper extends Pearl approach causality interventions setting stratified abductive logic programs shows stable models programs given causal interpretation building philosophical dations recent work Bochman Eelink et al particular provides translation abductive logic programs causal systems clarifying informal causal reading logic program rules supporting principled reasoning external actions main
sult establishes stable model semantics stratified programs conforms key philosophical principles causation causal ficiency natural necessity irrelevance unobserved effects justifies use stratified abductive logic programs framework causal modeling predicting effects interventions Keywords Causal Logic Stable Model Semantics Abductive Logic Programming Interventions Calculus Introduction central topic philosophical inquiry millennia ity entered mainstream artificial intelligence research work Pearl key feature account causal knowledge goes scriptive knowledge questions address descriptive knowledge permits inferences observations causal knowledge enables reasoning effects external interventions actions modeled system Example Consider road passes field sprinkler Assume sprinkler turned weather sensor sunny Suppose rains cloudy road wet rains sprinkler
activated Finally assume wet road dangerous Observing sprinkler conclude weather sunny actively intervening switching sprinkler cause weather sunny predict effect intervention needs causal merely descriptive knowledge Rückschloß Weitkämper evaluating effects possible actions primary tions modeling place paved way adoption causal frameworks wide variety domains Pearl develops theory causality exclusively formalisms Bayesian networks structural causal models Example Recall Example denote c event weather cloudy s event sprinkler r event rain w event road wet d event road dangerous Pearl models causal mechanisms system structural equations r c s w r d w mechanisms specify cloudy Pearl treats c external variable error
term solutions corresponding causal model M obtained solving Equations c c sprinkler observed according Equations sunny intervention manually switching sprinkler represented modifying causal mechanism sprinkler regardless weather captured following structural equations r c s w r d w c considered external variable solutions corresponding causal model Ms represent possible states world intervention switching sprinkler manually Note s true independently weather philosophy idea causal explanations given rules form ϕ causes ψ established instance René Descartes ciples Philosophy translation Miller Miller makes abductive logic programming natural target formalism senting causal knowledge abductive logic program consists set rules P set propositions called abducibles
Similar external variables Pearl causal models abducibles independently assumed true false Rückschloß Weitkämper apply Clark completion relate probabilistic logic programming Pearl theory causality ring Pearl notion intervention Similarly abductive logic programs translated causal models enabling principled treatment interventions Example situation Example gives rise rules r s w w d c true false considered abducible c Clark completion yields models resulting abductive logic program exactly solutions causal model M Example Causal Modeling Abductive Logic Programs Intervening switching sprinkler manually results rules r s w w d Clark completion yields models resulting abductive logic program coincide solutions causal model Ms Example Notably
approach provides informal semantics abductive logic programs rule h bn interpreted causes h translating abductive logic programs causal models lead counterintuitive results cyclic causal relations involved Example Assume neighboring houses Let fi denote event House hi fire sfi event House hi starts burning reasonable assume fire House leads fire House vice versa situation modeled cyclic abductive logic program P consisting abducibles rules Proceeding Example relating P causal model program admits model ω ω houses fire house actually started burn contradicts intuition houses spontaneously catch fire merely affect work extends applicability Pearl ideas stratified abductive logic programs involve cyclic causal relations
Building prior work Bochman Eelink et al connects logic programming Pearl causal models philosophical Principles stated Theorem lates abductive logic programs causal systems Eelink et al relating stable model semantics Principles Theorem shows Principle implies Finally Theorem establishes stratified programs satisfy Principle Overall results stratified abductive logic programs stable model semantics conform principles supporting use causal modeling prediction effects external interventions Principle Causal Foundation Causal explanations originate nal premises explanations lie scope given model Principle Natural Necessity given existence cause fect necessarily follow Thomas Aquinas Summa Contra Gentiles translation Anderson Principle Sufficient Causation reason effect cause Gottfried Wilhelm Leibniz Truths
translation Loemker Rückschloß Weitkämper Principle Causal Irrelevance Unobserved effects influence beliefs Principle Non Interference impact interventions restricted causal direction causes effects Preliminaries present material contribution builds Pearl causal models abductive logic programs logical theories causality developed Bochman Eelink et al Pearl Causal Models Pearl suggests modeling causal relationships deterministic functions leads following definition structural causal models Definition Causal Model structural causal model M internal variables V external variables U system equations includes structural equation form X internal variable X parents X set internal variables error term subset external variables fX function maps assignment values value solution ω structural causal model
M assignment values variables V satisfies structural equations Notation parents error terms internal able V typically evident defining function fV Accordingly omit explicit references Pa Error work Example causal model M Example external variables U c internal variables V r s w d Structural Equations solutions c r s w d c r s w d artificial intelligence causal models particularly valuable represent external interventions explained Chapter Pearl key idea construct modified model incorporates minimal changes structural equations required enforce external intervention Definition Modified Causal Model Fix causal model Let subset internal variables value assignment modified model submodel Mi model
obtained M replacing able X structural equation X X Causal Modeling Abductive Logic Programs Notation Let V Boolean internal variable structural causal model case write MV MV MV Example causal model Ms Example modified model sponding value assignment s following solutions c r s w g c r s w g represent possible states system manually switching sprinkler Example actions force variable causal model new value Pearl emphasizes submodels Mi typically arise forming actions set certain variables specific values process formalized introduction operator Abductive Logic Programming work adopts standard notation propositions propositional formulas structures structure identified set propositions true term
world refers consistent set literals maximal respect inclusion worlds ω correspondence structures terms interchangeably Example structure ω propositional alphabet P c r s w d Example complete state description Example identify structure set propositions c r w d Fix propositional alphabet Logic programs consist rules clause Definition Clauses Logic Programs normal clause C formula form h denote h h bn h atom referred head clause C bn finite set literals known body C h denotes C h calls C fact logic program P finite set clauses dependence graph P directed graph alphabet P defined follows edge p exists clause C
q denoted edge p negative exists clause C q Similarly edge p positive exists clause C q p Note edge negative positive simultaneously cycle finite alternating sequence nodes edges form q begins ends node program P acyclic dependence graph contains cycle stratified dependence graph contain cycle negative edge Rückschloß Weitkämper Clark translates acyclic programs P propositional formulas stating valid proposition model ω needs reason support ω Definition Clark Completion Supported Model Semantics Let P logic program Clark completion P set formulas supported model P model Clark completion ω Example Rules define logic program P unique supported model given Example Rules
yield stratified program supported model semantics formally defined general propositional logic programs associates unique possibly set models program P yields counterintuitive results cyclic programs Example Rules define logic program P supported models houses fire initial cause start burning contradicting everyday intuition general potentially cyclic programs Gelfond Lifschitz argue stable model semantics provides appropriate notion model adopting common formulation reducts work lows equivalent definition based unfounded sets originally introduced Saccà Zaniolo listed Definition D Lifschitz Definition Unfounded Sets Stable Models Let ω ture non subset set atoms true ω P logic program unfounded set respect ω P p rule P head
p body literal b true ω belongs structure ω stable model P satisfies clause P interpreted propositional formula unfounded set respect ω Example Example stable model intended set stable unfounded respect program Gelfond Lifschitz prove following results Theorem Supported Stable Models stable model logic program supported model Theorem Stable Models Stratified Programs stratified gram unique stable model Causal Modeling Abductive Logic Programs Abductive logic programming identified distinct branch logic programming Kakas Mancarella goal providing planation given set observations terms called abducibles Definition Abductive Logic Program integrity constraint IC expression form written finite set literals abductive logic program triplet P P
IC consisting logic program P finite set integrity constraints IC set ducibles abducible u head clause Finally P acyclic stratified underlying logic program P context databases integrity constraints serve sanity checks data Chapter setting represent observations ensure knowledge encoded causal rules program P explanations consistent given observations Example Let P denote logic program Example expect causal knowledge Example insufficient explain cloudy declare c abducible define c observe sprinkler leading integrity constraint IC yields abductive logic program P P IC Lastly recall semantics abductive logic program Definition Models Abductive Logic Programs stable ported model ω abductive logic program P P IC
satisfies integrity constraints IC ω IC meaning ω IC stable supported model program P set ϵ called explanation ω context program P tent model choice abducibles Example abductive logic program Example stable ported models Example explanations c respectively consistent observation sprinkler expressed integrity constraint supported model abductive logic program P Example Following lines Rückschloß Weitkämper connect bilistic logic programming Pearl theory causality realize abductive logic programs integrity constraints supported model tics rise causal models transferring notion intervention Definition CM Semantics Let P P abductive logic gram integrity constraints causal model semantics P causal model given external variables internal ables P
structural equations p p Rückschloß Weitkämper Note construction structure ω solution supported model represent intervention forcing atoms attain values according assignment modified abductive logic gram Pi Pi Ai obtained P modifications Remove clauses C P Add fact p Pi p Note construction Example Example Rules correspond modified program Ps corresponds assignement s Bochman Logical Theory Causality Causal Systems verify stable model semantics causally meaningful work builds work Bochman Eelink et al rely idea causal knowledge expressed form rules Definition Causal Rules Causal Theories literal causal rule R expression form denoted bn bn l literals cause l effect Informally R means
causes addition l atom rule R atomic default rule causal rule form l causal theory set causal rules called atomic contains atomic causal rules Example Example gives rise following causal theory default rule expresses explanation required House burning House assumed burn nation given default rules included truth value chosen freely Bochman extends rules causal theory explainability relation Definition Let causal theory binary relation plainability defined inductively causal rules follows λ λ Causal rules λ λ Literal Monotonicity λ λ Literal Cut p propositions p literals Literal Contradiction find λ λ explains Causal Modeling Abductive Logic Programs Remark Bochman initially allows
causal rules form ϕ plainability relations form ϕ ψ ϕ ψ arbitrary formulas argues explainability satisfies properties material plication reflexivity ϕ necessarily hold Given causal theory sense Definition work restricts attention explainability relations characterized Definition Theorem Bochman provides basis characterization Example Example find causal theory explicitly assert defaults Bochman semantics causal theories grounded Principles Definition Causal World Semantics causal world causal theory world ω literal l following formalization Principles hold Formalization Principle ω l Formalization Principle l ω causal world semantics set causal worlds Bochman gives following alternative characterisation causal worlds ω causal theory Definition Completion Causal Theories completion causal
theory set formulas l literal Theorem Completion Causal Theories Theorem causal world semantics causal theory set models completion ω world ω Example Example causal world ω tradicts everyday causal reasoning explained Example Note ω causal world framework causal theories allows cyclic explanations Example avoid cyclic explanations Example Eelink et al extend Bochman causal theories accommodate set external premises E require explanation Motivated ideas Aristotles rior Analytics additionally apply Principle leads set causal systems Definition Causal System causal system CS E O sists causal theory causal knowledge CS set literals E called external premises CS set formulas O called vations CS
causal system CS observations O Rückschloß Weitkämper causal system CS observes causal system CS applies default negation negative literal p external premise external premise effect causal rule system CS atomic atomic causal theory causal theory l called explanatory closure CS causally founded explanation explanation λ l λ Causally founded explanations formalize Principle Formalization Principle explanation causally founded causally founded world ω world ω O literal l following formalizations Principles satisfied Formalization Principle exists causally founded explanation ω l l Formalization Principle l exists causally founded explanation ω Example Example gives rise causal system default negation observations defined CS E atomic
causal theory E sfi set external premises explanatory closure CS coincides causal theory Example Note cyclic explanations Example causally founded world ω Example causally founded Problem Statement Example shows abductive logic programming causal model semantics yield counterintuitive results presence cyclic causal tionships perspective logic programming issues typically addressed applying stable model semantics Gelfond Lifschitz remains open question approach admits causally meaningful interpretation accounts interventions Main Result section fix propositional alphabet begin ducing Bochman transformation identifies abductive logic programs causal systems featuring default negation defined Definition Informally Bochman transformation interprets clauses h causes h treats abducibles external premises explanations lie given
model regards integrity constraints observations Causal Modeling Abductive Logic Programs Definition Bochman Transformation Bochman tion abductive logic program P P IC causal system fault negation E O C E p O IC Example Let P abductive logic program Example causal system Example Bochman transformation Let P P IC abductive logic program stable models P correspond causally founded worlds Bochman mation E O formalizations Definition supports stable model semantics follows Principles begin relating unfounded sets explainability causal theories Definition Internal External Explanations Let atomic causal theory ω model propositional theory obtained reading causal rules logical implications set positive literals true ω
external explanation expression form λ l λ set literals true ω belong internal explanation expression form λ external Let ω causally founded world Bochman transformation Definition ensures subset atoms true ω unfounded respect program extending underlying logic program P rule λ internal explanation λ Lemma Let ω Definition rule λ corresponds internal explanation λ tion internal Proof Definition follows rules iterated applications literal cut literal monotonicity literal contradiction suffices long rules premises inference rules internal consequence use notation Definition literal monotonicity statement clear λ set literals true ω subset λ ω structure q true ω literals contradiction yield external explanation
remains consider literal cut Assume contradiction λ external despite premises λ internal λ set literals true ω λ premises internal atom l true ω contradicts fact ω model propositional theory corresponding rules note literal cut monotonicity contradiction true propositional logic triple arrow read logical implication λ obtained rules axioms ω model λ λ l yielding desired contradiction Rückschloß Weitkämper Lemma hand prove result Theorem Bochman Transformation Bochman transformation bijection abductive logic programs causal systems default negation abductive logic program P P IC stable model ω ω causally founded world respect Bochman tion E O Proof construction Bochman transformation bijection ductive
logic programs causal systems default negation tegrity constraints carried unchanged Bochman transformation observations assume loss generality P integrity constraints stable model P causal founded world E let ω stable model need literals l ω l l Note rules precisely clauses underlying logic program P Clark completion P coincides completion explanatory closure stable model supported model structure ω model completion set Λ literals introduce notation indicate set literals l Λ Theorem states ω literal monotonicity remains ω words ω Note negated literals E set positive literals unfounded set Assume unfounded p clause p bn bn ω implies ω bi n n
fold iteration literal cut ω p p turn contradicts p concludes proof stable models contain non unfounded sets atoms Overall shown ω ω causally founded world turn converse direction demonstrating causally founded world stable model Theorem causally founded world model propositional theory corresponding clauses remains ω unfounded sets atoms respect P Assume set ω causally founded world obtain ω p p abducible atom true ω corresponds fact P unfounded set disjoint ω p external Lemma implies rules external unfounded subset ω concludes proof ω stable model P claimed Causal Modeling Abductive Logic Programs According Theorem Principles izations Definition entail causal
interpretation abductive logic programming necessarily yields stable model semantics Gelfond schitz raises question abductive logic program admits causal interpretation Example Let e denote event farmer ecological h event hot let s denote pests survive weather p pests field t farmer applies toxin Assume pests survive hot toxin applied pests remain toxin applied pests present farmer ecological relations define abductive logic program P abducibles h e rules P t p s s program P stable model explanation ϵ h concludes causal direction hot farmer ecological clearly contradicts everyday intuition argue represent causal knowledge abductive logic program satisfy Principle explored Williamson context
Bayesian networks abductive logic programming interpret Principle following semantic constraint Formalization Principle Let P P IC consistent abductive logic program set S propositions let P S denote set propositions q descendants dependency graph G proposition Finally denote P S program consisting clauses C P satisfies Principle set S P P S structure ω program P S ω P S stable model possible falsify ω P Remark Williamson proposes Principle context maximum entropy weakening Markov assumption Bayesian networks ingly formalization viewed deterministic analogue Markov assumption Example program P Example satisfy Principle argue abductive logic program P satisfying Principle formalization admits causal
interpretation stable model semantics raises question P predict effect external interventions According Pearl joint act observing intervening leads counterfactual reasoning lies scope contribution fore restrict interest programs integrity constraints argue admit meaningful representation interventions Principle holds following result shows Principle implies Principle Rückschloß Weitkämper Theorem Let P P abductive logic program integrity constraints satisfies Principle let assigment set sitions S Define P S S P S C S P S P S following equivalent ω S ω S stable model ω P ω S reduct ω ω S stable model P ω S ωi S stable model ωi Pi ω
S reduct ωi Proof proof rests splitting lemma set propositions S denote P S Let PS set clauses heads S write set clauses heads Finally set Let ω world abuse notation abductive logic program Q Q B denote logic program Q equivalence stable models P S precisely reducts stable models Pi Note P S P S consider splitting P S PS P S splitting intervention propositions S ancestors reduct stable model Pi P S stable model P S P direction let ω stable model P S P S P S ω consists facts clearly stable model assumption program P S
ωS P S ωS stable model splitting lemma stable model Pi extends ω turn equivalence Note S P S splitting reduct stable model P P S stable model P let ω stable model P need ω extended stable model Denote P S note closed predecessors clearly closed successors employ splitting P P Note vocabularies P disjoint head occurring P successor proposition lie P closed predecessors vocabularies logic programs disjoint stable models union precisely unions stable models P consistent P stable model stable model extends stable model P result follows immediately Formalization Finally obtain stratified abductive logic programs stable model semantics
satisfy Principle Theorem stratified abductive logic program satisfies Principle Proof P P IC stratified P S ω S fore P S ω precisely stable model Causal Modeling Abductive Logic Programs Conclusion Related work Let P P IC abductive logic program work proposes causal interpretation P reads clause h bn causes h interprets abducibles external premises explanations given model treats integrity constraints IC observations Theorem formalizations Definition shows ciples entail stable model semantics Example illustrates general programs violate ple Theorem confirms stratified programs respect principle porting causal interpretability stable model semantics absence observations IC Theorem shows Principle implies stratified programs integrity
constraints support able predictions interventions authors explored relation causal logic logic programming knowledge earliest results appeared context causally enriched versions situation calculus McCain Lin Wang translate causal constraints languages disjunctive logic programs classical negation McCain transformation extended broader class causal theories including order ones Ferraris et al line work relies classical negation departing standard framework negation failure aim causal theories executable provide causal semantics logic programs Conversely authors investigated logic programs admit causal interpretation includes work Giunchiglia et al Bochman stable model semantics translate logic gramming clause form c causal rule V Compared Bochman transformation Definition yields rule formulation difficult
interpret particular noted Eelink et al use embedded logical implication causal reasoning framework far transparent use ordinary propositions formulation implies logic programs correspond causal rules highly specific syntactic form perspective logic programs express causal dependencies positive atoms severely limit causal expressiveness knowledge works address feasibility modeling interventions consider Principles Bochman transformation Definition maps facts rules form interpretation remains open Remark Future work investigate suitable causal readings rules identify class programs satisfying Principle extend framework disjunctive grams work addresses program meaningfully represent effect interventions remains explored program tains sufficient information represent particular interventions Principle appropriately formalized fact equivalent Principle Rückschloß Weitkämper
References Anderson Summa Contra Gentiles Book Creation University Notre Dame Press Arif MacNeil Applying structural causal model work observational causal inference ecology Ecological Monographs Bochman causal logic logic programming Dubois Welty Williams eds Principles Knowledge Representation Reasoning ceedings Ninth International Conference pp AAAI Press Bochman Explanatory Nonmonotonic Reasoning World Scientific Bochman Logical Theory Causality MIT Press Chomicki Saake eds Logics databases information systems Kluwer Academic Publishers USA Clark Negation failure Logic Data Bases pp Springer Boston MA Denecker Kakas Abduction logic programming tional Logic Logic Programming pp Springer Eelink Rückschloß Weitkämper artificial intelligence leads knowledge inquiry inspired Aristotle posterior analytics
Fages Consistency Clark completion existence stable models ods Logic Computer Science Ferraris Lee Lierler Lifschitz Yang Representing order causal theories logic programs Theory Practice Logic Programming Gao Zheng Wang Feng Li Causal inference mender systems survey future directions ACM Trans Inf Syst Gelfond Lifschitz stable model semantics logic programming Proceedings International Logic Programming Conference Symposium pp MIT Press Giunchiglia Lee Lifschitz McCain Turner Nonmonotonic causal theories Artificial Intelligence Giunchiglia Lifschitz action language based causal explanation Preliminary report Mostow Rich eds Proceedings Fifteenth National Conference Artificial Intelligence AAAI pp AAAI Press MIT Press Kakas Mancarella Generalized stable models semantics abduction European
Conference Artificial Intelligence ECAI pp Leibniz truths Philosophical Papers Letters pp Springer Netherlands Causal Modeling Abductive Logic Programs Lifschitz Thirteen definitions stable model Blass Dershowitz Reisig eds Fields Logic Computation pp Springer Lifschitz Turner Splitting logic program Hentenryck ed Logic Programming Proceedings Eleventh International Conference Logic Programming Santa Marherita Ligure Italy June pp MIT Press Lin Embracing causality specifying indirect effects actions Proceedings Fourteenth International Joint Conference Artificial telligence IJCAI Montréal Québec Canada August umes pp Morgan Kaufmann Lin Wang causal theories logic programs Gelfond Leone Pfeifer eds Logic Programming tonic Reasoning International Conference pp Springer McCain Causality commonsense reasoning
actions thesis versity Texas Austin McCain Turner Causal theories action change Kuipers Webber eds Proceedings Fourteenth National Conference ficial Intelligence AAAI pp AAAI Press MIT Press Miller Miller René Descartes Principles Philosophy Springer Dordrecht Pearl Causality Cambridge University Press Rückschloß Weitkämper Exploiting power pearl causality probabilistic logic programming Proceedings International Conference Logic Programming Workshops CEUR Workshop Proceedings vol Saccà Zaniolo Stable models non determinism logic programs negation Proceedings Ninth ACM SIGACT SIGMOD SIGART Symposium Principles Database Systems pp ACM Press Williamson Foundations Bayesian Networks pp Springer lands Wu Peng Li Zhang Sun Li Qian Liu Guo Causal inference medical domain
survey Applied ligence
math LO Jul INTERLEAVING LOGIC COUNTING JOHAN VAN BENTHEM THOMAS ICARD Abstract Reasoning quantifier expressions natural language combines logical arithmetical features transcending strict divides qualitative quantitative topic cooperation styles occurs common linguistic usage extension broader practice natural language plus grassroots mathematics begin brief review FO order logic counting operators cardinality comparisons system known high complexity drowns finer aspects combination logic counting small fragment represent numerical syllogisms basic reasoning comparative size monadic order logic counting MFO provide normal forms allow axiomatization determine arithmetical notions defined finite infinite models conversely discuss logical notions defined purely arithmetical ones sort logics induced investigate series
strengthenings MFO normal form methods monadic second order version close precise sense additive Presburger Arithmetic versions natural device tuple counting Diophantine equations making logic undecidable define system ML combines basic modal logic binary accessibility relations counting needed formulate ubiquitous reasoning patterns Pigeonhole Principle prove decidability ML provide new kind bisimulation matching expressive power language complement fragment approach pursued discuss ways lowering complexity FO changing semantics counting natural ways approach replaces cardinalities abstract motivated values mass mereological aggregating notions second approach keeps cardinalities generalizes meaning counting work models allow dependencies variables Finally return starting point natural language confronting architecture formal systems
linguistic quantifier vocabulary syntax natural reasoning modules monotonicity calculus addition encounters formal semantics discuss role counting semantic evaluation procedures quantifier expressions determine instance binary quantifiers computable finite semantic automata conclude general thoughts glements logic counting formal systems rethinking qualitative quantitative divide connecting analysis empirical findings cognitive science Date July corrected version Interleaving Logic Counting article appeared Bulletin Symbolic Logic pp Contents Introduction Inference Computing Order Logic Counting Counting Logic Logic Counting Finite Models Fragments L Monadic Order Counting Logic Core Principles Normal Forms Questions Definability Questions Axiomatization Monadic Second Order Counting Logic Finitary Case Infinitary Case Addendum Notions Definability Counting
Sequences Diophantine Inequalities Normal Forms Second Order Extensions Infinitary Counting Alternative Route Explicit Arithmetical Operators Addition Multiplication Arithmetical Operations Interim Summary Modal Logic Binary Relations Language Semantics Basic Model Theory Bisimulation Normal Forms ML Language Extensions Generalizing Counting Semantics Counting Probability Proportionality Mass Weight Abstract Values Non classical Logics Embedding Multisorted FO Generalized Dependence Semantics Generalized Quantifiers Natural Language Quantifier Expressions Logical Semantics Linguistic Vocabulary Varieties Monotonicity Reasoning Dynamic Modalities Semantic Automata Cognitive Questions Conclusion Acknowledgements References Appendix Related Work Logic Counting Appendix Infinity Quantifer Monadic Second Order Logic Appendix Cardinal Arithmetic Quantifier Elimination Separation Appendix Finite Automata Quantifier Recognition
Procedures Appendix Logical Syntax Counting Introduction Inference Computing archetypal logical inference basic quantifier B B C conclude C slightly modified premises natural language B B C time need think little bit conclude C extra bit thought involves considering possible exceptions generally counting fact term quantifier suggests quantities semantics quantifier expressions logic linguistics involves numbers emphasis permutation invariance abstracts away feature predicates size mix logic counting absolute numbers extends size comparisons B B C safely draw conclusion C similar simple inference patterns govern explicitly comparative expressions B C valid reasoning patterns comparatives challenging following inference require drawing Venn diagram B C
B C C B echoes mathematical Triangle Inequality underlying metric geometry Numerical comparisons natural language occur proportions happens relative sense B comparing numbers Bs number Bs overall defined precisely running example later Qualitative logical analyses seen replacing quantitative theories basic qualitative ones instance foundations probability measurement theory illuminating success measured representation theorems historically logic quantitative reasoning instance probability went pioneering work Bolzano Boole hard Boole propositional logic qualitative basic form binary arithmetic way making logical inference form counting sense divide arose time Frege logicism insisted logical notions come arithmetical ones constructed sure reductionist program yielded fundamental notions results owe lot
modern logic arrival paper follow linguistic practice started treat logic counting taken realm numerical comparisons basic arithmetic par follows linguistic practice broad sense including ubiquitous forms reasoning called grassroots mathematics pure natural language inference typical example underlies following pattern farmers cows farmers number cows reader find difficult follow straightforward matter overt logical linguistic form Instead needed following Pigeonhole Principle puts n objects k boxes n k box contain objects k number cows owned runs n number farmers Pigeonhole Principle occurs elementary mathematics non trivial consequences applied imaginatively interest cognitive science benchmark reasoning ability including finding right representation problems Mercier et
al paper principle occur places determine position combined systems logic counting start investigation logic counting known combining standard system like order logic counting syntax cardinality comparisons leads system FO high complexity purpose view illuminating quick look FO properties start work exploring simple combinations logic counting complex systems presentation follows mainstream practice offering sequence formal systems increasing expressive strength prove results systems demonstrate precise mixture logic counting end paper return naturally occurring practice mixed qualitative quantitative reasoning started linking Generalized Quantifier Theory natural language touching empirical issues cognitive science Finally sequence appendices broaden context point entanglements logic counting ubiquity phenomenon True
understanding logical systems work involves numbers counting manipulating syntax proofs formula induction semantically use numerical invariants Ehrenfeucht games ways looking topics results presented paper Simple combinations logic counting seen fragments richer logics generalized quantifiers Barwise Feferman Peters Westerståhl sense looking fine structure fragments known systems mathematical logic interplay logic counting long studied computational logic Otto Schweikardt Accordingly themes results literature theoretical computer science appear places paper added appendix references wide hopefully representative swath preceding literature overview capacity background technical main novelty paper series simple combined systems define study contribution empirical perspective adding connections natural language cognition addition technical results logic
counting stance logic computation cognition fruitful worth pursuing section present higher end combination logic counting pass main themes detail lower end systems focus analysis core paper Order Logic Counting obvious starting point consider counting operator standard order logic allowing count number objects satisfying given formula x order variable φ order formula order model M variable assignment s term xφ denotes cardinality set x satisfying φ s M sx d Count terms denote cardinal numbers kinds assertions want cardinal numbers formalize interesting reasoning counting start basic fundamental capacity comparison inductively define count comparison formulas xφ obvious interpretation according M s iff
s language L logical system FO system studied thoroughly natural construe FO order logic generalized quantifier known literature Rescher quantifier Herre et al Otto related extension considered Rescher philosophical literature called Frege quantifier Antonelli known quantifiers called Härtig quantifier Chang quantifier easily definable FO Peters Westerståhl convenient abbreviate Härtig quantifier xφ xφ likewise abbreviate xφ xφ Typical extensions order logic following Proposition FO fails compact lacks Löwenheim Skølem property cardinality ℵω Proof note infinity quantifier easily definable FO y y substitution y defined usual force domain size ℵk simply stating instance k predicate symbols Pk Compactness fails easily abbreviating V
xi xj abbreviate xn set n ω unsatisfiable finitely satisfiable stronger FO ordinary FO note following Fact enforce FO binary relation R order order type ω Proof Let σ statement R serial strict total order serial irreflexive transitive total conjoin σ statement x saying element finitely R predecessors follows validity problem FO arithmetical fact hard allow embedding comparisons satisfiability problem comparison amounts existence injective function Fact set validities FO embedded terms complete general case situation worse Herre et al showed following result order logic Härtig Theorem Herre et al set validities FO FO clearly brings potent combination logical expressive power
explicit count comparison degree tease apart separate contributions logic counting rich setting Specifically comparisons add counting repertoire native order logic vice versa logic extract counting begin second question Counting Logic Let restrict attention small fragment language L described Given variables Var predicate symbols Pred allow types atomic formulas operation building complex formulas Let generated grammar φ xn x y xφ Aside predication variable inequality compare cardinalities observation Boolean implication defined x occurs free φ ψ ψ xφ Boolean negation defined abbreviation x cf Frege x variable occur free φ define recover Boolean connective variable equality respect count comparison incorporates Boolean
structure familiar Boolean laws emerge principles count comparisons instance pattern φ encoded simply Going order quantification expressible xφ brings FO define infinity quantifier dual austere atomic primitives count comparisons encode significant logic provided course allow iteration comparisons comparisons Remark Extended logical vocabulary Counting define non order quantifiers considered logical extended sense example binary quantifier φ ψ definable closer order fact hardness result Theorem holds negationless fragment FO Mayer given computable reduction negationless fragment fragment negation logic counting suggests different kinds universal quantifier depending extend standard meaning finite sets infinite ones option dual existential quantifier defined expresses exceptionless universal quantification interesting
weaker variants xφ says set objects satisfying φ size universe possible exceptions smaller size version quantifier elegant mathematical properties interesting measure theoretic applications Steinhorn Remark Non classical Logics addition options qua expressive power counting offers options deductive power definitions reconstruct classical logic comparisons FO way inherently classical instead naturally extract non classical connectives route implication redefine negation terms arbitrary predicate let stand sentence x free φ lose direction law double negation direction remains valid retain converse contraposition φ losing contraposition resulting logic intuitionistic flavor worth determining exactly dramatic route non classical logics change semantics terms altogether explore route Logic Counting
pure FO capable encoding facts counting arithmetic far extensive mentioned order logic define simple counting quantifiers like order logic means counting syntax formula expressing n objects satisfying given condition achieves concatenating n existential quantifiers adding n conjuncts Basic arithmetic principles like m follow elementary logical patterns like distribution existential quantification conjunction applied requisite number times m times style counting syntax produces case case formulation Pigeonhole Example Suppose k monadic predicate symbols Pk let n says k predicates include n objects include objects schema course valid choice k n examples counting syntax subsequent sections especially Remark Appendix E Remark fact FO
count syntax reverberates interesting ways consider finite variable fragments FO variable fragment known bounded finite model property Mortimer turn establishes decidability fragment counting quantifiers easily enforce infinite models y y y simply Pigeonhole Principle natural encoding propositional logic complexity theorists interested lower bounds lengths proofs instances principle different proof systems Cook Reckhow Krajíček language fact decidable Grädel et al like variable fragment counting satisfiability problem NExpTime complete Pratt Hartmann complexity analysis system extensions Kieroński et al reveals arithmetical content appear analyses plain variable fragment witness connections integer programming semi linear sets Finite Models natural consider related system language interpretations restricted
finite models system FOϕ L extends language order logic Trakhtenbrot Theorem tells validities FOϕ computably enumerable Nonetheless FOϕ variations intensely studied literature finite model theory Otto Schweikardt summaries relevant work example distinctive issues come finitary setting ask asymptotic probabilities formulas FOϕ finite structures shown Grumbach Tollu FO Härtig quantifier fact possesses zero law pure FO possession zero law commonly interpreted evidence logic formalize non trivial counting taken justification choice comparison equality primitive FOϕ lacks zero law asymptotic probability conjectured Grumbach Tollu extension FOϕ nonetheless possesses limit law limits rational numbers purposes finite model theory descriptive complexity authors motivated consider proper
extensions language L notable example fixed point logic counting Cai et al purpose different aim isolate weaker fragments language reveal subtle interplay logic counting elementary reasoning pinpointing differences commonalities finitary infinitary patterns counting Fragments L order logic counting natural starting point exploring subject observations invite search natural fragments weaker variants FO desirable example identify decidable fragments L perspective noteworthy familiar ways taming complexity effective example finite variable fragments result decidability shown Grädel et al variable fragment FO undecidable complete observe reduction complexity compared Theorem variable fragment FOϕ undecidable Evidently significant source complexity potent combination counting arbitrary relational reasoning witness Lemma
undecidability proof Grädel et al variable fragment crucially involves counting successors binary relations dramatic route tamer syllogistic propositional fragment Moss Ding et al instance let language propositional logic count comparisons resulting system PL easily shown decidable follow immediately results route eliminates relational reasoning order quantification alternative route relational reasoning retain order quantification monadic fragment L allow counting Language Logical System Typical Expression L FO y x u x u v y y z MSO x x MFO y Lml ML PL Table hierarchy counting languages logics covered logical system L version Lϕ restrict finite models systems terms denote natural numbers
relations preserves counting content FO Observe example definition infinity quantifier reconstruction logical connectives count comparisons depend way arity available predicates use MFO monadic order logic counting base system explore richer combinations context consider adding second order quantification system MSO ability count individuals sequences individuals systems course counting relations common natural explore tractable modal fragment L Lml way taming interaction counting quantification relational reasoning summary appears Table Following work consider different route altogether changing semantics L Relaxing logical interpretation relativizing sets admissible variable assignments cf Németi numerical content terms results systems retain character FO gaining tractability Monadic Order Counting Logic system
MFO monadic order logic identity cardinality comparisons restricted expressive power captures good deal natural reasoning mentioned Introduction easy numerical syllogisms represented simple comparative reasoning quantifiers like MFO represent earlier complex inference B C x B C x C B x underlying Venn diagram style reasoning analyzed generally basic linguistic inference repertoire MFO represent called grassroots mathematics Note instance Example encoding Pigeonhole Principle involved monadic predicates fact need MFO express natural infinitary generalization stating infinitely objects finitely disjoint boxes pigeonholes result box having infinitely objects look systematically monadic counting logic express Suppose Pred Pn finite list possible state descriptions Pred form
V j extension state description Si model region easily state count comparisons regions count comparison statement succinctly written numerical variables replacing cardinalities si Si pairwise disjoint generally encode constraints involving sums cardinalities regions disjunctions state descriptions instance sentence like x W W j encodes typical linear inequality sums variables X si X j sj closing Booleans course express equality strict inequality versions restricting finite models resulting logical system MFOϕ case solutions natural numbers allow models arbitrary cardinality solutions involve infinite cardinal numbers system MFO express MFOϕ MFO simple linear inequalities seen instructive example formula defining infinity quantifier encoding state description
S essentially inequality statement s use individual variables instance general pattern relevant finite case present section apply equally MFOϕ MFO consider non overlapping sets state descriptions respective cardinalities label encode inequalities like X si X j sj k X si X j sj instance express assert existence k distinct variables y satisfy removing elements regions spanned results cardinality regions spanned x y shorthand W similarly P x yi x yi Figure visualization formula expressing number P points blue exactly greater numbers non P points green k size extracted set P points size y expressed replacing equality strict inequality fact k
variables y addition variable x count comparisons encode constant simply taking variables y adding regions spanned Figure visualization x y x y effectively stating k words argument extends inequality statements Core Principles systems MFOϕ MFO evidently invariant automorphisms monadic setting automorphisms precisely maps permute elements region points satisfy given state description indistinguishable means property holds point region holds point region theme permutation invariance characteristic counting return discuss generalized quantifiers demonstrated use individual variables essentially allows manipulating regions removing adding points correspondingly state general invariance principle Fix variables y fixed finite set P predicate letters let specify state description x variables
y formula φ predicates P x satisfying αy φ x satisfying αy satisfies φ Codified general invariance principle INV α satisfy φ specified α count formula reference φ redundant fact INV follows stronger statement stronger provided admit infinite models x useful observation terms xφ following Extraposition Principle SUB uncaptured subformulas Subformulas φ involve x contribute fine grained information term denotation free variables ψ bound variables xφ following valid ψ SUB γ result substituting β occurrence γ α Normal Forms principles recorded INV SUB basic propositional reasoning elementary principles allow derivation normal form result works uniformly MFOϕ MFO step formula equivalent
embedded quantifiers terms replaced stacks unembedded existential quantifiers dramatic departure relational FO embedding non trivial Recall FO embedded count comparisons complete stark contrast Theorem Define depth recursion α atomic Generically monadic formula free variables y x written disjunctive normal form W x x specifies state descriptions y variables αy x previous subsection x formula general positive depth want formula x x x j x x equivalent embedded count comparisons inside x terms SUB subformulas outside count comparisons leaves x x x j x x analyze Let κk range formulas x k appeal INV equivalent formula κk αy x αy j
x Note traded level embedding existential quantifier x subformulas lower nesting count operators taken care inductive hypothesis αy αy j depth concludes argument Lemma formula equivalent count comparison subformula depth exactly Lemma main result section Theorem depth k sentence equivalent MFOϕ MFO disjunction conjunctions sentences specifying m m sums cardinalities state descriptions m Proof generally formula depth k predicates P free variables y yn equivalent disjunction ranges possible descriptions y σ complete description regions P specifying m m m notation denotes formula specifies description σ assumption claim disjunct variable assignments s M s M satisfies k description σ statement theorem
special case free variables n Example example disjunct predicate letter P formula inside existential quantifier Fig formula k free variables formula count comparison Note free variables means provided holds depth k formulas equivalent formulas satisfying proceed inducting k starting case depth formulas k free variables y yn critical case count comparison x x x SUB separate descriptions y obtain formula disjunct exactly disjuncts inside terms agree characterization variables routine check exhaustive check cases count comparison disjunct context asserts m m m disjunction comparisons fits form satisfied general normal forms fixed k n closed Boolean combinations need consider case depth k
n variables Lemma assume count comparison subformulas depth suffices consider existential quantification induction assume z z formula equivalent z z z remains seen form disjunct satisfying inductive assumption know s M s z z M satisfies k description σ M s z z variant s M z z establishes result Connection Integer Programming ordinary monadic order logic putting sentence normal form result significantly longer formula satisfiability problem monadic order logic variable fragment complete Lewis checking satisfiability normal forms NP monadic logic checking satisfiability normal form MFOϕ relatively low complexity fact complexity set types integer program solvability known decidable NP time
Borosh Treybig special case integer programming coefficients words special case inequalities like included Karp original list NP complete problems lower bound conclude satisfiability problem normal forms MFOϕ NP complete Questions Definability Theorem affords refined understanding numerical relations defined MFOϕ MFO T set state descriptions let denote sum cardinalities extensions M state descriptions M m m iff m M signifies M agree sentences depth k Theorem immediately gives Corollary M iff M initial example characterize precisely binary logical quantifiers definable MFOϕ proof discussion generalized quantifiers Theorem binary quantifiers definable MFOϕ correspond exactly expressible order theory includes standard logical quantifiers etc following
gives example statement expressed Fact twice Ps Qs expressed MFOϕ Proof Supposing sentence depth k light Corollary suffices k find M disagree statement M Define model M elements statement clearly holds define elements statement fails M second example consider natural rendering natural language expression taken refer number contextual threshold sophisticated uncommon reading cf Westerståhl Rett Qs P amounts comparison proportion Ps Qs proportion Ps overall symbolize x Fact Qs P expressed MFOϕ Proof k find models M disagree statement M suffices specify cardinalities regions model p q r s models let r k q p M let s let s
cases s p q r M M M r inequality fails return analysis natural language constructions Note Corollary derive undefinability results MFO Fact successor function infinite cardinals expressible MFO Proof models agree order cardinalities infinite definable sets stand relation Interpolation Failure consequence Theorem particularly simple normal form result letterless fragment fragment predicate symbols built atomic formulas fact normal forms identical monadic order logic infinity quantifier Carreiro et al Lemma letterless sentence equivalent MFO disjunction formulas having following forms restriction MFOϕ finite models simplifies include statements form consequence Proposition MFOϕ MFO enjoys interpolation property Proof Let formula true finite models sizes
Let formula x x finite models requires domain odd Evidently Let χ purported interpolant χ letterless Lemma implies disjunction sentences specified forms Furthermore entailed assume disjunct straightforward case analysis shows χ true models fixed size case entailed true finite models finite size onward case entail familiar way extending language guarantee interpolation allow second order quantification turn extension analyze reasoning content normal form analysis bit illustrate sort combined system counting logic Questions Axiomatization calculus valid reasoning suggested current systems basic monadic systems MFOϕ MFO locate kind separation components general logical principles allow normal form result Theorem b specific numerical reasoning solving
systems inequalities discuss component turn system MFOϕ allows finite domains general system MFO involves component dealing infinite sets remark end Step normal form principles underlying normal form result follows General validities propositional order predicate logic b general principles INV SUB highlighted earlier c linear order properties relation linearity Principle c case distinctions worth high lighting xφ COMP soundness COMP MFO depends axiom choice COMP equivalent axiom choice Hartogs Significantly generalized semantics discussed Section Principles b remain valid strong reasoning principle c naturally replaced pre order properties Step II result normal form analysis left satisfiability problem inequalities variables denote natural numbers
system solved effectively known Fourier Motzkin Algorithm Schrijver stage solved reasoning problem spirit paper having simple combination logic counting calculus uses logic reduce reasoning problem numerical elegantly solved terms precisely sort combination find natural insightful Remark Step III determine exact arithmetical principles drive Fourier Motzkin algorithm sketch algorithm works follows picks variable s long possible considers cases variable s occurs right inequalities system s dropped inequalities setting value suffice ii variable s occurs left inequalities inequalities s occurs dropped true end choosing suitably large value iii case s occurs left right inequalities groups inequalities form u v w s z
forms y s r s x forms sums follows u v y s gives u y t u v r s x gives u r v x end set variable free statements concrete natural numbers remains inspected immediately truth falsity step algorithm checked principles guarantee soundness representative illustrations steps involve evident principles inequalities symmetry associativity addition monotonicity inferences implication u u z Step involves equality v v step ii involves u v key step iii involves principles like equivalence z u u z addition principles implication u w u w final inspection step involves simple principles successor function think numbers
encoded unary format preceding observations mixed axiomatization system MFOϕ letting logic good reducing assertions normal form letting arithmetical component good solving equational problems involving numbers stage analysis reductions syntactic normal forms plus separate combinatorial analysis common practice logic quantifier elimination arguments specific division labor logic counting perfect fit methodological spirit paper general empirical reasoning practices started return combinations logic explicit arithmetic bit systematically natural explore road greater purity ask purely logical axiomatization purely numerical consider roads turn arithmetical steps Fourier Motzkin algorithm replaced illuminating purely logical proof system goes routine transcription interesting conceptual issue variable elimination step algorithm typically forms
sums single variables step iii sums direct interpretation logical systems particular defining expression logical languages Fact ways dealing problem instance adding special inference rules Ding et al essentially axiomatizes slightly smaller system PL cf discussion Appendix inference rules seen expressing admissibility certain model constructions logic taking disjoint unions makes sense case definable given model M denote extension P disjoint union M aware obvious model constructions matching invariants needed MFO leave issue open problem Equally terms purity opposite ask purely numerical calculus systems restrict monadic fragment small sublanguage consisting predication variable inequality count comparison logical operators definable axiomatization possible principle example
following numerical claims suppressing individual variables chosen fresh capture basic principles propositional logic φ φ wants transcription independently motivated numerical system generates logic purely logical axiomatization leave providing illuminating purely numerical axiomatization systems open problem Infinite cardinalities general system MFO deal infinite ities makes difference principles producing normal forms changes subsequent phase solving equations key observation simple expression distinguishing infinite finite extensions s completely separate reasoning inequalities finite variables reasoning variables denoting infinite sets cf Ding et al systematically second order version MFO turn Monadic Second Order Counting Logic logical point view natural extension allow quantification unary predicates denoting subsets
domain resulting language finitary general systems MSOϕ MSO respectively immediate observation order language MFO count comparisons xφ definable Härtig quantifier xφ Peters Westerståhl second order quantification present providing definition straightforward powerful MSOϕ MSO comparison MFOϕ MFO question interest known Ackermann adding second order quantification monadic order logic increase expressive power time add quantification finite sets MFO equivalent monadic logic infinity quantifier Väänänen case equality Appendix B case including equality failure interpolation Proposition shows expect similar collapse adding monadic second order quantification counting extensions MFO saw MFO distinguish finite infinite effect automatically gain access quantification finite sets fact gain terms arithmetical
expressive power shown follows Example preview finite setting contrast MFOϕ Fact MSOϕ statement twice Ps Qs expressible essentially asserts existence set extension outside Q size Q P size union turns Example tip iceberg addition obviously teeing interpolants sense second order systems MSOϕ MSO fill gaps MFOϕ MFO systems enforce certain type inequality sums Eqns second order versions capable enforcing arbitrary linear constraints cardinalities involve addition numbers proceed precise finitary case infinitary Finitary Case saw normal forms MFOϕ correspond junctions sets inequality constraints class solvability problem NP complete general setting integer programming close correspondence sets linear inequalities quantifier free formulas Presburger
Arithmetic order logic addition natural numbers Oppen sets solutions inequalities equivalently assignments satisfying Presburger formulas free variables exactly semi linear sets Ginsburg Spanier generalization ultimately periodic sets numbers relevant definitions precise results Definition set V n ary vectors called linear system equations variables vn um constants bn m vn bn mum x exist values um v x solution V semi linear finite union linear sets Definition Suppose Sn state descriptions predicates P φ sentence predicates φ defines set V case model M M iff Remark notions definability sense analysis later systems Section alternative notion trace definability comparison Lemma semi linear
set definable MSOϕ Proof closes disjunction suffices linear set definable describe encode linear set form sentence words assert existence Sets Zi Zi j ai j ai j j Individuals zi bi bi ii Add conjuncts x j j p j l j p l xZi j j p iii Finally conjoin claim state description Si cardinality union Zi j p zi bi x j Zi j x zi l given j sets Zi j p correspond ai j copies variable uj individual variables zi l count constant base number bi numerical equalities stated iii guarantee variable right cardinality according conditions
specifies ii existentially quantifying variables resulting formula defines linear set sense want direction sentences fact define semi linear sets result note MSO possesses prenex normal form Lemma sentence predicates P equivalent MSO prenex form form Xn Xn order sentence treating Xn additional cates Qn second order quantifiers Proof Sketch argument usual prenex normal forms order logic case MFO soundness INV allows extract order second order quantifier scope case need consider order universal quantifier scoping directly second order quantifier point convert order universal quantifier universal second order quantifier restricted singleton sets formula Q second order quantifier equivalent x Theorem know Xn
normal form involving expressions m m cardinalities state descriptions P additional predicates Xn formulas easily seen semi linear linear semi linear sets closed Boolean combinations second order quantifiers distribute disjunction main goal following closure property second order quantifiers Lemma Let X predicate variable predicate letters predicate variables Suppose X defines linear set state descriptions X defines linear set descriptions Proof X linear assume defines solutions X linear define linear set equations projecting variable Specifically note state description k k k equivalent state descriptions Si Sj fact k equivalent Si new linear system variables follows k bi bj ai m aj
remains M X solution equations M X subset domain M sX X Treating X predicate constant model assumption gives solution state description k equivalent disjunction Si cardinality sum satisfy constraints state descriptions independent X means gives solution Suppose gives solution particular choices um need find set M sX X extensions disjoint define suffices identify subsets suppose k equivalent Si Si equivalent k Sj equivalent k let Bk subset size bi ai mum complement size bj aj mum possible simply sum numbers Finally let S Bk absorbing X language defining like M tuple gives solution choices um X easily follows M
sX X finally M X foregoing establishes Theorem numerical relations definable MSOϕ semi linear sets words MSOϕ expresses numerical relations Presburger Arithmetic Remark allow arbitrary second order quantification saw Lemma needed initial block existential second order quantifiers encode set fact sentence MSOϕ defines semi linear set demonstrates collapse MSOϕ purely existential fragment order case numerous undefinable results follow example Corollary expression definable MSOϕ Proof Adopting notation proof Fact constraint state descriptions r semi linear theory definability Presburger Arithmetic carries exactly MSOϕ thanks Theorem algorithmic means putting formula MSOϕ normal form finding suitable semi linear form decidability follows decidability Presburger Arithmetic
Corollary validity problem MSOϕ decidable Infinitary Case Allowing second order quantification increase expressive power initial system MFOϕ essentially amounts proper fragment Presburger Arithmetic MSOϕ gave precisely Presburger Arithmetic look system MSO models arbitrary cardinality immediate difference contrast MFO Fact successor function cardinal numbers easily expressed formula states cardinality strictly P cardinal arithmetic MSO encode case MSOϕ calibrate appeal additive order cardinal arithmetic Consider elementary theory structure addition cardinals numbers ℵω theory cardinal numbers order language binary function symbol addition Appendix C theory admits quantifier elimination provided augment language constants definable MSO functions relations s successor function greater relation equivalence modulo
k k Furthermore derive normal form result language Proposition order sentence equivalent structure disjunction conjunctions δ specifying ordinary order variables disjunct finite infinite δ description linear set finite variables ϕ description set infinite cardinals s indices infinite variables ι understood kind separation result finitary Presburger Arithmetic simply ordinary addition infinitary observe isomorphism s s sending k ℵk words additive structure cardinals ℵω amounts product aim MSO possesses normal forms Proposition statement form δ expressed note δ merely requires distinguishing finite infinite sets recall Eq definability linear set specified ϕ shown Lemma ι conjunction formulas types v v v ℵk v
ℵk noted successor expressible instance assert P cardinality simply stating P infinite infinite set smaller cardinality statement expressible exhausts definable MSO given Lemma remains observe sets closed projection existentially quantifying variables suppose formula Y Y predicate variable O predicate variables letters assume Y form Y Y Y analogously additive language Y describes state descriptions variables Y Y characterizes infinite state descriptions Y describes linear set need analyze Y Y Y replace Y formula specifying state description S finite iff S S finite according Y List finite state descriptions according Y Sk subformula Y defines linear set possible finite cardinalities bk
ak mum Suppose Si S finite S infinite constraint constraint infinite carving finite portion S size possible case simply drop equation Si Si S Sj S finite repeat argument version result traced Mostowski Tarski general notion product subsumes case Feferman Vaught combining equations single equation result set equations cardinalities state descriptions O asserted finite subformula Y represents constraints form v w v w v w infinite state descriptions O Y k fold successors state descriptions aleph numbers view isomorphism s s construe conjuncts describing relations natural numbers relations special case linear sets encoded run argument merge equations S S single
equation S provided infinite S S infinite equation remains S cardinality resulting formula general involve addition discussed Appendix C variables infinite eliminate explicit sums equivalences t v w v w Theorem definable relations cardinal numbers MSO exactly definable additive order logic effect shown reduce sentence additive order formula variable xi x corresponding state description satisfiable true decidability elementary theory Theorem Appendix C obtain Corollary validity problem MSO decidable Addendum Notions Definability preceding results systems MFO MSO adopted intuitive notion called direct definability arithmetical predicates formulas combined count logics Stated precisely recall notion works follows Definition Suppose Sn state descriptions predicates
P φ sentence predicates φ defines set V case model M M iff Adopting definition general consequences following observation holds logical systems considered paper order second order generally counting logic having decidable model checking problem Fact directly definable arithmetical predicates decidable Proof sketch Suppose φ defines n ary numerical predicate check given tuple mn numbers V suffices consider unique model cardinalities zones given state descriptions Sn zones assumption decidable φ true model notions definable motivated instance Definition Trace definability Let φ trace defines V case trace φ V tuple mn V model M M mi contrast direct definability trace definable
arithmetical predicate P holds finite model makes defining formula φ true non coding state descriptions define non subsets domain way crucial truth φ Given easy following logics larger class mentioned earlier Fact trace definable arithmetical predicates recursively enumerable distinction notions definability effect arithmetical definability results counting logics MFO MSO reason essentially order theory quantifier elimination unbounded existential quantifiers numbers replaced bounded ones suitable normal forms systems modulo equalities semilinear forms instance existentially quantifying semi linear predicate variables equivalent semi linear definition differently recursive enumerability reduces decidability additive arithmetic notions definability diverge richer counting logics involve multiplication quantifier elimination longer holds
turn extension counting logics multiplication enters Counting Sequences far considered base monadic system MFOϕ second order extension MSOϕ essentially restricted reasoning sums numbers theme carries setting infinite models MFO MSO previous systems involve unary variable binding operators count sets objects natural logical point view count sequences objects polyadic quantifiers ubiquitous natural language cf consider extension essentially moving sets products sets like understand additional arithmetical capacity affords Let order monadic language polyadic counting terms x xk sequence variables appear φ M s iff M sx d M sy d finite models let resulting system general case known polyadic counting order logic
expressive unary counting FOϕ Otto Example monadic fragment particularly dramatic shown following example Example Consider earlier Qs P defined repeated x express follows y y finite model term y gives product model total cardinality region P Q hold term right gives product cardinalities P Evidently incorporates reasoning multiplication x y P Q copy Q y x y Figure visualization formula expressing times number P points blue exactly number Q points squared red formula asserts blue lines equal number red lines simplified version Example general construction Lemma case pictured Example encode Pythagorean triples cardinalities state descriptions statement y multiplication comes taking
products addition example arises disjunction initial system MFOϕ examples involves different combination multiplication addition Example sentence expresses constraint y v x y u v u v x u v y Note use variables x y v x y simply want multiply cardinality P fact Q holds x y matter second consider triples points satisfying Q points add points x y important Q holds guarantees add product second visualization Figure examples certainly expressed MFOϕ MSOϕ scope Diophantine Inequalities start consider polynomial inequality mk j monomials variables v vn monomial form ven n en natural numbers like sentences express Diophantine inequalities type
result generalizes observations Lemma Diophantine inequality expressed Proof Let sum coefficients mk j let maximum sums P ei sentence form αi βi z x need ensure tuple values x satisfies αi formulas βi formulas αi contributes exactly overall sum ven n end let αi conjunction following formulas similar conjunct Sn predicating en variables respectively ii remaining x include conjunct x iii final conjunct zia variables zia guaranteed unique disjunct αi conjunct iii ensures αi contributes multiplied number tuples satisfying tuple appears exactly unique values Defining βis analogously produces formula models capture precisely solutions provided sum numbers course solutions add case
fail finitely solution bn simply disjoin statement bn easily definable MFOϕ Conjunctions inequalities studied class Diophantine equations Matiyasevich Robinson Davis Putnam MRDP Theorem shows decision procedure determine given Diophantine equation solution Proposition satisfiability problem problem determining given formula finite model undecidable possible enumerate satisfiable formulas effective way valid sentences negations define equations solutions computably enumerable Proposition computably axiomatizable Normal Forms direction normal form observation version invariance principle INV holds present setting complete description list x variables relative y analogous argument formula equivalent embedded Similar Lemma embedded removed expense string existential quantifiers Theorem Theorem definable sets exactly definable Boolean tions Diophantine
inequalities Proof Sketch want generally formula free variables y equivalent disjunction ranges possible descriptions y σ conjunction strict weak Diophantine inequalities Specifically disjunct s M s M satisfies description σ preliminary reduction formulas single count depth earlier method appealing Automorphism Invariance works essentially invariance refers tuples objects regions leading extraposition cases involving prefix existential quantifiers tuples objects proof Theorem formula equivalent form satisfying induction quantifier depth formulas base case quantifiers single normal forms disjunctions conjunctions takes form αy x αy j x straightforward check disjunct corresponds set Diophantine inequality constraints satisfying inductive case essentially proof Theorem reducing claim normal forms
closed existential quantification Diophantine form arises follows products sizes regions needed multiple counting exponents stay fixed arity multiple count operators formula apart fixed tuple objects distributed regions contributes fixed numerical factors Diophantine format Remark Recall notion trace definability given Def light Theorem MRDP Theorem easy trace definable sets exactly recursively enumerable sets words light Fact expressive comes systems model checking problem decidable Second Order Extensions jump MFOϕ MSOϕ incorporating second order quantification relatively minor Arithmetically speaking simply allowed filling class linear inequalities able encode Given defines Diophantine sets Lemma predicates defined systems decidable Fact interesting question class arithmetical predicates defined
second order extension Recall trace definable sets recursive enumerable sets Remark difficult question directly definable predicates particular close set decidable arithmetical predicates known decidable arithmetical predicates reursively enumerable set indices simple diagonal argument precludes existence syntactic format captures decidable arithmetical properties natural syntactic subclasses studied order current section corrects earlier statement published version paper language bounded arithmetic contains equalities polynomial terms Boolean operations bounded numerical quantifiers form y t polynomial term observations conjectures note sizes subsets object domain second order quantifier ranges bounded sum sizes coding state descriptions variables y model check definition given predicate unique model described earlier non
coding state descriptions denotation recall proof Fact suggests following Conjecture arithmetical predicates directly definable defined Presumably need restricted fragment language clear venturing requires expressive resources instance arithmetical predicate x y z bounded y z quadratic term define predicate bound like need non monadic second order counting logic quantify binary relations giving required exponent having increase domain size model Higher fixed exponents polynomial terms require higher predicate arities leave precise syntactic determination arithmetical defining power higher order extensions interesting problem investigation Infinitary Counting systems consider general versions allow infinite models shown Appendix C similar purely additive case cardinal arithmetic ℵω addition
multiplication separates cleanly finitary infinitary components infinitary component effectively reducing order theory express multiplicative relationships cardinal numbers Alternative Route Explicit Arithmetical Operators sequence systems far studied motivated primarily natural operations logic viz second order quantification polyadicity able calibrate arithmetical content operations base monadic system MFO approach extending MFO spirit logic counting strengthen counting component natural ways particular allowing complex terms built directly arithmetical operations Instead comparisons involving terms like xφ allow comparing instance sums terms xφ yψ general allow inequalities complex terms study consequences different choices complex term building operators salient course addition multiplication turns speaking abstractly arrived systems Addition
Let MFO system results allowing arbitrary finite sums basic terms words allow terms form xnφn know MSO express inequalities Conversely normal form result MSO means linear inequalities shows system MFO fact equally expressive comes defining relations cardinal numbers Note numerical reasoning involved Fourier Motzkin Remark transcribed language ado instance encode crucial step iii simple scheme similar analogous work rational linear programming Fagin et al codify steps algorithm axioms formal system Multiplication arithmetical point view natural allow arbitrary finite products basic system relate systems counting sequences Needless explicit multiplication addition able encode arithmetical relations Similar case MSOϕ explicit addition simulate addition
avail second order quantification let MSOϕ second order monadic fragment products fact binary products suffice Echoing observations dating Skølem encode arbitrary Diophantine inequalities consider variables v corresponding state descriptions express MSOϕ introduce k j predicate variables consider statement Xi disjoint monomial clearly expressed product possibly order quantification original variables P set equal corresponding term Xi define set solutions sense Definition formula includes free predicate variables existentially quantify variables leave fine grained analysis fragments systems purely order fragment future explorations Arithmetical Operations Aside addition multiplication naturally consider host common arithmetical functions relations example unlike addition multiplication exponentiation trivialize infinitary setting finitary
setting exponentiation definable addition multiplication Gödel famous β function Generalized Continuum Hypothesis stated succinctly MSO exponentiation Y abbreviates set cardinality illuminating study properties system different models set theory natural example relation divisibility arises study natural language quantifiers automata hierarchies particular Proposition non trivial observation finite case Julia Robinson order logic divisibility successor function provide suite arithmetically definable relations Robinson time existential fragment Presburger Arithmetic divisibility known decidable Lipshitz leaves open possibility systems remain relatively behaved recall Remark defer explicit arithmetical excursions occasion Interim Summary present monadic setting assess system ability reason counting analyzing arithmetical content family definable relations systems speak
unary predicates Boolean combinations interested abstract relations cardinal numbers sentences systems define essentially taking cardinalities state descriptions generally non overlapping predicates numerical variables seen landscape rich naturally calibrated familiar order arithmetical languages grasp pure monadic fragment consider behaved fragments employing relational reasoning Modal Logic Binary Relations started adding counting operators language order logic found system FO high complexity moved base level monadic fragments decidable allowed combinations logic counting work controlled settings simple intuitive examples reasoning numerical aspects involve binary relations Example known Pigeonhole Principle says n objects k n boxes box contain objects particular values k n principle expressed monadic
order logic unary predicates boxes recall generic formulation need binary relations admit following elegant statement Consider binary relation R domain larger cardinality range object predecessors relation formal notation Rxy Ryx implies Ryx light makes sense study count versions fragments FO allow reference binary relations running high complexity noted earlier language L end explore count versions modal languages detail starting simplest case returning extensions suggested Pigeonhole Principle later basic notions results modal logic needed section refer literature appropriate places Language Semantics language propositional modal logic counting Lml syntax defined inductively follows φ p φ φ depth formulas defined recursively earlier logics
standard clauses atoms p Booleans semantics language uses standard modal relational models M W R V points models define truth formulas term values mutual recursion point s write Rs t Rst R successors semantic key clauses s M s φ iff s s Figure example model holds root point points p holds colored blue number successors non p successors p successors greater number successors p successors non p successors Given define existential modality φ negation define universal dual definability Booleans saw MFO let rest resulting system ML denote logic interpreted finite models MLϕ Remark case variety quantifiers MFO natural counting
modalities successors successors study logic separately expressive power iterated counting simple modal language produce trivial assertions reader consider formula determine says numerically instance finite trees example model depicted Figure enforce infinity sets successors modal formula requires infinity successors satisfying Basic Model Theory invariance properties modal counting language useful studying expressive power generated submodel modal model submodel closed taking successors Blackburn et al Given forward looking nature modal counting language order R following counterpart analogous invariance property basic modal language Proposition Formulas Lml invariant generated submodels b Terms Lml value generated submodels Proof straightforward mutual induction formulas numerical terms Finite Depth
Property modal logic goes finite depth refers following cut versions models s submodel M consisting points reached s n relational steps Proposition model M Lml φ M s φ iff s φ following invariance property refers standard tree unraveling arbitrary relational models yielding tree like models basic modal logic Blackburn et al Proposition Formulas Lml invariant tree unraveling map taking finite branches end points b Terms Lml value tree unraveling points related map Proof proof straightforward induction formulas numerical terms fact immediate successors branch tree correspondence successors end point original model Finally define duplication tree making copies immediate successors root
successor t splits heading disjoint copy original subtree construction defined models general iterated going tree van Benthem use generality Proposition Lml root invariant tree duplication Proof crucial cases numerical modal comparison statements φ language obviously closed taking multiples invariance properties limits expressive power instance counting logic ML contain known system graded modal logic describes specific finite numbers successors Fine Corollary graded modality successor definable ML invariant tree duplication fact ML graded modal logic incomparable expressive power Demri Lugiez powerful system subsumes Bisimulation preservation facts lies general notion bisimulation Lml convenience define standard modal bisimulation Blackburn et al satisfying requirement cardinality
comparison sets satisfying structural property matching modal definability Definition Let Z modal bisimulation points models M N satisfying usual conditions atomic harmony proposition letters Z connected points b standard forth clauses matching relational successors Z connected points define auxiliary relation points M follows x y iff z xZz yZz relation model N defined likewise Z following comparative cardinality conditions hold sZt X Y closed sets successors s X cardinality sense b requirement opposite direction Figure Note Clause mean sets X Y closed respect successors s necessarily model M likewise Clause b understand map note Rt given closedness X fact Z
modal bisimulation Proposition Formulas Lml invariant set y xZy x M N Figure ordinary modal bisimulation Z M N depicted dotted line models root point closed sets successors set set sets encircled blue red Definition ordering sets cardinality hold Proof non routine inductive argument checking preserve truth values atomic formulas φ ways points s t sZt note set φ successors point s model M satisfies closure condition inductive assumption bisimulation invariance formula φ true set ψ successors apply comparison clause sets X Y set successors t satisfying φ definition point Z connected point X satisfies φ inductive hypothesis point Rt
Z connected point Rs inductive hypothesis fails satisfy φ reasoning works Y ψ follows φ true t Given symmetry comparative clause argument works opposite direction Bisimulation invariance certain notions definable Example Infinity set successors definable ML Consider models root successor root infinitely successors proposition letters true points Connecting roots connecting successors models easily seen bisimulation sense general modal logic converse results require additional conditions formulate versions starting Hennessy Milner result image finite models point finitely relational successors Proposition points image finite modal relational models relation E Lml Proof standard argument modal literature Lml contains basic modal language E ordinary modal
bisimulation set comparison clause Start sEt closed set X successors s definable successors follows known model theoretic definability argument set closed equivalence finite set successors fact seen follows Suppose M x Lml Rs ordinary forth clause modal bisimulation x E related u Rt E related u assumed closure assume true M Given preceding observation shows truth formula α s α defines X β defines Y Given definition E formula true t N suffices note definability M plus inductive hypothesis set α successors t likewise β successors common assumption image finiteness runs counter fact ML compare infinite cardinalities successors reach perfect
correspondence employ device passing infinitary modal language allowing conjunctions disjunctions arbitrary sets formulas language Theorem following equivalent exists connecting M s N t b M s N t satisfy formulas Proof inductive proof invariance assertion b essentially b use earlier reasoning image finite models literally noting specific models sets successors small class formulas closure language arbitrary set conjunctions disjunctions Remark bisimulation analysis presented puts crucial count comparisons forth clauses brute force refined notion bisimulation directly relate counting procedures underlie comparative cardinality judgments models leave desideratum study standard modal themes like bisimulation frame correspondence ML extensions Fu Zhao Normal Forms ML
modal counting language admits syntactic normal forms combine standard normal forms modal logic numerical equational normal forms found MFO idea start earlier state descriptions describe inductively successors increasing distance types occur multiplicity follows fix finite vocabulary proposition letters Definition type complete conjunction literals type conjunction type complete set inequalities describing linear order count terms T describe unions n types inductive step definition makes sense easy inductively set n types finite understand definition note modal types record inductively types lower rank present absent successors given points ML merely enriches precise numerical information Fact formula ML count depth k equivalent disjunction k
types Proof k disjunctive normal form propositional logic case k formula depth equivalent Boolean combination proposition letters formulas φ φ ψ depth inductive hypothesis formulas equivalent disjunctions k types formula equivalent disjunction conjunctions statements negations comparison atoms replaced strict inequalities certain number comparisons regions given need replace formula disjunction completions fill comparisons regions possible linearity points comparison earlier normal forms monadic counting logic compress modal normal forms depth ML simplicity level counting refers points farther away modal accessibility structure intuitively nested count information refers different positions normal forms monadic counting language loose necessarily contain complete information regions model difference slight
linearity cardinality order loose forms expanded disjunctions complete forms implicitly invoked fact stating Corollary modal case allow loose forms chose complete version following point Remark modal logic normal form results proved semantically showing formula φ depth k equivalent disjunction k types occurring pointed models φ holds semantic argument involves finite restriction notion modal bisimulation k forth steps similar argument given complex notion bisimulation identified Normal forms related Scott sentences infinitary languages describing suitable ordinal depths syntactic types language realized given model Scott Modal Scott sentences include information numbers occurrences types define given pointed models bisimulation Proposition ML decidable Proof SAT
decidable normal forms depth trivial depth k proceed means following pseudo algorithm Working outside check successively given atomic description root point satisfiable b system inequalities types level k occurring successors root numerically satisfiable Fourier Motzkin algorithm allowing infinite cardinalities described finally c non zero term occurs solution point b relevant type level k test satisfiability simpler types simple decision procedure correct stages b c procedure largely separate satisfy types stage c copying taking disjoint subtrees satisfy desired number successors root described equalities stage b truth values disturbed procedure precisely content earlier observations invariance modal counting formulas generated submodels Remark preceding
analysis constructive contains information reasoning system ML spell detail Note results Demri Lugiez strictly larger modal fragment Thm Proposition fact sharpened decidability polynomial space Language Extensions modal language weak respects instance unlike system MFO talk specific finite numbers relevant objects case successors current point addition lack resources express features earlier mentioned Pigeonhole Principle x y x modal rendering syntax explicit variables principle requires numerical graded modalities b forward backward ordering R c notion counting involved local current point involves global operator g referring domain modal devices Pigeonhole Principle come look like existential modality E defined having global count greater briefly
discuss extensions turn Adding graded modalities ML natural system expressive power MFO sets successors fact Demri Lugiez present generalized graded modal logic Presburger like constraints types successors extends system ML Given analysis additive arithmetic terms second order counting logics connections second order version ML mentioned Example adding backward modalities converse accessibility relation leads tense logical version ML extension straightforward earlier notions need non trivial adaptation typical valid tense logical principles φ φ relating directions R suggest systematic analysis connections counting forward backward directions interest adding global counting operators noted define usual global existential modality domain Extending known fact standard modal
provides alternative notation monadic order logic identity consider global counting device yielding presumably simple modal counterpart system MFO course MFO systems considered earlier sections suggest modal extensions instance analogue notion multiple counting involve multi dimensional modal counting logics Marx Venema importantly representing natural patterns reasoning add second order quantifiers sets analogy earlier system MSO result second order version ML comparable basic modal logic quantifiers propositions Fine fact add quantification proposition letters ML global counting gives alternative modal notation sentences MSO thanks Theorem Lemma argument system decidable Corollary concrete use found discussing quantifier constructions natural language speaking need add statement φ
expressing exactly φ point Generalizing Counting Semantics systems studied dealt syntactic fine structure tractable fragments natural excessively rich complex system FO complementary means recovering intractability change semantics cf van Benthem present setting possibilities suggest motivation broaden interpretation denote elements general class algebraic structures generalize logical semantics ways known reduce complexity allowing variation space allowable variable assignments Németi discuss turn emphasis generalized value approach exploratory section provide level detail earlier presentation Counting break interpretation count terms xφ steps model M domain D variable assignment s consider set s x d M sx d φ second step map subsets S domain numbers
map f S Ultimately set s s x want consider generalizing Eq allowing broader class functions f P P poset set cardinal numbers Probability Proportionality example let P real unit interval rational interval suffice finite models natural map consider function f sending S ratio straightforward verify valid reasoning principles systems MFOϕ MSOϕ remain unchanged basic propositional modal systems PLϕ MLϕ given proportionality interpretation s proportion successor points φ holds interpretation terms xφ φ construed specifying probability φ satisfied connection elementary logics counting probability explicit van der Hoek probability measures obtained way regular assign non set non zero probability cf Ding
et al happens polyadic systems work natural analogues instance recall analysis Qs P Eq Interpreted probabilistic statement measure µ says Q Q confirms P Reichenbach interpretations counting proportion longer agree general logical principles allow poladicity Example Consider formula z y z satisfiable counting interpretation require contrast proportionality interpretation merely requires echoes broader theme reasoning conditional probabilities amounts general reasoning real fields Mossé et al Ibeling et al Mass Weight Abstract Values Probability proportionality clear quantitative numerical measures sizes ratios generalizing logics support qualitative interpretations calculi weight mass Link step set think terms xφ denoting collective entity intuitive sense like semantics
plural expressions mass terms natural language values assigned lie qualitative mereological algebra minimum needed interpreting pre order mereological algebra structure come form addition multiplication fusion mereological primitives Leśniewski pursue general perspective deserves separate ment Instead note changes earlier logical analysis Non classical Logics Recall pointed different logics classical non classical come counting component systems current generalized setting ways inducing logical operations multiply instance long recover classical Booleans way merely drop requirement invalidate paradoxes material implication φ validating principles typical relevant logics Restall leave exploration way inducing logical systems future work Embedding Multisorted FO second logical observation technical generalized semantics earlier
conclusions system behavior complexity need reconsidered Provided stipulate order conditions partial order P map f natural requirements abstract mereological semantics set valid principles FO computably enumerable style analysis applies extended system allows counting tuples Theorem generalized models satisfiability translated faithfully satisfiability associated sorted order language Proof Sketch idea follows generalized semantics works sorted structures domain D objects domain P collectives predicates denotations value domain V binary relation E objects predicates function f P sending predicates values binary relation value domain state needed work order terms analogy Henkin models second order logic Extensionality principle stating predicates standing E relation objects b
set Comprehension principles making sure domain predicates closed definitions language finitely parameters place translate sorted order language particular Tr translation expression xφ read p q translation extends multiple count operators available predicates arbitrary finite arities natural closure properties described order style straightforward formula φ satisfiable abstract value semantics iff translation satisfiable sorted model effectively axiomatized order theory consisting Extensionality Comprehension Incidentally strategy bring complexity second order versions let second order quantifiers range Henkin style special set available predicates sorted models shift order perspective noteworthy repercussions meta properties Consider failure Compactness observed earlier Proposition property hold order reduction outlined Example interest
works standard counterexample Compactness finitely satisfiable set n ω satisfiable standard cardinality semantics generalized semantics Concretely language identity predicate dropping unary predicates convenience consider finite models Mn cardinalities n ultrapower respect free ultrafilter resulting model order properties finite models hold concretely generalized function f works value domain uncountable linear order consisting copy standard natural numbers followed copies integers end copy negative integers finite subsets f gives sizes standard natural numbers cofinite sets values final copy negative integers counting infinite largest element Generalized Dependence Semantics logic oriented approach lowering complexity initial system FO called generalized assignment semantics order logic models come
range admissible available assignments gaps space functions variables objects encode dependencies correlations variables Németi Baltag van Benthem main truth condition M s iff exists admissible assignment t model equal s value x M t φ known set validities generalized assignment models decidable Németi additional order principles impose existential closure conditions admissible assignments Church Rosser style confluence properties support encoding undecidable tiling problems elucidating assumptions underlying undecidability FO standard models generalized assignment models support decidable language extensions polyadic tuple quantifiers van Benthem explicit atoms expressing functional dependence variables y sets variables X Baltag van Benthem extend semantics FO generalized assignment models
need stipulation going count setting options considered given richer environment available arbitrary assignments natural assignment s model M X Y φ denotes cardinality set tuples values taken set Y assignments M agree s X values b φ true counts ranges values variables conditional current values fixed variables leaving open status variables occurring formula φ terms count notion instance existential quantifier dependence modalities Baltag van Benthem easily defined style introduced functional dependence y X written X y x x express notions correlation forms independence instance X y says local values X leave value range y cardinality equal total value range y
model submit combination generalized assignment semantics count terms interesting exploring natural open problems scope paper Generalized Quantifiers Natural Language preceding section concludes analysis elementary combinations logic counting terms standard hierarchy designed formal systems Let return setting Introduction look issues manifest natural language vehicle broader daily practices logical reasoning counting obvious source comparison Generalized Quantifier Theory Barwise Cooper van Benthem Peters Westerståhl area logic counting co existed field places emphasis logic formal syntax counting aspect resides semantics develop interface empirical practice detail quantification natural language theory developed connect interesting ways earlier systems happens new questions arise ways point notation section letters
B C subsets domain Following semantic literature use letters interchangeably predicate symbols formal language provided confusion arise Quantifier Expressions Logical Semantics wide range quantifier words quantificational constructions natural language quantifier vocabulary includes order expressions numerals like combinations like higher order expressions expressions meaning highly context dependent quantifier vocabulary includes comparative expressions B C B C Twice B C basic pattern usually takes binary format QAB Q quantifier expression B unary predicates denoting sets objects generally assumed quantifiers natural languages satisfy universal constraints Conservativity QAB holds iff holds follows line literature assume Conservativity results given general formulations widely assumed constraints hold
cases important Extension saying relation QAB depend total universe objects inside sets B located Finally true quantifier expressions purely numerical sense satisfying following constraint played key role Permutation Invariance QAB holds iff permutation π D B b c e Effect Conservativity Extension Invariance b tree numbers b highlighted red b b b number x x b e c c Arithmetical expressions binary quantifiers Figure Assuming Conservativity Extension Invariance need concerned b b tree numbers consists pairs b Highlighted light red pairs quantifier c Examples quantifiers arithmetic expressions Note addition requiring multiplication quantifier violates Extension total effect conditions ties quantifiers closely
counting specify meaning quantifier expression Q suffices list acceptance behavior pairs numbers b b B QAB holds Figure Accordingly Generalized Quantifier Theory studies quantifiers equally numerical terms logical ones typical tool Generalized Quantifier Theory visualizing double perspective called tree numbers representing quantifiers graphically terms pairs b representing cardinalities Figure quantifier seen subset tree special properties quantifiers geometrical patterns tree instance Q upward monotonic right hand argument Q closed moving upward upward diagonal line point holds Upward monotonicity left hand argument shows acceptance sub quadrant generated points b Q holds geometric descriptions lead simple characterizations possible monotonic quantifiers order definable quantifiers
van Benthem Remark perspective tree numbers approach interesting twists arises passing logical syntax numerical content quantifier expressions place geometrizes numerical content qualitative geometric logic Linguistic Vocabulary start logical system MFO shows analogies preceding style analysis terms general constraints enjoys following known logical property Fact closed relativization definable subdomains Proof crucial step defining relativization map φ given tion property allows explore valid reasoning principles MFO assume Conservativity Extension come fore earlier analysis illustration following principle Quantity makes pure cardinality statement B inside set strong form Conservativity conclusion statement hold set C numerical behavior respect expresses form Permutation Invariance terms validities MFO
inference patterns specific quantifiers Introduction highlighted numerical syllogisms order quantifiers numerals exceptive quantifiers k comparative quantifiers Numerical syllogisms analyzed practice Venn Diagrams number information written zones Figure representation intuitive basis normal forms system MFO recall Figure encodes reasoning informal comparisons precise means definability results illustration model theoretic definability analysis presented Theorem binary quantifiers definable MFOϕ correspond exactly expressible order theory Proof Consider elementary theory natural numbers binary relation greater simplicity assume definable function symbols logic quantifier elimination formula y equivalent Boolean combination equalities inequalities terms form key theorem exactly normal forms binary quantifiers definable MFO precise characterize exactly type defined
formulas B relativization Fact relevant state descriptions Theorem B equivalent disjunction m inequalities constant numbers m involving assume inequalities involve sums denote size domain eliminate remaining cases statement k rewritten large disjunction ways dividing k Likewise rewritten large disjunction pairs adding remaining cases handled similarly result Boolean compound inequalities expressible order theory map language formulas B arithmetical language employing distinguished variables x y corresponding respectively foregoing easy expression B corresponds arithmetic formula y free variables x direction arithmetical formula y form produced quantifier elimination easily seen expressed appropriate formula order quantifiers Q AB natural language definable course MFO define non
order quantifiers B normal form format cuts standard order second order boundaries Remark binary quantifiers definable MFO classified algebraically terms normal forms geometrical perspective provided tree numbers discrete version usual representations solution sets systems linear inequalities cf Schrijver systematic treatment special case numbers b inequalities occurring normal forms reduce following simpler types k ii k iii b k iv b k v b k vi b k plus versions b interchanged forms occur note terms Ti normal forms disjunctions state descriptions disjunction allowed hand suppress possible forms k finite disjunctions equalities tree numbers types correspond simple geometrical patterns describes left
sloping diagonal line ii infinite downward triangle iii horizontal line iv trapezoid horizontal line v vertical line vi slice rightward half triangle Analyzing finite disjunctions look intersections produced earlier definability arguments focus happens finite tree level finite number points level dealt adding explicit definitions terms intersecting lines shapes simplified term finite unions horizontal line finite set points trapezoid finite union triangles triangle union slices left following basic shapes certain tree level diagonal lines vertical lines slices Intersections produce finite unions single points b diagonal lines point onward c horizontal lines point onward d slices point onward instance intersecting slices different
orientations produces infinite band extending vertically downward finite union vertical lines left finite disjunctions following types quantifiers Exactly k B exactly m B b k exactly m B k exactly m B c k equally B B d k m B fewer B non B left looking slice vice versa case geometric analysis extends order quantifiers van Benthem basic shapes needed diagonal lines triangles Remark noted preceding analysis quantifiers finite domains restriction assumed semantics natural language generalization infinite models interest extension tree numbers representation infinite cardinalities cf van Deemter Clearly infinitely quantifiers definable manner realized natural language drive interesting search
examples examples instance simple pattern b defy simple unforced linguistic description terms course words paraphrase says artificial ways cf interested quantifiers actually lexicalized natural languages roughly sense morphosyntactically simple Keenan Paperno hand realistic quantifiers natural language base system express Corollary MFO express quantifier number B proportionality quantifiers n B n additional quantifiers require resources second order system MSO overshoots considerably compared natural language define Presburger definable logical quantifiers following direction consequence Theorem Corollary binary quantifiers definable MSO exactly expressible order theory generation logic continues b says number B equals twice number rephrased thirds Bs intelligible natural basic quantifier vocabulary Finally
richer system Diophantine arithmetic raises issues found earlier Corollary quantifiers definable exactly expressible Boolean combinations Diophantne inequalities small bounded fragment complete order theory suggested van Benthem arithmetical content linguistic quantifiers essentially restricted addition case multiplication irrelevant understanding linguistic quantifier repertoire current analysis throws doubts picture natural meaning involved multiplication natural language resources comparing proportions form pairs objects basic syntax witness naturally occurring relational phrases married resulting counting pairs longer tuples suggests connections multiple count logic formulas define multiplication somewhat artificial variable binding patterns need occur natural language multiplicative content natural quantifier expressions remains determined Finally preceding discussion basic quantifier vocabulary
natural language complex quantifier constructions known constructions logical interest cumulative branching quantifiers Peters Westerståhl particular construction worth highlighting role particles qualifying meanings quantifier combinations Consider sentence like family different problem simple demanding existence choice function families problems particle different requires choice function like cardinality comparison statements crucial difference case function lie inside given relation concrete sentence relation having case natural language poses challenge Remark Linguists aware related phenomena advanced relatively complex machinery handle range attested patterns Brasoveanu Bumford notable dynamic accounts Notably constructions come context probabilistic reasoning Harrison Trainor et al way reverberates natural language witness modal language probability likelihood
Holliday Icard suspect notion guarded injection definable strong counting logic FO finite cardinalities connection weaker logics considered paper case modal system Hall marriage theorem graph theory Hall says injection set set B contained relation R B iff subset C simple definition different sentences like modal logic global counting second order quantification sets Example Let F unary predicate family G problems suppose moves relation x y required definition X shorthand concludes brief comparison quantifier expressions natural language expressive resources Clearly matter proving theorems exploring empirical fit hierarchy system design suggest patterns architecture natural language precisely fit evident common constructions natural language
pose non trivial questions concerning logical systems provided illustrations deeper investigation linguistic versus logical architecture require paper Varieties Monotonicity Reasoning quantifier vocabulary inference patterns natural language Monotonicity inferences arise occurrences predicate positive syntactic position replaced upward occurrences predicate larger denotation negative position downward predicate smaller extension van Benthem Sánchez Valencia Icard Moss Monotonicity inference works natural language kinds quantifiers numerical expressions witness valid inference like B C E E B C Monotonicity inclusion premises valid inference form logical systems particular ones studied Let mark syntactic positions follows formulas MFO atomic formula occurs positively positive negative occurrences polarity conjunctions disjunctions polarity
switches negations finally atoms xφ occurrences ψ switch polarity φ polarity easy following Proposition Positive occurrences formulas MFO support valid upward tonicity inferences negative occurrences downward monotonicity inferences Remark likely MFO satisfies Lyndon Theorem effect semantic monotonicity amounts positive definability logical equivalence van Benthem Icard et al normal forms contain information necessary constructive proof result leave open problem syntax MFO fact suggests kinds monotonicity reasoning usual inclusion premises cardinality premises forms imply happens inductive clauses positive negative occurrences work crucial failure atomic clause premises Bx obviously imply Ax Clearly numerical monotonicity implies set theoretic variant converse fail quantifier B C
upward set monotonic argument B obviously numerically monotonic B larger set disjoint B Remark Numerical monotonicity stated interesting features mixture logic counting special case true replace predicate B cardinality true strong insensitivity property intuitively separates φ purely numerical assertion plus assertion provable preservation theorem formulas order logic MFO complete characterization numerically monotonic formulas provable normal forms end small observation Consider binary quantifiers Q definable logic MFO cases kinds monotonicity close instance Q AB upward set monotone argument B upward cardinality monotone following sense restricted set Q AB C B Q AC crucial property Permutation Invariance given quantifier Q fixed set
sizes subsets B accepts set monotonicity plus permutation invariance imply sizes upward closed restriction comparing inside necessary cardinal monotonicity B arbitrary larger C inside easily fail failure occurs upward set monotonicity left hand argument larger set C change context evaluation permutation invariance support valid second order inference pattern left upward set monotonic quantifiers Q Q AB C Q taken set equinumerous C contains left upward monotonicity applies Cardinality monotonicity resembles monotonicity numerical terms variable x occurs positively x retains polarity addition multiplication left hand inequalities switching polarity right hand inequalities Making work normal forms takes care numerical terms Ti refer
sizes predicates state descriptions unified perspective monotonicity logical arithmetical syntax proposed Icard Moss concrete examples van Benthem Liu note different versions set based size based monotonicity inference hold natural language expression B involve increasing decreasing size relevant zones Venn diagram Remark Natural logic Monotonicity reasoning natural language engine natural logic van Benthem Sánchez Valencia Moss efficient forms surface reasoning based simple fragments proof systems expressive calculi studied literature interest locate natural logic fragments inside example Pratt Hartmann Moss Moss Topal Kisby et al Dynamic Modalities Monotonicity inference viewed dynamically terms model change change internal current model merely changes denotation predicate
larger smaller set X objects turning current M new model X operations models arise different intuitive takes upward monotonicity inference mean add new objects current model satisfy predicate case relevant relation models extension perspective generalized earlier analogy monotonicity numerical terms stand zones model normal forms replacement x x applies regions defined state descriptions single predicates recent years model change studied adding dynamic modalities logical languages cf recent study van Benthem et al standard example says ψ true relativize current model submodel objects satisfying φ fits earlier discussion Conservativity Extension quantifiers upward inclusion monotonicity sense suggests modality holds ψ true models
arising current increasing denotation Downward monotonicity refer decreasing denotation drastically removing objects current model dynamic modality model change says removal current model object satisfying φ results model satisfying ψ Proposition MFO closed dynamic modalities φ MSO closed modality Proof case relativization dealt providing axioms recursively analyze possible syntactic shapes formula ψ proof deletion modality inspection normal forms manner similar van Benthem et al Thm MFO closed predicate extension modality define having number points property straightforward definition second order MSO Similar closure results obtained monadic dynamic modalities describing effects adding object current model Remark source model change occurred discussion counting modal
languages Instead adding explicit numerical information like graded modal languages count setting aside objects replacing removing adding objects current model instance having k successors property p definable deletion modality k times counting syntax link Remark possible finer notions modal bisimulation analyze counting procedures typical way comparing sizes sets picks object set plus object set puts objects aside iterating process keeping track effects removals matched objects exactly MLSR style bisimulations van Benthem et al Technical topics like dynamic modalities far natural language distance great Natural language contain verbs fact change fit setting Sun Liu samples logical reasoning ancient Chinese tradition involve
monotonicity inferences dynamic verbs increase Semantic Automata final topic comes Generalized Quantifier Theory brings entanglement logic counting natural way b b b Figure Acyclic finite automaton recognizing exactly machine begins left state middle accepting state classifying quantifiers terms associated verification procedures determining complexity Automata Hierarchy van Benthem word count course polysemous verbal use act counting nominal use total counted focus dynamic aspects counting Semantic automata read strings symbols b standing types relevant objects encountered traversing finite domain Figure element corresponds occurrence string element corresponds occurrence automaton reads string accepts precisely pair b quantifier automata complexity jumps predicted quantifier denotations studied
models mixture quantifier reasoning brain cognitive sciences Szymanik overview Example acyclic finite automaton Figure recognizes quantifier exactly accepts pair pairs exactly element fewer lead non acceptance familiar operations quantifiers iteration correspond systematically natural operations standard classes automata Steinert Threlkeld Icard list known results subject Proposition order definable binary quantifiers exactly recognized acyclic finite automata van Benthem b Finite automata non trivial cycles recognize number B related periodic quantifiers fact finite automata recognize precisely quantifiers definable order logic divisibility Mostowski c binary quantifier related proportionality quantifiers computable finite automata computable pushdown store automata fact pushdown automata recognize precisely quantifiers definable additive
Presburger Arithmetic semi linear sets van Benthem numerous deep connections earlier systems obviously saw semi linear sets analysis MSOϕ Theorem Proposition adds computational dimension characterization quantifiers definable MSOϕ precisely verified pushdown automata counting procedures required verifying claims MSOϕ carried pushdown store Identifying computational analogue systems illuminating instance initial system MFOϕ misses quantifiers definable finite number illustrative example Corollary capturing quantifiers demand unbounded memory exactly half makes sense interrogate direction systems combining logic counting capture quantifiers recognizable intermediate classes counter automata weaker classes like recognizing subregular languages cf Graf leave questions analysis end final observation tying earlier themes including permutation invariance
seen multiple points theme permutation invariance paramount analysis logic counting Given assumption quantifiers corresponding formal languages closed permutations instance ababa appears quantifier language aaabb relatively exceptional property sets strings permutation closures languages accepted finite automata pushdown automata actually coincide happens characterize semi linear sets Parikh interest understand permutation closed commutative languages right languages studied beginning formal language theory Eilenberg Schützenberger question following restricting permutation closed languages semi linear sets accepted finite automata way calibrating counting capacity finite state machines relative semi linear sets alphabet size recall linear sets Definition solutions equations given constants choices um semi linear sets finite unions
linear sets Definition Let set rectilinear form set semi rectilinear finite union rectilinear sets helpful explain notion earlier geometrical setting tree numbers Remark Linear forms general define diagonal horizontal lines complex patterns triangles slices crucial difference order produce diagonal line coordinate needs incremented period producing horizontal line requires simultaneous increment coordination typically recognizing capacity finite state machines hand finite state machines capable performing counting tasks keeping track cycles numbers b read parity check define quantifiers like number MFO geometric meaning cycles shows automorphisms tree positions accepted quantifier precise nature explained proof following result main offering Theorem binary quantifiers recognized finite
semantic automata precisely associated arithmetical definitions semi rectilinear theorem follows results Kanazawa Ehrenfeucht et al completeness offer proof Appendix Needless beginning study counting procedures relation semantic meanings natural complement logic counting entanglements studied paper Cognitive Questions encountered previous section examples interleaving logic counting natural language entanglement display psychology neuroscience pointed Carey children learn explicit numerical terms examples quantifiers work Barner et al shown strong correlation development comprehension number terms comprehension logical Early learning basic logical numerical constructs evidently intertwined argued continues mature grassroots mathematics ordinary reasoning practices specifically logical systems studied relate cognition fundamental primitives assumed logical systems numerical comparisons
φ φ ability comparisons present wide range species appears available human infants birth Feigenson et al Dehaene Unsurprisingly emerges quantificational phrases children learn alongside plurals Carey evidence basic operations like addition subtraction preverbal infants Feigenson et al adults researchers uncovered distinct brain areas encoding addition making numerical comparisons Dehaene raises question computationally speaking numerical comparisons prominent theme empirical literature distinction reasoning individuals properties reasoning collections ensembles properties solve concrete task determining Bs conceivable families strategies Match B check left Explicitly count numbers B compare numbers Perceptually approximate B compare approximations require enumerating relevant objects explicit way like semantic automata discussed
previous section bypasses explicit enumeration counting procedure relying instead fast parallel perceptual processing visually estimate number balls bin approximate number system ANS fact ubiquitous phylogenetically ancient Dehaene experimental work gone distinguishing hypotheses like specific instances Carey striking example investigates psychological representation quantifier expressions natural language Pietroski et al Lidz et al Knowlton et al b Consider instance verifying sentence like dots blue Figure strategies principle blue dots B like non blue dots Lidz et al Pietroski et al present convincing evidence people fact employ strategy like counts B likely determined ANS Queries involving invoke ANS method people use appears distinct
Knowlton et al work Knowlton et al different English psychologist Piaget famously argued children understanding number built logical primitives version logicism Subsequent research revealed subtle entanglement numerical primitives arising earlier Carey Dehaene discussion Figure display dots experimental participants asked determine dots blue blue dots yellow dots Pietroski et al Knowlton et al expressions universal quantification fact elicit different representations altogether prompt representations ensembles cardinalities elicit individual level procedural strategy like semantic automata Relating tasks logical systems consider order term think φ describing constraints determine counted availability strategies depends extent mind filter instance successful application approximate number system depends specific perceptual
qualities spatial temporal contiguity Dehaene application depends easy match pairs repetition logical property distinctive monadic order system MFO extensions allow kind quantifying terms like xφ recall Figure Consider query blue dots yellow dots B MFO encoded naturally x remove blue dots compare natural second order version MSO appropriate abbreviations introduced earlier x essentially asks locate subset blue dots subtract total number blue dots comparing type predicate subtraction consistent observed patterns Lidz et al specify precise procedure interesting question verification sentences like induce representations like Exceptive phrases dared attempt bonus question best students means removing subparts predicate Peters Westerståhl Chapter given
SUB need consider subformulas φ mention special case interest understanding organisms reason number zero xx x Recent work suggests range crows Kirschhock et al memory required principle implement strategies instance possible implement memory provided B represented lists Thanks anonymous reviewer calling attention dimension Moving MFO MSO evidence fundamental numerical representations involving polyadicity multiplication course running example like antonym exceedingly common appearing early development significant debate expressions analyzed Rett closely unified mass counterparts like little Rothstein cf discussion direct evidence polyadicity multiplication comes surprising finding month infants compare proportions instance preferring ratio Denison Xu phenomena appear consistent representation involving counts pairs
like suggested ANS directly represent compare rational numbers Clarke Beck look like probabilistic interpretation described Teasing apart different possibilities presents exciting opportunity interface experimental inquiry theoretical explorations example contacts empirical cognitive science themes paper let return Pigeonhole Principle experimental study patterns resembling opening example repeated Premise farmers cows Conclusion farmers exact number cows Mercier et al found participants realized conclusion definitely follows proposed explanation apply Pigeonhole Principle need construe numbers forming categories viz property having exactly k cows k instance order encoding Pigeonhole Principle clearly valid realizing Pi need stand numerical predicates requires step interpretation relational encodings Pigeonhole modal variant
elegant generality lacking monadic formulation interpretive step stimulus representation formidable relation Rxy meaning x y cows people accustomed thinking premise like cow owners numbers cows owned come naturally immediately people short way reasoning puzzles grassroots mathematics subtle abstract applications principles advanced topics Pigeonhole principle manifests mathematics surprising ways instance simple proof Erdős Szekeres Theorem graph theory Seidenberg infinitary version principle recall Eq case k appears proofs known Bolzano Weierstrass principle straightforward experiments Mercier et al difficulty choosing relevant predicates applies place suggested based examples like sins virtues Hoeksema understood grammatically quantifier adjectival modifier short dividing interval containing infinite sequence subintervals
guarantee infinitely points inside subintervals turn infinitary patterns logic counting additional array cognitive questions arise Chief question initial conceptions numbers counting extended accommodate basic infinitary reasoning researchers suggested individual developmental stages mastering modern concept infinity actually mirror historical development concept Moreno Waldegg echoing broader theme familiar Piaget Garcia Galileo bewilderment infinite sets matched proper subsets terminology s s satisfiable Bolzano explicit introduction infinity potential feature set describe giving clear meaning notation φ φs unbounded eventually Cantor paradise children undergo surprisingly similar sequence transitions Moreno Waldegg intriguing consider systems studied correspond intermediate way stations development capturing suitably restricted range intuitive infinitary
patterns monadic modal systems involve addition multiplication infinitary patterns systems complex finitary counterparts type logical complexity brought match intuitive cognitive complexity worth investigating concludes brief tour salient points contact empirical issues cognitive sciences deeper foray contacts undoubtedly reveal connections opportunities Conclusion paper presented number contributions studying interplay logic counting viewed basic phenomenon human reasoning right fact encountered perspectives means combine logic counting main perspective adopted consilience synergistic co existence complement related bodies research theory generalized quantifiers Barwise Feferman Peters Westerståhl computational logic Otto Schweikardt explored hierarchy progressively richer formal systems exemplifying perspective summarized Table common theme running systems separation logical
reasoning patterns needed derive meaningful normal forms varieties numerical reasoning suggested normal forms spanned fragments additive arithmetic Diophantine inequalities elementary arithmetic encompassing basic counting binary relations case infinitary reasoning cleanly separated systems considered revealed simplified version corresponding finitary patterns Finally probed natural generalizations systems obtained broadening possible interpretations numerical terms relaxing logical semantics Parallel formal development explored entanglements logic counting natural language thought Quantifier vocabulary provides kind microcosm illustrating broader motifs rich logical linguistic psychological computational dimensions highlighting novel mixtures logic counting touched ontogenetically phylogenetically basic examples number sense addition sophisticated reasoning patterns cusp mature mathematics famous Pigeonhole Principle paradigmatic
instance explorations individual contributions logic counting distinctly identifiable nonetheless resist disentangelement system like MFO starting point analysis count term xφ assumed denote cardinal number logical description specified φ characteristically quantitative principle permutation invariance begets qualitative principles logical language INV SUB turn allow derivation explicitly numerical normal forms support familiar numerical algorithms Hilbert partly simultaneous development laws logic arithmetic requisite Similar patterns permeate discussions extensions MFO empirical phenomena language cognition Monotonicity inference typical example operates level abstracts away logical arithmetical details instance treating number lines predicate hierarchies par perspectives logic counting emphasized present treatment historically prominent reflect aspiration methodological purity briefly
considered logic extracted pure counting saw classical logic emerges remarkably austere numerical primitives non classical systems elicited instance place true universal quantifier entertain variants like xφ states objects satisfy φ count direction considered counting principles implicit order logical systems recurrent theme counting syntax typical connection developed Appendix E exploration place angles logic counting pursued paper exhaust rich ubiquitous entanglement logic counting mention instance natural illuminating computational perspectives briefly explored form procedural semantics logical expressions afforded semantic automata allow calibrate counting content meanings quantifier expressions globally measure numerical content entire logical system terms computational complexity satisfiability problem precise sense NP hard
logical system instance ordinary propositional logic said solve arbitrary integer programs simple viz polynomial SAT reduction similar vein hard system overtly quantative order dynamic logic Harel answers arbitrary arithmetical queries angle affords relatively coarse grained means calibrating logical numerical reasoning seen present article collapse expressively intuitively distinct systems MFO MSO entanglements computational complexity deeper seen methods proof complexity logical encodings numerical principles like Pigeonhole center stage Cook Reckhow Krajíček Research programs like reinforce view consilience co existence natural habitat closing important acknowledge reductive aspirations methodological purity originate motivations logical mathematical program logicism instance concerned philosophical puzzles epistemology metaphysics number Hale
Wright Measurement theorists maintained qualitative nonnumerical empirical laws objective significance numerical representations merely matter tion chosen computational convenience Krantz et al pp stance philosophical methodological issues hope shown important borders thresholds understanding reasoning qualitative quantitative reasoning simple complex combinations logic counting lose foundational purity pursuing path gain better understanding human reasoning abilities return Acknowledgements helpful feedback like thank Xiaoxuan Fu David Gonzalez Erich Grädel Makoto Kanazawa Phokion Kolaitis Thomas Mayer Paul Pietroski Alexander Pruss Stanislav Speranski Rineke Verbrugge Zhiguang Zhao audiences Nordic Online Logic Seminar UC Berkeley Logic Colloquium Stanford University Logic Seminar Tsinghua University Logic Seminar grateful editors referees
Bulletin Symbolic Logic productive comments suggestions References Ackermann Solvable Cases Decision Problem Studies Logic Foundations Mathematics North Holland Publishing Company Antonelli Numerical abstraction Frege quantifier Notre Dame Journal Formal Logic Baader De Bortoli expressive power description logics cardinality constraints finite infinite sets Herzig Popescu editors Frontiers Combining Systems pages Bacchus Representing Reasoning Probabilistic Knowledge MIT Press Baltag van Benthem simple logic functional dependence Journal Philosophical Logic Barceló Kostylev Monet Pérez Reutter Silva logical expressiveness graph neural networks Proceedings International Conference Learning Representations ICLR Barner Chow Yang Finding meaning test relation quantifiers integers language development Cognitive Psychology Barwise Cooper Generalized quantifiers
natural language Linguistics Philosophy Barwise Feferman Model Theoretic Logics Association Symbolic Logic Bednarczyk Demri Fervari Mansutti Modal logics sition finite forests Expressivity complexity Proceedings Annual ACM IEEE Symposium Logic Computer Science page van Benthem Essays Logical Semantics Reidel Dordrecht van Benthem Language Action Categories Lambdas Dynamic Logic volume Studies Logic Elsevier Amsterdam van Benthem Program constructions safe bisimulation Studia Logica van Benthem Guards bounds generalized semantics Journal Logic Language Information van Benthem Liu New logical perspectives monotonicity Deng Liu Liu Westerståhl editors Monotonicity Logic Language Springer van Benthem Mierzewski Zaffora Blando modal logic stepwise removal Review Symbolic Logic page Blackburn
de Rijke Venema Modal Logic Cambridge University Press New York Borosh Treybig Bounds positive integral solutions linear diophantine equations Proceedings American Mathematical Society Brasoveanu Sentence internal different quantifier internal anaphora Linguistics Philosophy Bumford Incremental quantification dynamics pair list phenomena Semantics Pragmatics Burgess Axiomatizing logic comparative probability Notre Dame Journal Formal Logic Cai Fürer Immerman optimal lower bound number variables graph identification Combinatorica Carey Origin Concepts Oxford University Press Carreiro Facchini Venema Zanasi Model theory monadic predicate logic infinity quantifier Archive Mathematical Logic Clarke Beck number sense represents rational numbers Behavioral Brain Sciences pages Cook Reckhow relative efficiency propositional proof systems
Journal Symbolic Logic Corcoran Frank Maloney String theory Journal Symbolic Logic van Deemter Generalized quantifiers finite versus infinite van Benthem ter Meulen editors Generalized Quantifiers Natural Language pages Foris Dehaene Number Sense Oxford University Press Demri Lugiez Complexity modal logics Presburger constraints Journal Applied Logic Denison Xu origins probabilistic inference human infants Cognition Ding Harrison Trainor Holliday logic comparative cardinality Journal Symbolic Logic Ding Holliday Icard Regularity relative likelihood manuscript Ehrenfeucht Haussler Rozenberg regularity context free languages Theoretical Computer Science Eilenberg Schützenberger Rational sets commutative monoids Journal Algebra Endrullis Moss Syllogistic logic Mathematical Structures Computer Science Fagin Halpern Megiddo logic
reasoning probabilities Information Computation Feferman Vaught order properties products algebraic systems Fundamenta Mathematicae Feigenson Dehaene Spelke Core systems number Trends Cognitive Sciences Fine Propositional quantifiers modal logic Theoria Fine possible worlds Notre Dame Journal Formal Logic Fu Zhao Modal logic counting Definability semilinear sets correspondence theory Unpublished manuscript China University Political Science Law Beijing School Mathematics Statistics Taishan University Gärdenfors Qualitative probability intensional logic Journal Philosophical Logic Ginsburg Spanier Semigroups Presburger formulas languages Pacific Journal Mathematics Grädel Otto Rosen variable logic counting decidable LICS IEEE Computer Society Grädel Otto Rosen Undecidability results variable logics Archive Mathematical Logic Graf subregular bound
complexity lexical quantifiers Schlöder McHugh Roelofsen editors Proceedings Amsterdam Colloquium pages Grumbach Tollu expressive power counting Theoretical Computer Science Grzegorczyk Undecidability arithmetization Studia Logica Hale Wright Reason Proper Study Essays Neo Fregean Philosophy Mathematics Oxford University Press Hall representatives subsets Journal London Mathematical Society Halpern analysis order logics probability Artificial Intelligence Harel Recurring dominoes Making highly undecidable highly understandable Annals Discrete Mathematics Harrison Trainor Holliday Icard Inferring probability comparisons Mathematical Social Sciences Hartogs Über das Problem der Wohlordnung Mathematische Annalen Herre Krynicki Pinus Väänänen Härtig quantifier survey Journal Symbolic Logic Hilbert foundations logic arithmetic Monist Hoeksema Plurality conjunction ter Meulen
editor Studies Model Theoretic Semantics pages Foris van der Hoek Qualitative modalities International Journal Uncertainty Fuzziness Knowledge Based Systems van der Hoek de Rijke Generalized quantifier modal logic Journal Logic Language Information Hoffmann Commutative regular languages properties state complexity Ćirić Droste Pin editors Algebraic Informatics pages Springer Holliday Icard Axiomatization meaning sciences Ball Rabern editors Science Meaning Oxford University Press Ibeling Icard Mierzewski Mossé Probing qualitative divide probabilistic reasoning Annals Pure Applied Logic Icard Moss Recent progress monotonicity Linguistic Issues Language Technology Icard Moss Tune monotonicity calculus completeness Kanazawa de Groote Sadrzadeh editors Proceedings Meeting Mathematics Language pages Kanazawa Monadic
quantifiers recognized deterministic pushdown automata Aloni Franke Roelofsen editors Proceedings Amsterdam Colloquium pages Karp Reducibility combinatorial problems Miller Thatcher Bohlinger editors Complexity Computer Computations pages Springer Keenan Paperno Overview Handbook Quantifiers Natural Language volume Studies Linguistics Philosophy pages Springer Kieroński Pratt Hartmann Tendera variable logics counting semantic constraints ACM SIGLOG News Kirschhock Ditz Nieder Behavioral neuronal representation numerosity zero crow Journal Neuroscience Kisby Blanco Kruckman Moss Logics sizes union intersection Proceedings AAAI Knowlton Hunter Odic Wellwood Halberda Pietroski Lidz Linguistic meanings cognitive instructions Annals New York Academy Sciences Knowlton Pietroski Halberda Lidz mental representation universal quantifiers Linguistics Philosophy forthcoming Kraft
Pratt Seidenberg Intuitive probability finite sets Annals Mathematical Statistics Krajíček Proof Complexity Cambridge University Press Krantz Luce Suppes Tversky Foundations Measurement volume Academic Press New York Kuske Schweikardt order logic counting weak hanf normal forms exist computed Proceedings Annual ACM IEEE Symposium Logic Computer Science Lai Endrullis Moss Majority digraphs Proceedings American Mathematical Society Leśniewski O podstawach matematyki Przegląd Filozoficzny Lewis Complexity results classes quantificational formulas Journal Computer System Sciences Lidz Pietroski Halberda Hunter Interface transparency psychosemantics Natural Language Semantics Lindström order predicate logic generalized quantifiers Theoria Link Algebraic Semantics Language Philosophy Cambridge University Press Lipshitz diophantine problem addition divisibility
Transactions American Mathematical Society Marx Venema Multi Dimensional Modal Logic Springer Mayer investigation negationless fragment Rescher Härtig quantifier Meier Ortiz editors Foundations Information Knowledge Systems pages Mercier Politzer Sperber causes failure apply pigeonhole principle simple reasoning problems Thinking Reasoning Moreno Waldegg conceptual evolution actual mathematical infinity Educational Studies Mathematics Mortimer languages variables Mathematical Logic Quarterly Moss Natural logic Handbook Contemporary Semantic Theory Second Edition pages Wiley Blackwell Moss Syllogistic logic cardinality comparisons Bimbó editor Michael Dunn Information Based Logics pages Springer Moss Topal Syllogistic logic cardinality comparisons infinite sets Review Symbolic Logic Mossé Ibeling Icard causal reasoning harder probabilistic reasoning
Review Symbolic Logic Mostowski Tarski Arithmetical classes types ordered systems Bulletin American Mathematical Society Mostowski Computational semantics monadic quantifiers Journal Applied Non Classical Logics Németi Fine structure analysis order logic Marx Masuch Pólos editors Arrow Logic Multidimensional Logic pages CSLI Publications Oppen upper bound complexity Presburger Arithmetic Journal Computer System Sciences Otto Bounded Variable Logics Counting Springer Parikh context free languages Journal ACM Peters Westerståhl Quantifiers Language Logic Oxford University Press Piaget Garcia Psychogenèse et Histoire des Sciences Paris Flammarion Pietroski Lidz Hunter Halberda meaning Semantics numerosity psychology Mind Language Pratt Hartmann Complexity variable fragment counting quantifiers Journal Logic Language
Information Pratt Hartmann computational complexity numerically definite syllogistic related logics Bulletin Symbolic Logic Pratt Hartmann syllogisms numerical syllogistic Languages Formal Natural volume LNCS pages Springer Quine Concatenation basis arithmetic Journal Symbolic Logic Reichenbach Direction Time University California Press Berkeley Rescher Plurality quantification Journal Symbolic Logic Restall Introduction Substructural Logics Routledge Rett semantics little Language Linguistics Compass Robinson Definability decision problems arithmetic Journal Symbolic Logic Rothstein Counting mass count distinction Journal Semantics Sánchez Valencia Studies Natural Logic Categorial Grammar PhD thesis Universiteit van Amsterdam Schrijver Theory Linear Integer Programming John Wiley Sons Schweikardt Arithmetic order logic counting quantifiers ACM actions Compututational
Logic Scott Logic denumerably long formulas finite strings quantifiers Addition Henkin Tarski editors Theory Models pages North Holland Seidenberg simple proof theorem Erdős Szekeres Journal London Mathematical Society Skølem Diophantische Gleichungen Ergebnisse der Mathematik und ihrer Grenzgebiete Springer Berlin Slomson monadic fragment predicate calculus Chang quantifier equality Löb editor Proceedings Summer School Logic Leeds pages Berlin Heidelberg Springer Berlin Heidelberg Steinert Threlkeld Icard Iterating semantic automata Linguistics Philosophy Steinhorn Borel structures order extended logics Harrington Morley Sˆvêdrov Simpson editors Harvey Friedman Research Foundations Mathematics volume Studies Logic Foundations Mathematics pages Elsevier Sun Liu inference pattern Mou Mohist logic montonicity reasoning
view Roczniki Filozoficzne Szymanik Quantifiers Cognition Logical Computational Perspectives Springer Tarski Mostowski Robinson Undecidable Theories North Holland Publishing Väänänen Remarks generalized quantifiers second order logics Set Theory Hierarchy Theory volume pages Prace Naukowe Instytutu Matematyki Politechniki Wroclawskiej Wroclaw Visser Growing commas study sequentiality concatenation Notre Dame Journal Formal Logic Westerståhl Logical constants quantifier languages Linguistics Philosophy appendices present additional material broadens context main results paper Appendix survey relevant literature Appendices B C D present details results mentioned main text concerning infinity quantifiers monadic second order logic infinitary addition multiplication semantic automata respectively Finally Appendix E highlights intriguing interface logic counting
largely ignored paper historical tradition results entanglement syntax logical systems systems arithmetic Appendix Related Work Logic Counting mentioned vast important research mixtures logic counting discuss logical systems literature bear close relationship hierarchy systems studied summarized Tables Logics Generalized Quantifiers expansive literature explored adding generalized quantifiers order logic languages including monadic order logic system FO studied explicitly literature Herre et al Antonelli Peters Westerståhl course closely related Härtig quantifier xφ strict version xφ introduced explicitly Lindström Earlier Rescher considered unary version xφ Work monadic fragment FO generalized quantifiers dates Slomson studied Chang quantifier xφ context refer Peters Westerståhl results references area
related particular generalized quantifiers FO MFO Computational Logic largest body work related systems comes computation logic significant strand focuses extensions FO interpreted finite models Cai et al Grumbach Tollu Schweikardt Kuske Schweikardt discussed Remark known finite variable fragments counting quantifiers results negative Otto Grädel et al Kieroński et al forth games similar spirit Definition explored Cai et al Otto Syllogistic Propositional Counting Logic number weak fragment MFO PL studied extended syllogistic systems example series papers charts territory small systems including k related operators Pratt Hartmann Moss Lai et al Endrullis Moss Moss Topal Kisby et al Pratt Hartmann particular explores
FO free variable seen decidable notes natural probabilistic interpretation system Locating precisely systems fit inside logics worthwile Notably enjoy low complexity Recent work Ding et al essentially deals PL interpreted possibly infinite models highlighted Table main difference PL sentences MFO ability express inequalities numerical bounds important instance s showing MFO unlike PL characterize infinite predicates higher expressive power numerical bounds marks important distinction valid principles instance main principle axiomatizations Ding et al employs type polarization rule Kraft et al Burgess Adapted setting provided predicate P occurs φ ψ rule x infer ψ Polarization Polarization admissible basic system MFO implies things
consistent formulas true duplicating size regions true sets inequalities numerical bounds ones expressible MFO discussed remains seen intricate polarization rule MFO support purely logical axiomatization Probability Logic mentioned connection probability logic systems PLϕ MLϕ MFOϕ MSOϕ interpreted probabilistically ado viz proportionality interpretation PLϕ guishable propositional probability logic considered van der Hoek equivalent system studied earlier Gärdenfors provided restricted regular probability measures assigning non sets strictly positive probability MSOϕ easily seen equally expressive probability logic linear inequalities studied Fagin et al assumption regularity discussion regularity probability logic Ding et al strong probability logic studied Bacchus Halpern allowing inequalities sums products terms
πxφ cf polyadic terms interpreted cardinalities Cartesian products terms πxφ interpreted directly products probabilities general leads different set principles cf Example Quantifiers term variables allowed Unsurprisingly languages highly undecidable decidable fragments found allowing monadic predicates eliminating variable equality Halpern Graded Modal Logic areas modal description logics number authors Fine considered graded modal logics involving unary modalities like mentioned ML express modalities Corollary course reverse true binary modality expressive capacity graded modal logic broad study connections generalized quantifiers appears van der Hoek de Rijke recently researchers probed precise counting capacity systems employing notions count bisimulations Baader De Bortoli study related logical
systems expressively equivalent complex graded modal logic Bednarczyk et al natural expressive extensions remain relatively low complexity Demri Lugiez Emerging connections graded modal logic classes graph neural networks Barceló et al promise dimensions subject Appendix Infinity Quantifer Monadic Second Order Logic Let monadic order logic infinity quantifier simply language let WMSO weak monadic second order logic quantification finite sets turns WMSO expressively equivalent version result equality Väänänen describe result equality translate WMSO interesting case direction normal form result Carreiro et al Thm sentence equivalent disjunction existentially quantified formulas form z Supposing X monadic predicates assuming finite sets values equivalent form
z commutes disjunction need consider happens appending formula evidently equivalent formula occurrences X z concludes argument direction Appendix Cardinal Arithmetic Quantifier Elimination Separation Consider elementary theory structure C order theory addition cardinal numbers ℵω ordinary Presburger Arithmetic s definable structure s function takes cardinal number largest cardinal number congruence mod n n ω Note definable Assume constants functions relations signature considering s n ω derive normal form quantifier free fragment propositional reasoning assume disjunction conjunctions atomic formulas t u t u t u propositional reasoning assume disjunct includes conjunct x x variable x appearing disjunct allows separate atomic formulas involving
finite terms involving infinite terms successor function course takes cardinals infinite terms absorb finite terms sums Furthermore t u contains infinite term assume loss t u contain infinite terms types atomic formulas trivialize words obtained normal form characterized disjunctions conjunctions include statements variables finite infinite set statements describing finite terms set statements describing infinite terms finite component usual regimented types atomic statements involve sums terms form k x variable law y x usual models conjunctions effectively solutions linear programs infinite component successor fact distributes addition y allows similar regimentation regimentation possible note replaced eliminate sums instance t equivalent disjunction t
v u reduction works strict inequalities component describing infinite terms simply contains conjuncts form x x x ℵk x ℵk k trivial isomorphism s s k ℵk shows definable subsets infinite cardinals coincides definable sets indices N viz finite co finite sets course easily establishes decidability determining quantifier free formula original language satisfiable Summarizing Proposition order quantifier free formula equivalent structure s n ω disjunction conjunctions specifying variables disjunct finite infinite finite component description linear set infinite component description set infinite cardinals s aleph number indices Corollary quantifier free theory decidable order theory C ordinary Presburger Arithmetic theory admit quantifier
elimination theory augmented language Consider formula θ normal form Proposition θ conjunction δ δ description variables ι describes infinite terms ϕ describes finite terms normal form x appear ι ϕ simplifies ι ϕ assumed involve infinite finite terms respectively case perform quantifier elimination usual additive arithmetic reducing quantifier free statement s congruence relations case want reduce quantifer free form s fact proceeds exactly quantifier elimination procedure s isomorphism structure s quantificational theory Having shown quantifier elimination establishes Theorem order theory C decidable essentially result order arithmetic cardinals let structure cardinal numbers ℵω addition multiplication order theory structure course undecidable easy
substructure substructure definable sense term t denotes natural number t argument formula equivalent disjunction conjunctions δ δ specifies terms ι involves infinite terms ϕ finite terms ϕ component arbitrary arithmetical formula quantifier elimination course fails ι component allow quantifier elimination consider elementary theory crucial step purely additive case equality statement t u v t u v equivalent structure disjunction t u v u similarly strict inequalities complex terms implying systematically eliminate addition multiplication quantifier elimination language augmented constant successor s follows fact s Theorem formula language order arithmetic equivalent disjunction conjunctions involving finite infinite component set infinitary formulas terms declared
infinite possesses quantifier elimination define precisely relations cardinals pure language equality strict inequality Appendix Finite Automata Quantifier Recognition Procedures Finite automata particularly simple counting devices follows determine binary logical quantifiers device recognize recall main definitions statement result Linear sets solutions equations rectilinear sets k Finally set semi rectilinear finite union rectilinear sets purpose appendix notate linear sets notation rectilinear sets seen defined forms ij lists periods type type result states Theorem following equivalent permutation closed languages L L regular b set occurrence vectors strings L semi rectilinear Proof idea proof associate semi rectilinear forms finite automata showing works shall geometrical
representations number places like tree numbers generalized quantifiers rotation grid N N fits purposes better fact terminology rectilinear motivated shapes grid shall known useful properties finite automata closure unions languages recognized fact nondeterministic finite automata recognizing power deterministic ones fact recognizing power deterministic finite automata changed allow transitions symbol read states b suffices implication rectilinear forms closed regular languages closed taking unions special cases easily shown regular single vector vector plus period starting main proof warm example Example rectilinear form matches permutation closed regular language strings odd number symbols occurrences symbol following finite automaton recognizes strings Horizontal arrows b moves
vertical arrows moves rightmost states allow b moves starting state accepting state illustrations easy state j visited having seen j occurrences b plus number occurrences equals plus multiple reflecting available cyclic detours b correct string recognized cycling ending moving cycling ending moving finally cycling ending general principle clear Taken b automaton recognizes given language Incidentally automaton unique preceding reasoning yield conclusion allowed cycling middle layers state transition diagram consider general rectilinear form F ij Let sum plus maximum numbers ik occurring left periods F defined likewise right hand pairs occurring define non deterministic partial finite automaton S States pairs u
v u v recognizing state transition function defined follows types moves x y reading x y x y state analogously reading b II state x y x y period occurs Likewise b moves periods j automaton S permutation invariant reading string X drive S state S state T permuted version X drive S state S state following shown direct inspection defined transitions Fact automaton S permutation invariant Proof suffices b transitions interchanged input state changing output state easily established considering combinations Type transitions Type II transitions Lemma following assertions equivalent String X recognized defined automaton S ii occurrence numbers b
X set defined rectilinear form Proof ii Suppose string X drives S starting state accepting state prove following stronger invariance statement induction length finite strings Claim string X drives S state x y occurrence numbers X generated x y plus possibly finite sum periods occurring rectilinear form Proof Claim claim clear string starting state use fact automaton S defined identity inductive step inspecting possible transitions discuss moves b moves similar Suppose Xa drives initial state S x y moves x y reading final inductive hypothesis X occurrence numbers match stated description state x y occurrence numbers Xa satisfy description respect
x y b suppose Xa reaches x y S moves x y reading final inductive hypothesis occurrence numbers X match stated description x y definition S period F allowing cyclic occurrence numbers Xa satisfy stated description state x y particular accepting state reached string pair occurrence numbers given rectilinear set ii Let string X occurrence numbers given rectilinear set particular values period variables permutation invariance automaton S string X recognized iff following permuted version recognized symbols symbols b remaining symbols followed remaining b ii sequence takes recognizing state symbols final ii discounted making appropriate looping moves corresponding admissible periods returning
b Consider permutation closed regular language produce suitable automaton work rest proof Fact L recognized permutation invariant deterministic finite automaton Proof Consider standard Nerode construction regular languages strings called equivalent send continuations accepting states recognizing deterministic finite automaton language equivalence classes states transition function plus accepting states defined obvious manner suffices note simple fact regular language start permutation closed Nerode automaton permutation invariant earlier sense permutation invariance allows define pair numbers j unique state Sij S reach starting state presented string occurrence numbers j accepting iff Sij strictly necessary follows helpful think structures abstractly bimodal relational models S grid unraveling
N N carries commuting functions moving step moving step right following connection arises Fact Sij modal p morphism grid N N automaton consider grid model N N automaton equivalent S obvious sense analyze geometrical shape b T T T S S S B B C C D D D D Explanation grid automaton symbols b represent functions grid model state S recurring state start reading symbols starting state interval S S row true matching intervals horizontal rows higher arise applying function b number times identical states particular rectangles right area B analysis works recurring state T left C area area
arbitrary state content finite non recurring state sequences bounded length size given automaton Finally rectangle D special corner points given origins S T intervals D repeat fill remaining quadrant N N identical copies consider recognizing state U occurrences grid described follows area area diagram typical features rectilinear forms emerge area finite disjunction descriptions single vectors area B finite disjunction occurrences U rectangle plus periods k length interval S area B enumeration analogous period l moving upward Finally area D occurrences U quadrant described finite disjunction occurrences generating rectangle allowing periods l particular oblique periods j like period defining non regular
quantifier needed enumeration preceding descriptions taken disjunctively occurrences accepting states grid shows permutation closed language recognized given automaton S semi rectilinear description earlier mentioned characterization order quantifiers van Benthem special case crucial area D collapses state behavior extends downward generalizations result probably holds arbitrary finite alphabets given affinities treatment graph theoretic analysis permutation closed regular languages arbitrary alphabets Hoffmann cf Ehrenfeucht et al addition Kanazawa gives arithmetical description permutation invariant languages recognized pushdown automata questions raised results proof method terms formats structure special regular expressions describe permutation invariant finite automata algebraic laws govern manipulation Rectilinear forms flattening nested iterations level
reminiscent flattening nested count terms normal forms MFO modal perspective proof yield insights particular use grid N N significant decoration finite set states form tiling modal logics tiling problems high complexity connecting counting logics natural question results reflected arithmetical definability results finite state quantifiers terms inequalities normal forms MFO directly order language Presburger Arithmetic Finally counting logics typically allow infinite cardinalities automata analysis extended infinite cardinalities Büchi automata infinite strings Appendix Logical Syntax Counting addition mixtures logic counting discussed paper perspective long history Working logical system presupposes understanding syntax syntax combinatorial entity syntactic manipulations close computing saw hints encountered counting
syntax Example Remark connection goes deeper Counting arithmetic start soon introduce logical system defining set formed expressions language mention specifications counts legal proof derivation potentially vicious circle emphasized Hilbert beginning modern logic usual exposition laws logic certain fundamental concepts arithmetic employed example concept aggregate concept number Subsequently work revealed deep precise sense syntax counting sides coin instance echoing related ideas Tarski Hermes Löb Quine showed order theory natural numbers true arithmetic fact bi interpretable order theory concatenation strings theory semigroups theory natural numbers essentially theory concatenation operator strings intuition connect theme themes present work consider laws concatenation alphabet size consisting
Let ε string easy check following principles valid ε x y y x ε x x y x y Induction happens interpreting ε principles completely axiomatize Presburger Arithmetic precisely need run argument quantifier elimination system met paper different guises Intuitively laws addition laws concatenation unary notations Quine showed surprisingly correspondence extends arithmetic long symbol Similar results shown second order number theory second order theories strings Corcoran et al recently Grzegorczyk demonstrated weak theory tion replace axiomatic theories arithmetic celebrated proof sufficiently strong theories undecidable incomplete Remarkably allows Gödel style arguments detour arithmetization syntax use Chinese remainder theorem Later Visser proved
Grzegorczyk theory concatenation fact essentially undecidable sense Tarski et al showing mutually interpretable Robinson Arithmetic papers ensuing literature contain wealth results rich topic adding dimension interplay logic counting
cs CV Jul MoDiT Learning Highly Consistent Motion Coefficients Diffusion Transformer Talking Head Generation Yucheng Wang Dan Xu Hong Kong University Science Technology Figure MoDiT learns highly consistent Motion Coefficients diffusion transformer predicts accurate flow based previously learnt enhances consistency talking head video aim address temporal jittering caused weak temporal constraints leading frame inconsistencies identity drift resulting insufficient information extraction compromising facial identity preservation unnatural blinking behavior inadequate modeling realistic blink dynamics Abstract Audio driven talking head generation critical plications virtual assistants video games films natural lip movements essential Despite progress field challenges remain producing consistent realistic facial animations Existing
ods based GANs UNet based diffusion models face major limitations temporal jittering caused weak temporal constraints resulting frame tencies ii identity drift insufficient tion extraction leading poor preservation facial tity iii unnatural blinking behavior inadequate modeling realistic blink dynamics address sues propose MoDiT novel framework combines Morphable Model Diffusion based Transformer contributions include cal denoising strategy revised temporal attention biased self cross attention mechanisms enabling model refine lip synchronization progressively enhance face coherence effectively mitigating temporal jittering ii integration coefficients provide explicit spatial constraints ensuring accurate informed cal flow prediction improved lip synchronization results preserving identity consistency iii refined blinking
strategy model natural eye ments smoother realistic blinking behaviors visit repository Introduction Audio driven talking head generation plays crucial role applications virtual assistants video games movie productions goal task generate lifelike lip movements precisely nized driving audio Achieving tion allows digital humans effectively convey tone rhythm speech creating engaging interaction experience users Despite promising advancements audio driven talking head generation critical challenges persist producing highly consistent results cent methods face problems like jittering identity inconsistency argue UNet based fusion models struggle maintain consistency spatial temporal perspectives Spatially pre trained Variational Autoencoder VAE encoder fails extract explicit information leading insufficient spatial
ence example Aniportrait Figure Temporally despite recent advances temporal modules aimed improving consistency insufficient straints latent sequences expressed time affects coherence ii diffusion models present potential solution gradually removing noise capture entire distribution current techniques rely GRUs CNNs sequential processing tures struggle extensive context compared formers results temporal inconsistency iii correlation lip movements driving audio stronger facial motions like head ments blinkings driving audio posed problem find mappings lip movements facial motions driving audio situation ditional methods deterministic regression fail capture joint distribution learning single mapping directly Generative Adversarial Networks GANs shown promise encounter problem accommodating wide range facial sions
instability training stage learning mapping directly noise distribution data bution example shown Figure work MoDiT shown Figure presents novel approach integrating Morphable Model Diffusion based Transformer work offers explicit constraints frame rendering reduce spatial inconsistency diffusion denoising process utilize latent variables extracted audio signal initial establish temporal spatial conditions oping specialized strategy incorporates biased ditional self cross attention allow diffusion model focus different levels denoising stages starting lip region expanding entire face hierarchical approach helps balance lip tion naturalness overall expressions ing computational complexity ii use accurate coefficients predict accurate flow use results ensure better Lip Sync rendering pipeline iii
addition provide new strategy blinking smoother data VFHQ HDTF ing tested identity cross identity datasets shown Table demonstrate bility model evaluated completely distribution dataset shown Table Considering Hallo trained larger private dataset gue method achieves SOTA comparable results temporal spatial consistency summary work presents novel approach integrating Morphable Model Diffusion based Transformer framework audio driven talking head generation contributions include hierarchical denoising strategy revised temporal tion biased self cross attention mechanisms enabling model refine lip synchronization progressively enhance face coherence effectively mitigating ral jittering ii integration coefficients provide explicit spatial constraints ensuring accurate informed optical flow prediction improved lip nization
results preserving identity consistency iii refined blinking strategy model ral eye movements smoother realistic ing behaviors Related Work based Talking Head Generation Early methods talking head generation consider facial motion extracted given face video sequence driving signals generate talking head video convenient interaction audio driving methods use encoder decoder models map audio visual tures enabling realistic lip movements synchronized speech Techniques like disentangling pearance semantic features improved lip sync racy evaluation networks ensured synchronization Generative models GANs VAEs refined approaches focusing mouth region ing frames include head shoulder movements lack explicit structural data caused inconsistencies like unnatural poses aligned features
based Talking Head Generation based methods advanced facial animation capturing head motions tional expressions facial details lifelike results Models like AudioDVP NVP NeRF reenact facial expressions dynamically space ensuring consistency head poses ing integrating geometry neural ing methods achieve superior realism stability compared approaches struggle extreme poses spatial inconsistencies Talking Head Generation Diffusion Methods fusion models revolutionized talking head synthesis iteratively refining images high quality animations Early models like Diffused Heads GAIA set foundation DreamTalk VividTalk introduced expressive facial animations Recent advancements including EMO iTalker Hallo enhance emotional expressiveness long video handling character Figure Overview Diffusion Transformer Pipeline showcasing denoising
stages temporal spatial condition injection uses spatial conditions audio encoder temporal conditions diffusion transformer denoises sequences processed pose prediction mapping rendering new frames tomization pushing boundaries realistic sis ACTalker proposes mamba based condition fusion framework multi modal talking head tion methods achieve impressive results pression modeling temporal coherence character tomization lack explicit spatial constraints lip synchronization fail address natural eye movements comprehensively Proposed MoDiT Method Preliminary Face Model consider space predicted Morphable Models intermediate representation face shape S decoupled S S αUid βUexp S represents average face shape Uid Uexp orthonormal bases identity sion LSFM morphable model coefficients α β
capture identity expression respectively enhance lip synchronization ation model expression parameters β Diffusion Models Talking Head Generation denoising phase model trained reverse diffusion process converting random noise real data distribution inference denoising cess expressed pθ xt N µθ xt t Σθ xt t N xt βt ϵ βt ϵ αt Qt αi θ specifically denotes parameters neural network learning denoise objective training maximize likelihood observed data R T T optimizing evidence lower bound ELBO training denoising network ϵθ aims reconstruct noised input xt predicting added noise ϵ minimizing noise prediction error Lt h ϵ t condition model additional context information
c audio incorporate c ϵθ replacing µθ xt t Σθ xt t µθ xt t c Σθ xt t c Transformer Block Biased Attention Utilizing conventional Transformer architecture outputs self attention layer attention layer Ac determined follows Attention Qs Ks Vs Ac Attention Qc Kc Vc Qs Ks Vs represent query key value features self attention Qc Kc Vc RT counterparts cross attention square brackets indicate concatenation sequence sion MoDiT Pipeline shown Figure pipeline crafted generate new video frames given source audio single source image audio image pre processed Figure Illustration structure details Transformer Block designed integrated Bias Injection Strategy
showing use spatial temporal conditions enhance attention anisms improved feature extraction audio latent initial input prediction module poral Spatial Conditions guide subsequent Diffusion Transformer Pipeline composed Transformer Blocks denoising stage blocks handle expression tions create intermediate states Finally ping Net uses outputs predict poses dered new frames Renderer Transformer Block Biased Attention shown Figure transformer block method incorporates multi faceted attention mechanism enhance temporal spatial feature extraction begins self attention captures input noise latent Xt condition initial followed cross attention integrate audio condition shown Equation Temporal attention applied refine sequence time feed forward network FFN processes attention outputs producing predicted
noise ϵt block modulated bias injection egy stated introduces spatial temporal biases phase decider operates express lip sync phases optimizing model tasks ing precise temporal synchronization spatial alignment Bias Injection Strategy Bias Injection Module inspired Diffspeaker novel element crafted enhance diffusion process segmenting specific phases utilizing focused tention mechanisms details Bias Injection egy refer Algorithm early stages diffusion noise levels high module prioritizes lip ment diagonal bias bias helps concentrate Algorithm Bias Injection Strategy HyperParameter Latent Length L Diffusion Steps T Threshold tT Diagonal bias MD Dispersed expression bias Expression Attention AE Output Generated expression xt C channel size t
T ϵ t ϵ ˆxt t t tT AE MD AE t xt p return computational resources lip synchronization ensuring precise alignment despite presence substantial noise diffusion progresses noise diminishes ule shifts focus overall facial motions later stage dispersed expression bias applied cilitate smooth natural transitions facial expressions phased approach enhances computational ficiency significantly improves lip tion rate resulting harmonious blend detailed lip movements facial motions details Bias Injection Strategy refer Algorithm Revised Temporal Attention Jitter Removal Temporal attention fundamental mechanism capturing dependencies time steps dynamically weighing past future information Traditionally tion scores αt computed query key vectors Qt Kt
αt softmax t effective approach struggles adapt plex temporal variations treats time steps uniformly incorporating additional contextual information address limitation propose novel enhancement introducing learned latent variable zt cally adjusts attention scores based sequence text Specifically initial attention scores st refined transformation function implemented MLP enhanced attention mechanism st QtKT t st αt modification allows model adaptively adjust Figure mapping net structure generating realistic eye blinks featuring Blink Module head movement refinement attention weights based temporal context enabling better capture subtle patterns variations time Blinking Module Design structure modified mapping net shown ure essential generating realistic eye blinks video sequences
module designed ensure eye area effectively triggers blinks mapping net generates realistic eye blinks employing learnable blink sequence determine degree eye sure frame relying simple binary signal Blink Module fuses input features blink sequence multiple convolutional layers followed Leaky ReLU activations allowing network capture complex temporal dependencies subtle ances blinking behavior architecture iterated N times refine model ability generate natural blink sequences dynamically adjust blink intensity thermore module pre trained cropped dataset focusing eye movements ensuring blink quence accurately reflects degree eye closure pre training step critical better align blink sequence parameters enabling direct trol eye area structure network integrates
fully connected FC layers followed max activations refine outputs related head ments yaw pitch roll translation parameter included adjustments similar SadTalker Renderer Precise Flow Prediction shown Figure renderer system integrates multiple components generate realistic video frames source audio images Initially Figure Structure step renderer pipeline use result reference U Net models reference denoising Morphable Models pose blink adjustments module aligns lip movements input audio ducing synchronized reference serves input Reference UNet key tribution lies designing pipeline generates precise accurate optical flow moving simply ing output Specifically introduce erence UNet self attention enhance feature tion focus learning optical flow guides tic
facial feature movements Simultaneously aids capturing facial dynamics pose blink facilitated existing Mapping Net outputs stages fed Denoising UNet predicts optical flow necessary warping mechanism generate new frames Finally frames enhanced RefineNet ensure high quality coherent video Crucially novelty system lies tion flow based warping mechanism ment optical flow predictions better capture natural synchronized audio visual dynamics leveraging attention mechanisms effectively integrate audio visual cues data method ensures seamless realistic rendering process Training Loss addition noise prediction loss Equation incorporate velocity loss Lv Initially derive predicted motion ˆxt estimated noise ˆϵt velocity loss computed mean square error velocity Lv E
h utilize pre trained lip reading models compute lip reading loss Lread landmark loss Llks Overall final loss ExpNet L λtLt λreadLread λlksLlks λvLv λt λread λlks λv set Figure Comparison lip syncing models Visual results MakeItTalk AniPortrait SadTalker proposed method compared ground truth GT Note results SadTalker use GFPGAN enhancement qualitative generation results shown supplementary document Dataset HDTF VFHQ Identity Identity Cross Identity Identity Cross Identity LSE LSE LSE LSE LSE LSE LSE LSE MakeItTalk AniPortrait SadTalker DreamTalk Hallo Ground Truth Table Comparison state art method HDTF VFHQ dataset evaluate shot settings achieves best video quality animates lip region
regions original frame Experiment Datasets Following settings videos HDTF VFHQ Datasets duration onds video initial frame input second audio segment crop original videos following previous image animation ods resize video select aligned videos audios subjects train model Implementation Details pipeline utilize base version audio encoder trained model trained hours LibriSpeech dataset audio signals converted form spectral frequency latent subsequent ing parameters extracted trained deep face reconstruction method form experiments GPUs model learned continuous frames implemented Transformer model configuration includes dimensions hidden states dimensions ward networks alongside heads multi head attention single Transformer block self cross attention Transformer utilizes residual connections
meaning output attention operation summed input training sessions employed AdamW optimization algorithm setting learning rate diffusion step Evaluation Metrics demonstrate superiority method multiple metrics widely previous studies evaluate perceptual differences mouth shape including distance score LSE D confidence score LSE C ploy Frechet Inception Distance FID Compare State art Methods primary analysis conduct identity periment frame test video reference image corresponding audio functions driving signal creating video aligned sions different head movements identity experiment driving audio sourced video method frequently employed parisons video driven face reenactment compare method state art methods different approaches audio sion generations talking head generation MakeItTalk
based talking head generation SadTalker diffusion based talking head generation Aniportrait Hallo shown Table proach demonstrates best lip sync performance Identity datasets comparable results Cross Identity scenarios strate scalability model evaluated completely distribution dataset shown Table model achieves performance efficiency ble SOTA methods important note Hallo outperforms approaches training significantly larger dataset Additionally visualize samples strate performance shown Figure observe method produce rate smoother portrait video results demonstrate method effectively captures spatial temporal spondences enhancing generation lip synced videos boosting generation performance diffusion transformer model Testing distribution Dataset demonstrate scalability model evaluated completely distribution dataset shown Table added Mouth Landmark
tance LMD Face Action Unit Error AUE better demonstrating spatial temporal consistency Model LSE LSE Inference Time DreamTalk SadTalker Hallo Ground Truth Table Evaluation VoxCeleb HQ dataset Ablation studies Ablation pipeline component ablation study HDTF dataset Table evaluated impact key components model spatial temporal coherence Removing efficient constraints significantly reduced spatial ence affected lip synchronization low LSE D high LSE C emphasizing role maintaining spatial temporal alignment Excluding bias injection egy impaired spatial alignment local lip focus shown attention heatmap shifts Figure highlighting portance guiding attention facial regions generation Omitting temporal attention slightly reduced frame frame consistency indicating role ensuring
fluid animations addition Lv improved ence controlling velocity variations smoother tions LSE LSE source bias injection revised temporal attention Lv Table Ablation studies pipeline component HDTF dataset table evaluates impact excluding source bias injection temporal attention velocity loss Ablation Renderer Table highlights importance component model Removing source reference significantly reduced performance emphasizing roles Figure figure visualizes attention heatmap shifts middle final bias injection strategy maintaining coherence precision synchronization Figure shows comparison results different ers Overall model outperformed alternatives demonstrating structure components essential achieving high quality rendering LSE LSE source Reference Unet Table Ablation studies effect renderer table evaluates impact excluding
source erence UNet main table experiment evident achieves strong performance LSE C LSE D metrics attributed fact focuses exclusively generating changes mouth region outputs produced relatively low resolution Similarly limited scope generation performs FID metric Furthermore comparing results Hallo important note ing datasets differ significantly According Hallo paper method relies large private data limits ability assess contribution approach comparable manner Figure figure ablation study rendering comparing SadTalker results Ablation eye blink ablation study examined impact components eye blink generation model Removing Blink Module resulted significant drop ism blinks demonstrating role capturing temporal Figure Ablation study Eye Blink source image mum
blink distance SadTalker dependencies tested influence learned latent variable replacing binary signal led natural blink transitions Figure present detailed comparison maximum blink distance eyes normal state utilizing SadTalker model Blinking rapid process typically captured frames model demonstrates superior performance model achieves greater maximum blink distance ing lifelike natural looking blink compared SadTalker improvement enhances overall realism video providing authentic visual experience Supplementary Video Appendix comparison different renderers Conclusion conclusion MoDiT represents step forward field audio driven talking head generation integrating Morphable Model Diffusion based Transformer framework approach seeks address challenges spatial temporal inconsistencies promising sults lip synchronization realism animations leveraging
spatial control employing hierarchical attention strategy framework aims duce issues like jitter identity shifts encouraging natural expressions Furthermore improved blinking strategy contributes expressiveness generated videos enhancing potential user engagement results validated datasets Voxceleb HDTF VFHQ indicate robustness method tial contribute advancements digital human ogy work addresses existing limitations view foundation exploration vation virtual communication technologies hope research inspires future efforts refine expand References Volker Blanz Thomas Vetter morphable model synthesis faces ACM SIGGRAPH Lele Chen Ross K Maddox Zhiyao Duan Chenliang Xu Hierarchical cross modal talking face generation dynamic pixel wise loss CVPR Joon Son Chung Amir Jamaludin Andrew Zisserman
said arXiv preprint Joon Son Chung Andrew Senior Oriol Vinyals Andrew Zisserman Lip reading sentences wild CVPR Yu Deng Jiaolong Yang Sicheng Xu Dong Chen Yunde Jia Xin Tong Accurate face reconstruction weakly supervised learning single image image set CVPR Workshops Chenpeng Du Qi Chen Tianyu Xu Tan Xie Chen Kai Yu Sheng Zhao Jiang Bian Dae talker High fidelity speech driven talking face generation diffusion coder ACM MM Ian Goodfellow Jean Pouget Abadie Mehdi Mirza Bing Xu David Warde Farley Sherjil Ozair Aaron Courville Yoshua Bengio Generative adversarial networks nications ACM Yudong Guo Keyu Chen Sen Liang Yong Jin
Liu Hujun Bao Juyong Zhang Ad nerf Audio driven neural ance fields talking head synthesis ICCV Kazi Injamamul Haque Zerrin Yumak Facexhubert Text speech driven e x pressive facial animation synthesis self supervised speech representation ing ICMI Tianyu Junliang Guo Runyi Yu Yuchi Wang Jialiang Zhu Kaikai Leyi Li Xu Tan Chunyu Wang Han Hu Gaia Zero shot talking avatar generation arXiv preprint Martin Heusel Hubert Ramsauer Thomas Unterthiner Bernhard Nessler Sepp Hochreiter Gans trained time scale update rule converge local nash rium NeurIPS Fa Ting Hong Dan Xu Implicit identity representation conditioned memory compensation network talking head video generation
ICCV Fa Ting Hong Li Shen Dan Xu aware generative adversarial network talking head video generation arXiv preprint Fa Ting Hong Longhao Zhang Li Shen Dan Xu Depth aware generative adversarial network talking head video generation CVPR Fa Ting Hong Zunnan Xu Zixiang Zhou Jun Zhou Xiu Li Qin Lin Qinglin Lu Dan Xu Audio visual trolled video diffusion masked selective state spaces modeling natural talking head generation arXiv preprint Xinya Ji Hang Zhou Kaisiyuan Wang Wayne Wu Chen Change Loy Xun Cao Feng Xu Audio driven emotional video portraits CVPR Xinya Ji Hang Zhou Kaisiyuan Wang Wayne Wu Chen
Change Loy Xun Cao Feng Xu Audio driven emotional video portraits CVPR Salman Khan Muzammal Naseer Munawar Hayat Syed Waqas Zamir Fahad Shahbaz Khan Mubarak Shah Transformers vision survey arXiv Hyeongwoo Kim Pablo Garrido Ayush Tewari Weipeng Xu Justus Thies Matthias Niessner Patrick Pérez Christian Richardt Michael Zollhöfer Christian Theobalt Deep video portraits TOG Diederik P Kingma Max Welling Auto encoding tional bayes CoRR Tao Liu Feilong Chen Shuai Fan Chenpeng Du Qi Chen Xie Chen Kai Yu Anitalker Animate vivid verse talking faces identity decoupled facial motion encoding arXiv preprint Yunfei Liu Lijian Lin Fei Yu Changyin Zhou Yu
Li Moda Mapping audio driven portrait animation dual attentions ICCV Yuanxun Lu Jinxiang Chai Xun Cao Live speech traits real time photorealistic talking head animation TOG Pingchuan Ma Yujiang Wang Stavros Petridis Jie Shen Maja Pantic Training strategies improved reading ICASSP Yifeng Ma Shiwei Zhang Jiayu Wang Xiang Wang Yingya Zhang Zhidong Deng Dreamtalk expressive talking head generation meets diffusion probabilistic models arXiv preprint Zhiyuan Ma Xiangyu Zhu Guojun Qi Chen Qian iang Zhang Zhen Lei Diffspeaker Speech driven facial animation diffusion transformer arXiv preprint Vassil Panayotov Guoguo Chen Daniel Povey Sanjeev Khudanpur Librispeech asr corpus based public main audio
books ICASSP K R Prajwal Rudrabha Mukhopadhyay Vinay lip sync expert need speech lip generation wild ACM MM Robin Rombach Andreas Blattmann Dominik Lorenz Patrick Esser Björn Ommer High resolution image thesis latent diffusion models CVPR Maximilian Seitzer pytorch fid FID Score PyTorch Version Shuai Shen Wenliang Zhao Zibin Meng Wanhua Li Zheng Zhu Jie Zhou Jiwen Lu Difftalk Crafting diffusion models generalized audio driven portraits animation CVPR Aliaksandr Siarohin Stéphane Lathuilière Sergey Tulyakov Elisa ACM Transactions Graphics Nicu Sebe order motion model image animation NeurIPS Jiaming Song Chenlin Meng Stefano Ermon Denoising diffusion implicit models arXiv preprint Yang Song
Jingwen Zhu Dawei Li Xiaolong Wang Hairong Qi Talking face generation conditional rent adversarial network arXiv preprint Michał Stypułkowski Konstantinos Vougioukas Sen Maciej Stavros Petridis Maja Pantic Diffused heads Diffusion models beat gans talking face tion WACV Xusen Sun Longhao Zhang Hao Zhu Peng Zhang Bang Zhang Xinya Ji Kangneng Zhou Daiheng Gao Liefeng Bo Xun Cao Vividtalk shot audio driven ing head generation based hybrid prior arXiv preprint Yasheng Sun Hang Zhou Kaisiyuan Wang Qianyi Wu Zhibin Hong Jingtuo Liu Errui Ding Jingdong Wang wei Liu Koike Hideki Masked lip sync prediction audio visual contextual exploitation transformers GRAPH Asia
Supasorn Suwajanakorn Steven M Seitz Ira Kemelmacher Shlizerman Synthesizing obama ing lip sync audio TOG Shixiang Tang Yizhou Wang Lu Chen Yuan Wang Sida Peng Dan Xu Wanli Ouyang Human centric tion models Perception generation agentic modeling IJCAI Justus Thies Mohamed Elgharib Ayush Tewari Christian Theobalt Matthias Nießner Neural voice puppetry Audio driven facial reenactment ECCV Linrui Tian Qi Wang Bang Zhang Liefeng Bo Emo Emote portrait alive generating expressive portrait videos diffusion model weak conditions arXiv preprint Liangbin Xie Xintao Wang Honglun Zhang Chao Dong Ying Shan Vfhq high quality dataset benchmark video face super resolution Huawei Wei Zejun
Yang Zhisheng Wang Aniportrait Audio driven synthesis photorealistic portrait animation arXiv preprint Xin Wen Miao Wang Christian Richardt Ze Yin Chen Shi Min Hu Photorealistic audio driven video portraits TVCG Dan Xu Elisa Ricci Wanli Ouyang Xiaogang Wang Nicu Sebe et al Monocular depth estimation multi scale continuous crfs sequential deep networks TPAMI Mingwang Xu Hui Li Qingkun Su Hanlin Shang Liwei Zhang Ce Liu Jingdong Wang Yao Yao Siyu Zhu Hallo Hierarchical audio driven visual synthesis portrait image animation arXiv preprint Shuangjie Xu Yu Cheng Kang Gu Yang Yang Shiyu Chang Pan Zhou Jointly attentive spatial temporal pooling networks
video based person identification ICCV Sicheng Xu Guojun Chen Yu Xiao Guo Jiaolong Yang Chong Li Zhenyu Zang Yizhong Zhang Xin Tong Baining Guo Lifelike audio driven talking faces generated real time arXiv preprint Wenxuan Zhang Xiaodong Cun Xuan Wang Yong Zhang Xi Shen Yu Guo Ying Shan Fei Wang Sadtalker Learning realistic motion coefficients stylized driven single image talking face animation CVPR Zhimeng Zhang Lincheng Li Yu Ding Changjie Fan Flow guided shot talking face generation resolution audio visual dataset CVPR Jian Zhao Hui Zhang Thin plate spline motion model image animation CVPR Shuling Zhao Fa Ting Hong Xiaoshui
Huang Dan Xu Synergizing motion appearance Multi scale pensatory codebooks talking head video generation CVPR Hang Zhou Yu Liu Ziwei Liu Ping Luo Xiaogang Wang Talking face generation adversarially disentangled audio visual representation AAAI Hang Zhou Yasheng Sun Wayne Wu Chen Change Loy Xiaogang Wang Ziwei Liu Pose controllable talking face generation implicitly modularized audio visual resentation CVPR Yang Zhou Xintong Han Eli Shechtman Jose ria Evangelos Kalogerakis Dingzeyu Li Makelttalk speaker aware talking head animation TOG
cs RO Jul NavigScene Bridging Local Perception Global Navigation Visual Range Autonomous Driving Qucheng Peng Center Research Computer Vision University Central Florida Orlando FL USA Chen Bai XPENG Motors Santa Clara USA Guoxiang Zhang XPENG Motors Santa Clara USA Bo Xu XPENG Motors Santa Clara USA Xiaotong Liu XPENG Motors Santa Clara USA Xiaoyin Zheng XPENG Motors Santa Clara USA Chen Chen Center Research Computer Vision University Central Florida Orlando FL USA Cheng Lu XPENG Motors Santa Clara USA Abstract Autonomous driving systems significant advances perception prediction planning based local visual information struggle incorporate broader navigational context human drivers routinely
utilize address critical gap local sensor data global navigation information proposing NavigScene auxiliary navigation guided natural language dataset simulates human like driving environment autonomous driving systems develop complementary paradigms leverage NavigScene guided Reasoning enhances vision language models corporating navigation context prompting approach Navigation guided Preference Optimization reinforcement ing method extends Direct Preference Optimization improve vision language model responses establishing preferences navigation relevant summarized information guided Vision Language Action model integrates navigation guidance vision language models conventional driving models feature fusion Extensive experiments demonstrate approaches significantly improve performance ception prediction planning question answering tasks abling reasoning capabilities visual range improving generalization
diverse driving scenarios work represents significant step comprehensive autonomous driving systems capable navigating complex unfamiliar environments greater reliability safety CCS Concepts Computing methodologies vision tasks mation systems databases work Qucheng Peng internship XPENG Motors Silicon Valley office USA Keywords Vision language Model Multi modal Learning Autonomous ing Introduction b Planning global navigation guidance Planning global navigation guidance Question action ego Answer Continue straight Question action ego Answer Turn right right turn lane Navigation straight meters turn right Navigation Figure Comparison planning global navigation guidance b planning global navigation guidance ple vehicle needs turn right corner view range BVR knowledge navigation
planner makes conservative decision continue straight global BVR knowledge appropriately directs vehicle merge right turn lane Concrete examples experiments shown Fig Fig Autonomous driving systems remarkable progress recent years enabling vehicles perceive immediate surroundings predict movement nearby objects plan appropriate actions systems categorized main types vision language models VLMs question answering tasks end end ing models perception prediction planning approaches primarily rely responses agent environment visual range normally meters autonomous vehicles creates critical gap corporating global contextual information human like long term ACM MM Dublin Ireland Peng et al planning limitation constrains VLMs end end els hindering ability reason proactively
generalize unfamiliar scenes real world driving scenarios navigation applications Google Maps serve essential tools provide global tual information human drivers applications communicate ego vehicle intended future maneuvers turning left right proceeding straight alongside critical pieces mation distance upcoming maneuvers intersection type presence traffic signals Notably distance information cally extends visual perception capabilities onboard sensors cameras LiDAR classified visual range BVR information Despite cial effective planning decision making BVR information remains largely unexplored autonomous driving research Current datasets models predominantly focus frame frame perception prediction extend frame supervision adequately addressing global navigation context necessary comprehensive scene understanding long term planning Fig
demonstrate navigation guidance enhances question answering performance end end planning scenario navigation provides critical information indicating intersection located meters ahead ego vehicle execute right turn limited perception range onboard sensors typically restricted meters ego vehicle detect intersection sufficient advance notice initiate necessary lane change right turn lane contrast incorporating global BVR knowledge navigation tools planner proactively directs vehicle merge right turn lane advance upcoming turn demonstrating tangible benefits navigation guided planning address gap propose NavigScene auxiliary based dataset derived nuScenes NAVSIM datasets natural language navigation instructions ulate human like driving environment autonomous driving systems effectively imitating navigation tools Google Maps
provide BVR knowledge critical driving decisions ning NavigScene comprises subsets NavigScene nuScenes NavigScene NAVSIM dataset bridges disconnection tween local sensor data global navigation context providing paired data multi view sensor inputs images videos alongside corresponding natural language navigation guidance captures global driving environment carefully constructed pairing enables autonomous systems reason comprehensively driving scenarios informed planning decisions emulate human behaviors guided navigation applications Building human mimicking auxiliary dataset NavigScene propose paradigms leverage navigation guidance autonomous driving tasks like perception prediction planning navigation guided reasoning mented Navigation guided Supervised Fine tuning NSFT driving related tasks incorporating navigation ance prompts enable comprehensive reasoning
considers local visual cues global navigational context significantly improving model ability answer questions quiring knowledge immediate visual range Second reinforcement learning method named Navigation guided Preference Optimization NPO introduce auxiliary text summarization task enhance Direct Preference Optimization DPO lishing preference relationship summarized answers vision language model navigation guidance proving vision language models BVR reasoning generalization capabilities post training VLMs navigation guidance consists NSFT NPO Navigation guided Language Action NVLA model develop VLA baseline chitecture integrates navigation guidance vision language models conventional end end driving models feature fusion creating robust representation downstream tasks including perception prediction planning contributions summarized main aspects
propose NavigScene novel auxiliary dataset pairs local multi view sensor inputs global natural language navigation guidance addressing critical gap local perception global navigation context autonomous driving implement NavigScene complementary paradigms navigation guided reasoning navigation guided preference mization navigation guided vision language action model enhancing autonomous driving systems reasoning ization capabilities visual range limitations conduct comprehensive experiments tasks end end driving tasks including perception prediction planning demonstrating significant performance ments achieved incorporating global navigation knowledge autonomous driving systems Related Works LLMs VLMs Autonomous Driving NuScenes QA Visual Question Answering benchmark specifically signed autonomous driving scenarios establishing foundation baselines leverage advanced detection
VQA techniques introduces interpretable end end autonomous driving system powered Large Language Models DriveLM enhances Visual Language Models reasoning capabilities graph based visual question answering struct places greater emphasis crucial multi view poral information Visual Language Models essential robust autonomous driving systems VLP proposes novel framework exploits LLMs bridge gap linguistic understanding autonomous driving End end Autonomous Driving VAD employs ego query mechanism predict single mode trajectories vances approach implementing probabilistic space based multiple trajectories SparseDrive innovates designing lel motion planning modules reduce computational demands BEV features DiffusionDrive introduces truncated fusion policy enhance trajectory probabilistic representation MomAD focuses improving stability
maintaining consistency consecutive planning decisions Navigation based Datasets NavigScene Existing autonomous driving datasets predominantly emphasize local level descriptions key frames serve perception tasks NavigScene Bridging Local Perception Global Navigation Visual Range Autonomous Driving ACM MM Dublin Ireland Origin Latitude Longitude latitude longitude x z y Translation Vectors latitude longitude latitude longitude Source Latitude Longitude Destination Latitude Longitude Google Maps APIs Navigation Video Source Destination Sampling Video Prompt Given sequence F sampled frames navigation video constant speed Total path length meters analyze driving path answer intersections interchanges path intersections interchanges Return exactly Continue current road meters YES intersections interchanges Previous distance
intersection interchange encountered provide Distance point Frame intervals point Previous interval F meters Previous interval Frame intervals point Presence traffic signals traffic signs Required direction adjustment left turn right turn continue straight think step step structure response exactly format paragraph summary concrete distances segment mention calculations VLM based Guidance Generation Candidate Candidate N Self consistency Evaluation Generate N responses Prompt Begin journey continuing straight current road approximately meters making left turn intersection traffic signal Proceed straight meters intersection traffic signals Finally traveling additional meters right turn intersection stop sign Continue straight path complete journey Selected Candidate Navigation Guidance Visual Generation
B Text Generation Figure Navigation guidance generation process scene Visual Generation Source destination coordinates calculated origin coordinate translation vectors navigation video constructed Google Maps APIs evenly sampled extract multiple frames B Text Generation multiple frames processed vision language model specialized prompt generate candidate responses Self consistency evaluation selects highest scoring candidate final navigation guidance effectively adequately address visual range BVR knowledge essential scene understanding decision making limitation particularly significant given navigation applications integral components real world driving scenarios especially experienced human drivers traversing unfamiliar territories assessing current road tions simulate human like driving environments bridge gap propose NavigScene auxiliary navigation
guided natural language dataset simulates navigation applications provide global contextual information enhancing capability autonomous systems reason generalize BVR knowledge complex driving environments proposed NavigScene dataset derived scenes nuScenes dataset NAVSIM consists subsets NavigScene nuScenes NavigScene NAVSIM generation process single scene shown Fig Visual Generation establish scene determine latitudes tudes source destination calculation rates origin coordinates translation vectors source destination origin origin nates 𝜙 𝜆 𝜙represents latitude 𝜆represents longitude decimal degrees translation vector Δ𝑥 Δ𝑦 Δ𝑧 meters Δ𝑥denotes eastward component Δ𝑦the ward component Δ𝑧the upward component coordinates source destination calculated 𝜋 Δ𝑦 𝑅 𝜋 Δ𝑥 𝑅 cos 𝜋 𝜙 𝑅represents Earth
radius approximately Δ𝑥and Δ𝑦represent translations eastern northern directions meters Δ𝑧is excluded latitude longitude calculations influence horizontal positioning leverage Google Maps APIs generate navigation videos coordinates Direction API provides cise routes Static Map API acquires sequential images routes Distance Matrix API estimates driving distance duration Assuming constant velocity synthesize realistic tion videos simulating driving experience facilitate analysis evenly sample 𝐹frames videos subsequent text generation VLM Sec Text Generation Sec visual generation Fig integrating complete navigation videos VLMs end end architectures poses significant challenges alignment difficulties navigation videos sensor based training data address limitation transform sequential images natural language navigation descriptions VLMs
B Fig sequence frames generate 𝑁candidate navigations specialized prompt shown Fig prompt analyzes intersections interchanges determine driving directions estimates distances based frame intervals obtaining 𝑁candidate responses implement novel selection strategy identify optimal description define similarity metrics 𝑆𝑖𝑛𝑡𝑒𝑟 𝑆𝑑𝑖𝑠𝑡 𝑆𝑤𝑜𝑟𝑑 𝑆𝑖𝑛𝑡𝑒𝑟 represents intersection similarity emphasizing tional keywords accuracy candidate 𝑎𝑖 directional keywords extracted 𝑚𝑖 intersection similarity 𝑎𝑖and 𝑎𝑗is 𝑚𝑖 𝑚𝑗 𝑑for 𝑑 𝑆𝑑𝑖𝑠𝑡 represents distance value similarity Distance values 𝑎𝑖are 𝑛𝑖 distance similarity 𝑑 𝑆𝑤𝑜𝑟𝑑 represents lexical similarity calculated card index overall similarity score 𝑆𝑜𝑣𝑒𝑟 candidates ACM MM Dublin Ireland Peng et al Given directional accuracy critical followed tance precision lexical
similarity assign weights select optimal answer identifying candidate highest cumulative similarity arg max approach identifies best candidate serves final navigation simulate human like driving environment Methodology Navigation guided Reasoning Language Model Multi view images videos Question action ego Prompt Answer action continue straight Language Model Multi view images videos Question action ego Prompt Answer action turn right turn lane Navigation straight meters turn right b Navigation guided VLM Reasoning Non navigation VLM Reasoning Figure Comparison non navigation VLM reasoning b navigation guided VLM reasoning proposed navigation guided paradigm navigation guidance question form prompt VLM Best viewed zoomed traditional tasks autonomous
driving view images videos paired questions serve input VLM shown Fig reasoning paradigm limited scope information overlooks visual range BVR information crucial long term planning address limitation incorporate navigation guidance prompting approach shown Fig enriching reasoning process essential global information navigation guided reasoning represented 𝑀is vision language model 𝑎represents natural language outputs answer 𝑣denotes multi view images videos 𝑔is navigation guidance 𝑞is question concatenation 𝑔and 𝑞serves prompt paradigm applied Navigation guided Supervised Fine Tuning NSFT related task paper Navigation guided Preference Optimization Section introduced NSFT enhance VLM reasoning capabilities approach limitations generalizing unseen scenarios VLMs fewer parameters supervised fine tuning
restricts generalization ability reasoning pacity constrained parameter scale address comings propose Navigation guided Preference Optimization NPO extension Direct Preference Pptimization DPO applied NSFT improve generalization performance novel navigation scenarios NPO builds DPO integrating navigation related edge auxiliary text summarization task NPO VLM processes multi view images 𝑣and question 𝑞as inputs establish preference relationship detailed answer 𝑎 summarized version summarization 𝑠is generated online VLM 𝑀 𝑎is original answer supervised fine tuning pt prompt Summarize answer driving relevant question simple losing important information Following DPO methodology initialize learnable reward model 𝑀𝜃and frozen reference model obtain 𝑠𝜃 VLMs respectively navigation guidance 𝑔 quantify
quality summarized answer 𝑠using mutual information log log log Eq goals simplify summarized answer pared original answer 𝑎 enhancing relevance tween summarized answer 𝑠and navigation Based implementation Equation simplified incorporating measurement define reward summarized answers 𝛼is trade hyper parameter reward sures difference summarized answer 𝑠between reward model 𝑀𝜃and reference model difference guidance relevance summarized answers Similarly reward original answer 𝑎is objective function NPO formulated 𝜎is sigmoid function 𝐷represents preference dataset contains tuples multi view images question answer summary reward model summary reference model proposed NPO method introduce auxiliary task navigation guided text summarization reward model reference model strategic addition directs
reward model focus guidance relevant knowledge significantly hancing ability generate driving relevant concise responses preserving critical information aligned navigation ments integrated end end driving model Sec paradigm substantially improves overall autonomous driving system generalization capability Navigation guided Vision Language Action Model question answering tasks navigation guidance substantially enhances end end driving system performance Conventional end end models rely solely sensor data view images videos suffer limited reasoning capabilities NavigScene Bridging Local Perception Global Navigation Visual Range Autonomous Driving ACM MM Dublin Ireland Prompt Continue straight meters turn left traffic signal Contine straight meters road Navigation Guidance Multi view Images Videos
BEV Encoder Language Model BEV Features Feature Fusion MLP Fused Features Concatenate Perception Prediction Planning Task specific Networks Sparsity Reduction MLP Output Probability Figure Navigation guided vision language action model end end driving BEV features concatenated vision language tures generated frozen VLM learnable sparsity reduction MLP processed learnable feature fusion MLP produce fused features task specific networks poor generalization novel scenarios address comings propose Navigation guided Vision Language Action NVLA model integrates navigation guidance language models end end driving framework conventional end end models output driving task prediction planning represented 𝑜con 𝑗 𝑣denotes multi view images videos 𝐸is BEV encoder
𝐻𝑗is task specific network 𝑜con 𝑗 output task 𝑗 navigation guidance navigation guided VLA model incorporate VLM post trained NSFT NPO navigation guidance output probability distribution modern VLMs typically high dimensionality large vocabulary space example LlamaAdapter output probability dimension ing direct alignment fusion BEV features typically mensions models like SparseDrive challenging address mismatch introduce learnable sparsity reduction MLP 𝜙red compress VLM features dimension followed learnable feature fusion MLP 𝜙fus complete process represented 𝑜nav 𝑗 𝑀represents frozen VLM trained NSFT described Sec NPO described Sec 𝑔is navigation guidance 𝑜nav 𝑗 output task 𝑗with navigation guidance integration process illustrated Fig Experiments Table
Reasoning Results DriveLM nuScenes VLM NavigScene METEOR CIDEr SPICE GPT Comp Llama Adapter Datasets paper evaluate proposed NavigScene categories benchmark datasets category prises datasets designed supervised fine tuning forcement learning VLMs second category includes end end driving datasets assess integration NavigScene navigation guided VLMs autonomous driving models datasets utilize DriveLM nuScenes tains approximately scenes questions corresponding images camera views experiments allocate scenes testing remainder training employ NuInstruct dataset containing pairs images camera perspectives datasets derived nuScenes methodologically sound incorporate NavigScene nuScenes training processes end end driving evaluation employ benchmark datasets open loop nuScenes dataset paired nuScenes closed loop NAVSIM dataset
paired NavigScene NAVSIM nuScenes features driving scenes tured Boston Singapore spanning seconds prehensive bounding box annotations includes synchronized data cameras LiDAR sensor radars NAVSIM simulation dataset based nuPlan comprising hours driving data cameras LiDAR sensors Implementations NavigScene generation set weights VLMs evaluate Llama Llava VL fine tuning preference optimization apply LoRA rank learning rate output sequence length limited tokens maintaining default values parameters end end driving evaluation utilize VAD SparseDrive sparsity reduction MLP input channels match VLM ulary dimension output channels align original BEV features feature fusion MLP receives inputs twice original BEV feature dimension outputs tensors matching original BEV
features MLPs use learning rate preserving default values network parameters models employ AdamW optimizer NSFT NVLA epochs assigned original training VLMs end end models NPO conducted epochs experiments conducted times Nvidia GPUs Quantitative Results Tasks Tab compare performance VLMs post trained NavigScene NSFT NPO baselines eLM open source VLMs Following DriveLM evaluation protocol employ metrics including METEOR CIDEr SPICE GPT score completeness Comp Results consistently demonstrate NavigScene significantly enhances quality driving relevant responses VLMs Tab present results NuInstruct comparing line VLMs models post trained NavigScene Following NuInstruct evaluation protocols assess perception ties subtasks measuring distance Dis speed Spe instance number
Ins closest object identification Clo status recognition Sta road detection SaR prediction evaluate motion trajectory Mot future status Sta forecasting Risk assessment measured dimensions approach App lane changing Lan ongoing maneuvers Onc crossing Cro taking Ove braking Bra Results illustrate NavigScene significantly improves performance tested VLMs ing reasoning capabilities driving related tasks ACM MM Dublin Ireland Peng et al Table Reasoning Results NuInstruct VLM NavigScene Perception Prediction Risk Planning Dis Spe Ins Clo Sta SaR Mot Sta App Lan Onc Cro Ove Bra Llama Adapter scenario actions ego vehicle going straight speed turn left gradually going straight turn left going
straight Question NavigScene NavigScene Ground Truth Lidar scenario safe actions ego vehicle Lidar speed accelerate safe maintain current velocity increase speed proceed forward reduce speed gradually brakes going speed accelerate ahead decelerate gradually braking Question NavigScene NavigScene Ground Truth Figure Examples question answering DriveLM dataset motion ego reason speed maintain distance large vehicle proximity ego vehicle moving close ego ego speed braking vehicles safe distance ego ego straight Question NavigScene NavigScene Ground Truth Lidar plan ego reason pedestrian crossing ego car stay straight objects risks ego remain stationary pedestrian crossing vehicle Lidar Question NavigScene NavigScene Ground Truth Figure Examples question
answering NuInstruct dataset Qualitative Results Tasks Fig Fig examples VLM responses NavigScene integration BVR knowledge provided NavigScene significantly enhances VLM reasoning ities resulting complete accurate answers Additional samples exhibited Supplementary Material SM Quantitative Results End end Driving Tab compare end end driving configurations incorporating different VLAs original end end models open loop closed loop planning open loop tion following deactivate ego status adopt evaluation protocols UniAD results demonstrate integrating end end models VLMs NavigScene tially enhances performance yielding particularly significant improvements collision rate metrics closed loop evaluation assess planning performance tiple metrics fault collision NC drivable area compliance DAC time
collision TTC comfort Comf ego progress scores EP predictive driver model score PDMS reported percentages Integrating VLMs NavigScene nificantly improves system performance particularly DAC EP PDMS metrics correlate strongly human like ing capability accurate navigation interpretation results highlight importance incorporating BVR knowledge loop planning results based DriveLM additional results NuInstruct available SM Tab compare end end driving configurations tasks including detection tracking mapping motion casting observe NavigScene enhances driving system performance non planning tasks detection Notably detection mAP achieve improvements VAD SparseDrive Additional results NuInstruct available SM Qualitative Results End end Driving Fig present open loop planning examples comparing performance
NavigScene integration ing view range knowledge NavigScene enables autonomous driving system generate accurate driving mands route planning particularly evident right case vehicle correctly anticipates right turn ing early lane change assistance global navigation guidance Additional examples exhibited SM Fig present closed loop planning examples strating impact NavigScene integration left example vehicle intends switch fast track continue straight navigation guidance driving model incorrectly turns left Similarly right example vehicle needs turn right upcoming intersection NavigScene ues straight extended distance NavigScene appropriately slows waits opportunity turn right Extra examples shown SM Ablation Study Tasks Tables present ablation studies conducted nuScenes NuInstruct datasets
respectively examine perimental settings NSFT NPO VLM finetuned NavigScene NSFT VLM undergoes fine tuning NavigScene NPO VLM finetuned NavigScene trained NPO NavigScene NSFT NPO VLM finetuned NavigScene subsequently trained NPO NavigScene NavigScene Bridging Local Perception Global Navigation Visual Range Autonomous Driving ACM MM Dublin Ireland Table Results open loop closed loop planning settings Left Open loop planning performance nuScenes Right Closed loop planning performance NAVSIM VLMs underwent post training NSFT NPO DriveLM nuScenes NavigScene nuScenes followed prompting NavigScene nuScenes NavigScene NAVSIM respectively training End end Model VLM Open loop Planning nuScenes Closed loop Planning NAVSIM Collision Rate NC DAC TTC
Comf EP PDMS Avg Avg VAD Llama Adapter Sparse Drive Llama Adapter Table Object detection tracking mapping motion forecasting nuScenes VLMs post trained NSFT NPO nuScenes NavigScene nuScenes prompted NavigScene nuScenes training End end Model VLM Detection Tracking Mapping Motion Forecasting mAP mATE mASE NDS AMOTA AMOTP Recall IDS mAP mFDE MR EPA VAD Llama Adapter SparseDrive Llama Adapter NavigScene Turn Left NavigScene Straight Straight Ground Truth NavigScene Straight NavigScene Turn Right Ground Truth Turn Right Figure BEV visualization open loop planning nuScenes dataset NavigScene indicates original SparseDrive NavigScene VLA model integrates post trained DriveLM NavigScene NSFT NPO SparseDrive Arrows
point ego vehicles text left displays predicted driving commands orange curves represent predicted driving routes Left Vehicle proceeds straight reduces speed stop Right Vehicle anticipates right turn initiating early lane change NavigScene NavigScene Human NavigScene NavigScene Human Figure BEV Visualization closed loop planning NAVSIM dataset NavigScene indicates original SparseDrive NavigScene VLA model integrates post trained DriveLM NavigScene NSFT NPO SparseDrive Arrows point ego vehicles orange routes generated models green routes showing human operations Left Vehicle anticipates left turn initiating early lane change Right Vehicle anticipates right turn slowing waiting Based tables provided DriveLM nuScenes NuInstruct datasets demonstrate consistent performance ments
applying NSFT NPO VLMs significant performance gains occur NSFT NPO applied showing highest overall scores metrics datasets DriveLM nuScenes combination NSFT NPO improved METEOR CIDEr substantially achieving best CIDEr score Similarly NuInstruct dataset combined ACM MM Dublin Ireland Peng et al Table Ablation study DriveLM nuScenes VLM NSFT NPO METEOR CIDEr SPICE GPT Comp Llama Adapter approach led strong reductions Dis Spe results gest collaboration NSFT NPO techniques enhances VLMs ability understand complex driving scenarios generate appropriate responses Ablation Study Open loop Closed loop Planning Tab present ablation studies conducted nuScenes open loop planning NAVSIM closed loop planning tively examine
experimental settings VLM end end model applied NSFT NPO VLM finetuned NavigScene NSFT VLM undergoes fine tuning NavigScene NPO VLM finetuned igScene trained NPO NavigScene NSFT NPO VLM finetuned igScene subsequently trained NPO NavigScene VLMs frozen connected end end models struct VLA model autonomous driving Based ablation study Table obvious formance improvements incorporating NSFT NPO niques open loop closed loop planning VAD SparseDrive end end models observe SparseDrive erally outperforms VAD consistently forms Llama Adapter configurations Notably baseline systems VLM integration perform ably VLM integration proper post training techniques provides substantial gains particularly reducing collision rates improving trajectory accuracy synergistic
effect combining NSFT NPO demonstrates specific fine tuning followed preference optimization creates effective autonomous driving systems Cross city Generalization End end Driving Table present cross city generalization results cities nuScenes dataset Following examine fer tasks Boston Singapore task models trained Boston data evaluated zero shot manner Singapore data second task process reversed Additional results shown SM cross city generalization results demonstrate significant impact NPO autonomous driving systems generalization ity testing models trained Boston Singapore data vice versa VLA models NPO consistently outperform original end end architectures VAD SparseDrive VLA models NPO results highlight NPO effectiveness enhancing autonomous driving systems robustness
navigating unfamiliar urban environments different traffic patterns infrastructure designs Conclusion paper address critical limitation current autonomous driving systems disconnection local sensor data global navigation context introduced NavigScene iary navigation guided natural language dataset bridges gap simulating human like driving environments complementary paradigms based NavigScene guided Reasoning Navigation guided Preference Optimization Navigation guided Vision Language Action model achieve nificant improvements driving related tasks tion prediction planning enable reasoning capability yond visual range enhance generalization ability diverse ing scenarios work represents significant step comprehensive autonomous driving systems future research focus integrating dynamic multi modal navigation information word work brings autonomous driving tems
closer human like ability navigate complex unfamiliar environments improved reliability safety NavigScene Bridging Local Perception Global Navigation Visual Range Autonomous Driving ACM MM Dublin Ireland Table Ablation study NuInstruct VLM NSFT NPO Perception Prediction Risk Planning Dis Spe Ins Clo Sta SaR Mot Sta App Lan Onc Cro Ove Bra Llama Adapter Table Ablation study open loop closed loop planning settings Left Open loop planning performance nuScenes Right Closed loop planning performance NAVSIM VLMs NSFT NPO based DriveLM nuScenes NavigScene nuScenes End end Model VLM NSFT NPO Open loop Planning nuScenes Closed loop Planning NAVSIM Collision Rate NC DAC TTC
Comf EP PDMS Avg Avg VAD N N Llama Adapter SparseDrive N N Llama Adapter Table Cross city generalization results nuScenes VLMs conducted NSFT DriveLM nuScenes NavigScene nuScenes End end Model VLM NPO Boston Singapore Avg m Avg Col Avg m Avg Col VAD N Llama Adapter SparseDrive N Llama Adapter References Josh Achiam Steven Adler Sandhini Agarwal Lama Ahmad Ilge Akkaya rencia Leoni Aleman Diogo Almeida Janko Altenschmidt Sam Altman Shyamal Anadkat et al technical report arXiv preprint Peter Anderson Basura Fernando Mark Johnson Stephen Gould Spice Semantic propositional image caption evaluation Computer Vision ECCV European Conference Amsterdam Netherlands
October Proceedings V Springer Satanjeev Banerjee Alon Lavie METEOR automatic metric MT evaluation improved correlation human judgments Proceedings acl workshop intrinsic extrinsic evaluation measures machine translation summarization Jing Bi Yuting Wu Weiwei Xing Zhenjie Wei Enhancing ing Capabilities Small Language Models Solution Guidance Fine Tuning Proceedings International Conference Computational Linguistics Holger Caesar Varun Bankiti Alex H Lang Sourabh Vora Venice Erin Liong Qiang Xu Anush Krishnan Yu Pan Giancarlo Baldan Oscar Beijbom nuscenes multimodal dataset autonomous driving Proceedings IEEE CVF conference computer vision pattern recognition Holger Caesar Juraj Kabzan Kok Seang Tan Whye Kit Fong Eric Wolff Alex Lang Luke
Fletcher Oscar Beijbom Sammy Omari nuplan loop ml based planning benchmark autonomous vehicles arXiv preprint Li Chen Penghao Wu Kashyap Chitta Bernhard Jaeger Andreas Geiger Hongyang Li End end autonomous driving Challenges frontiers IEEE Transactions Pattern Analysis Machine Intelligence Shaoyu Chen Bo Jiang Hao Gao Bencheng Liao Qing Xu Qian Zhang Chang Huang Wenyu Liu Xinggang Wang End end vectorized autonomous driving probabilistic planning arXiv preprint Pranav Singh Chib Pravendra Singh Recent advancements end end autonomous driving deep learning survey IEEE Transactions Intelligent Vehicles Joao PA Dantas Marcos ROA Maximo Takashi Yoneyama tonomous agent visual range air combat deep reinforcement
ing approach Proceedings ACM SIGSIM Conference Principles Advanced Discrete Simulation Daniel Dauner Marcel Hallgarten Tianyu Li Xinshuo Weng Zhiyu Huang Zetong Yang Hongyang Li Igor Gilitschenski Boris Ivanovic Marco Pavone et al Navsim Data driven non reactive autonomous vehicle simulation benchmarking Advances Neural Information Processing Systems Xinpeng Ding Jianhua Han Hang Xu Xiaodan Liang Wei Zhang Xiaomeng Li Holistic autonomous driving understanding eye view injected multi modal large models Proceedings IEEE CVF Conference puter Vision Pattern Recognition Guanting Dong Hongyi Yuan Keming Lu Chengpeng Li Mingfeng Xue heng Liu Wei Wang Zheng Yuan Chang Zhou Jingren Zhou Abilities Large Language
Models Affected Supervised Fine tuning Data Composition Proceedings Annual Meeting Association Computational Linguistics Volume Long Papers ACM MM Dublin Ireland Peng et al Yulin Siqi Wang Wei Chen Tianci Xun Yusong Tan Sniffing Threatening Open World Objects Autonomous Driving Open Vocabulary Models Proceedings ACM International Conference Multimedia Yihan Hu Jiazhi Yang Li Chen Keyu Li Chonghao Sima Xizhou Zhu Siqi Chai Senyao Du Tianwei Lin Wenhai Wang et al Planning oriented autonomous driving Proceedings IEEE CVF conference computer vision pattern recognition Xiaosong Jia Zhenjie Yang Qifeng Li Zhiyuan Zhang Junchi Yan Multi Ability Benchmarking Closed Loop End End Autonomous Driving
Thirty Conference Neural Information Processing Systems Datasets Benchmarks Track Bo Jiang Shaoyu Chen Qing Xu Bencheng Liao Jiajie Chen Helong Zhou Qian Zhang Wenyu Liu Chang Huang Xinggang Wang Vad Vectorized scene representation efficient autonomous driving Proceedings IEEE CVF International Conference Computer Vision Peidong Li Dixiao Cui Navigation Guided Sparse Scene Representation End End Autonomous Driving Thirteenth International Conference Learning Representations Zhiqi Li Zhiding Yu Shiyi Lan Jiahan Li Jan Kautz Tong Lu Jose M Alvarez ego status need open loop end end autonomous driving Proceedings IEEE CVF Conference Computer Vision Pattern Recognition Bencheng Liao Shaoyu Chen Haoran Yin Bo
Jiang Cheng Wang Sixu Yan Xinbang Zhang Xiangyu Li Ying Zhang Qian Zhang Xinggang Wang DiffusionDrive Truncated Diffusion Model End End Autonomous Driving Proceedings IEEE CVF International Conference Computer Vision Wenlong Liao Sunyuan Qiang Xianfei Li Xiaolei Chen Haoyu Wang Yanyan Liang Junchi Yan Tao Pai Peng CalibRBEV Multi Camera Calibration Reversed eye view Representations Autonomous ing Proceedings ACM International Conference Multimedia Chin Yew Lin Rouge package automatic evaluation summaries Text summarization branches Haotian Liu Chunyuan Li Qingyang Wu Yong Jae Lee Visual instruction tuning Advances neural information processing systems Ilya Loshchilov Frank Hutter Decoupled Weight Decay Regularization International Conference
Learning Representations Ana Maria Marcu Long Chen Jan Hünermann Alice Karnsund Benoit Hanotte Prajwal Chidananda Saurabh Nair Vijay Badrinarayanan Alex Kendall Jamie Shotton et al LingoQA Visual question answering autonomous driving European Conference Computer Vision Springer Torsten Merz Farid Kendoul visual range obstacle avoidance infrastructure inspection autonomous helicopter IEEE RSJ International Conference Intelligent Robots Systems IEEE Ming Nie Renyuan Peng Chunwei Wang Xinyue Cai Jianhua Han Hang Xu Li Zhang interpretable chain based reasoning autonomous driving European Conference Computer Vision Springer Chenbin Pan Burhaneddin Yaman Tommaso Nesti Abhirup Mallik Alessandro G Allievi Senem Velipasalar Liu Ren Vlp Vision language planning
autonomous driving Proceedings IEEE CVF Conference Computer Vision Pattern Recognition Kishore Papineni Salim Roukos Todd Ward Wei Jing Zhu Bleu method automatic evaluation machine translation Proceedings annual meeting Association Computational Linguistics Tianwen Qian Jingjing Chen Linhai Zhuo Yang Jiao Yu Gang Jiang Nuscenes qa multi modal visual question answering benchmark autonomous driving scenario Proceedings AAAI Conference Artificial Intelligence Vol Rafael Rafailov Archit Sharma Eric Mitchell Christopher D Manning Stefano Ermon Chelsea Finn Direct preference optimization language model secretly reward model Advances Neural Information Processing Systems Fu Rong Wenjin Peng Meng Lan Qian Zhang Lefei Zhang ing Scene Understanding Traffic Scene
Assisted Topology Graph Proceedings ACM International Conference Multimedia Chonghao Sima Katrin Renz Kashyap Chitta Li Chen Hanxue Zhang Chengen Xie Jens Beißwenger Ping Luo Andreas Geiger Hongyang Li elm Driving graph visual question answering European Conference Computer Vision Springer Ziying Song Caiyan Jia Lin Liu Hongyu Pan Yongchang Zhang Junming Wang Xingyu Zhang Shaoqing Xu Lei Yang Yadan Luo Shake Wheel Momentum Aware Planning End End Autonomous Driving Proceedings IEEE CVF International Conference Computer Vision Gilbert Strang Kai Borre Linear algebra geodesy GPS Siam Wenchao Sun Xuewu Lin Yining Shi Chuang Zhang Haoran Wu Sifa Zheng Sparsedrive End end autonomous
driving sparse scene representation arXiv preprint Gabriel Svennerberg Beginning google maps API Apress Naftali Tishby Noga Zaslavsky Deep learning information bottleneck principle ieee information theory workshop itw Ieee Ramakrishna Vedantam C Lawrence Zitnick Devi Parikh Cider Consensus based image description evaluation Proceedings IEEE ference computer vision pattern recognition Peter West Ari Holtzman Jan Buys Yejin Choi BottleSum supervised Self supervised Sentence Summarization Information Bottleneck Principle Proceedings Conference Empirical ods Natural Language Processing International Joint Conference Natural Language Processing EMNLP IJCNLP Zhenhua Xu Yujia Zhang Enze Xie Zhen Zhao Yong Guo Kwan Yee K Wong Zhenguo Li Hengshuang Zhao Interpretable end
end autonomous driving large language model IEEE Robotics Automation Letters Yang Baosong Yang Beichen Zhang Binyuan Hui Bo Zheng Bowen Yu Chengyuan Li Dayiheng Liu Fei Huang Haoran Wei et al technical report arXiv preprint Renrui Zhang Jiaming Han Chris Liu Aojun Zhou Pan Lu Yu Qiao sheng Li Peng Gao LLaMA Adapter Efficient Fine tuning Large Language Models Zero initialized Attention Twelfth International Conference Learning Representations Wenzhao Zheng Ruiqi Song Xianda Guo Chenming Zhang Long Chen Genad Generative end end autonomous driving European Conference Computer Vision Springer
cs LG Jul Replacing thinking tool usage enables reasoning small language models Corrado Qualcomm AI Tim Qualcomm AI Roland Memisevic Qualcomm AI Abstract Recent advances established new machine learning paradigm based scaling compute inference time training time line work combination Supervised Fine Tuning SFT synthetic demonstrations Reinforcement Learning Verifiable Rewards RLVR training Large Language Models expend extra compute inference form thoughts expressed natural language paper propose instead format tokens multi turn interaction trace stateful tool turn new state tool appended context model job generate tokens necessary control tool custom DSL benchmark approach problem repairing malfunctioning Python code constrained setup allows
faster sampling experience denser reward signal allowing models size parameters learn proficiently expend additional compute task Introduction observed Wei et al prompting capable Language Models LMs think step step provide immediate answer lead significant performance boosts Currently Large Reasoning Models LRM Fengli et al generate additional tokens form Chains Thought CoTs order improve performance question task effectively new AI paradigm OpenAI DeepSeek AI LRMs aim improve performance task expending additional compute inference time class approaches referred Test Time Scaling TTS Inference time Compute ItC methods class includes approaches steer decoding process form tree search outputs scored engineered reward function verifier
model Beeching et al Snell et al OpenAI Liu et al idea ultimately traced way series works Silver et al TTS methods including LRMs taken embody thinking Ji et al slow deliberate thinking processes humans plan adapt previously unseen situations LRM training typically experience based outputs external demonstrations Zelikman et al Wang et al Setlur et al DeepSeek AI Hu et al Chu et al Recently taken form policy Reinforcement Learning RL tasks enable handcrafting opposed neural contribution AI Research initiative Qualcomm Technologies Preprint review modeling reward function verifiable tasks sourced domains math coding paradigm Reinforcement Learning Verifiable Rewards RLVR
compellingly demonstrated DeepSeek AI trained model reason policy RL CoTs Regardless reasoning learned experience expert demonstrations Ye et al Muennighoff et al additional tokens generated test time typically form reasoning steps described natural language Feng et al refer text based reasoning parameterization benefit generality representative power natural language comes pitfalls large models far bootstrapped CoTs DeepSeek AI smaller models attempt distill reasoning capabilities larger models subsequently attempt policy RL Dang Ngo attempt unlock reasoning behavior small models simple Supervised Fine Tuning SFT expert demonstrations Ye et al Muennighoff et al methods unproven work models fewer parameters suffer instabilities preclude stable
continuous learning experience Dang Ngo action space LM agent operates enormously large reward signal RLVR usually based task outcome DeepSeek AI Hu et al Dang Ngo available model completed reasoning produced answer challenges mitigated successful RLVR inevitably requires strong exploration prior small LMs struggle provide paper propose different parameterization thinking tokens reduces size action space allows denser reward signal benchmark proposal verifiable task repairing malfunctioning Python code demonstrate enables extending RLVR paradigm models small size Low Rank adaptation Hu et al proposal format thinking tokens tracing series interactions reasoner external tool refer Chain Edits CoE following contributions propose parameterize thinking
tokens interaction trace model text editor Python code edited custom DSL executed set unit tests obtain feedback refer traces Chains Edits CoE order teach small language models use CoEs propose pipeline consists SFT RLVR stages training stages use Low Rank adaptation Hu et al apply pipeline trio SLMs ranging size compare baseline method aims induce reasoning behavior form text based CoTs sizes CoE approach successfully elicits reasoning behavior model performs better CoE shot prompted direct answer text based CoT approach fails induce improvement Related work Training small size reasoners Building small reasoning models fewer eters achieved distillation larger capable teacher
model Mukherjee et al Mitra et al DeepSeek AI coupling policy model verifier model steers generation process Beeching et al Xinyu et al Training small reasoner scratch feasible presents especially RL involved challenges unstable training limited context length Dang Ngo DeepSeek AI explicitly stated training large teacher model RL distilling reasoning small student preferable directly training student work propose test recipe directly training small reasoning model recipe involves combination SFT synthetic teacher generated demonstrations RLVR affordance external tool verifier feedback observe recipe mitigates difficulties observed exclusively free form reasoning approach Dang Ngo integrate verifier feedback context inference decoding procedure remains
entirely auto regressive contrast Beeching et al Xinyu et al Code generation repair aim unlocking ItC scaling coding shared Piterbarg et al propose train LMs sequentially generate code sequence error free diffs single pass work focus code repair editing code generation equip LMs editing environment execution feedback Teaching LMs self refine code execution feedback explored works Gupta et al Yang et al Chen et al Madaan et al Gehring et al Gupta et al design code generation pipeline LM generates initial code draft iteratively revises outputting edit operation token based feedback form execution trace present work LM trained synthetic corruptions
code snippets confined small scale revisions Gehring et al use RL teach capable LMs refine code generations multiple turns benefiting execution feedback limit framework turns model required output fully revised code turn Solving coding problems scale agentic manner focus works Jimenez et al Yang et al Guo et al Yang et al Wei et al occasionally refer framework agentic task focus ab initio agentic nature main focus remains enabling ItC behavior agentic workflow Tool usage Language Models Teaching LMs properly use tools SFT context learning focus works Schick et al Hao et al Erdogan et al Li et al common
theme present work aid LMs avoiding common pitfalls hallucinations having rely exclusively context internal representations works Erdogan et al task simply learn proper use suite tools Taking advantage tools way build ItC structures subject ref Yang et al proposes tool usage traces means build expert CoTs procedure cloning work LM allowed use tools inference recent reference close spirit Feng et al employs combination SFT RL train tool based reasoner benchmark purely text based reasoners Chain Edits Box Example state complete line markers execution feedback L L def n k L sum L L L sum sum L k L sum
sum k L sum L return defined function wrong number wrong type arguments Traceback recent File run line File string line module TypeError missing required positional argument k outline characteristics CoE approach Like CoTs CoEs meant extra tokens model generates solve problem step step contrast CoTs define tokens tracing series interactions LM agent stateful environment agent alternates observing state environment issuing action executed environment locally edit state multiple turns interaction process agent observe produce tokens interpretable free form reasoning way agent generate tokens inference speeding sampling experience RL concentrating exploration track environment state task decomposes series local steps scored dense
reward function believe improvements CoT approach enable reasoning like capabilities weaker smaller models scratchpad DSL environment agent provided consists text editor code execution capabilities refer environment tool Like environment keeps state consists code snippet resides editor execution feedback results execution attempt snippet Like tool meant help agent solve code repair task agent taking actions modify editor state Verification execution feedback define code repair task consisting following elements natural language description coding task set unit tests candidate solution task checked task considered solved unit tests pass piece Python code fails unit tests wrong syntax inconsistent variable names subtle issues satisfying task
precise requirements ground truth code snippet solves task code repair task definition verifiable task text editor verification capabilities instantiated state editor given code content followed execution feedback results running content task unit tests content marked line numbers observed helpful agent separator sequence asterisks separates code content associated feedback Box provide example state text editor examples found Appendix stipulate unit tests pass task solved execution feedback editing DSL LM agent modify content scratchpad issuing editing commands domain specific language DSL design time happens edited content executed unit tests results execution feedback updated scratchpad contents forms updated state designed DSL encourage small
scale atomic edits Python code command consists instruction formatted uppercase followed arguments refer Table outline example command Training pipeline Having defined components CoE outline fine tune language model use CoEs solve code repair tasks multiple turns interaction pipeline consists stages Supervised Fine Tuning SFT dataset synthetic demonstrations CoE usage Reinforcement Learning Verifiable Rewards RLVR suite code repair tasks stages use LoRA Hu et al gradient updates Section outline code repair tasks constructed Supervised Fine Tuning CoE demonstrations step serves teach model fix broken piece code interacting text editor defined Section exposing syntax editor DSL model Table Outline DSL commands model
use editing content scratchpad model restricted single command interaction turn Command Description Example ADDL L python line number indicated argument add new line consisting python ADDL return REPL L python Replace line line number indicated argument line consisting python REPL return REPW python python L Replace python second argument python argument line argument REPW ronge range DELL L Delete line line number indicated argument DELL EXIT Terminate trace episode Table corruption types automated generation Chain Edits demonstrations corruption type matched DSL command reverse corruption Corruption type Reversing DSL command Delete line ADDL add line Add line DELL delete line Replace
line REPL replace line Add typo word REPW replace word Randomly replace character line REPL replace line learn edits relate previous state code verifier output achieve generating dataset synthetic CoE traces demonstrate state action trajectories tokens code repair tasks Generating CoE demonstrations Generating CoE demonstrations trivial option train human programmers DSL usage ask perform task repairing corrupted code snippets hand procedure require expensive time consuming work human experts Instead opt develop automated pipeline generating synthetic demonstrations pipeline consists steps Starting initial correct code snippet given task randomly generate corruption inverted provided DSL commands deleting line apply corruption code snippet Apply
step desired number times c original code snippet sufficiently corrupted c C steps C Create CoE demonstration reversing process starting corrupted code snippet step c C sequentially reverse applied corruptions DSL commands corruptions invertible construction recovers original correct ground truth code snippet step c corruption CoE demonstration trace constructed sequence states actions prefix prompt specifies task repaired code snippet perform provides number unit tests pass prompt explains DSL commands Box example prompt prefix prompt followed state contains fully corrupted code snippet sC equipped line markers corresponding execution feedback Box example rest demonstration consists sequence interleaved DSL commands actions resulting scratchpad
content execution feedback states recover original snippet final action special EXIT action signals end demonstration Box Example CoE demonstration prefix prompt task expert Python programmer goal fix mistakes code snippet interact code snippet applying provided DSL commands Valid DSL command templates DELL delete line specified line number ADDL add line specified line ber specified content REPL replace line specified line number specified content REPW replace specified strings line line number new string task Write python function check count divisors odd code pass tests assert assert Odd assert initial malfunctioning code snippet fix L import math L def L count L
L n L n L count c unt L L count count L count L def L L return Odd L def syntax proposed solution correct stack trace File string line def IndentationError expected indented block statement line employ types corruptions pipeline match DSL command reverse Table corruptions potentially reversible multiple DSL commands sequence commands example corruption adds typo line code inverted replacing word contains typo replacing entire line corrected line inverted deleting offending line separately adding corrected line turns interaction opt match corruption type single local DSL command simplicity generate synthetic dataset tasks ground truth code snippets Basic Python
Problems MBPP dataset Austin et al task MBPP dataset sample total number corruptions applied inclusive sample corruption types uniformly replacement Table obtain diverse trajectories repeat procedure times task train split MBPP times validation split leads dataset demonstrations training M tokens evaluation de duplication evaluation demonstrations validation data use remaining test set fine tune models imitate demonstrations supervised fine tuning LoRA Section details Reinforcement Learning Verifiable Rewards code repair benchmark training model use text editor employ RLVR final code repair task standard policy RL procedure code repair task consists repairing incorrect solutions problems MBPP training set generated sequence simple corruptions incorrect
solutions directly generated LM results harder task initial solutions correct CoE demonstrations outline code repair benchmark constructed Section model prompted use CoE procedure repair initial solutions previous SFT stage prompt describes task usage DSL commands prompt followed initial state contains broken code snippet equipped line markers corresponding execution feedback outlined Section interaction turn model prompted CoE far previous states actions outputs action form editing command action executed scratchpad rewarded engineered reward function store resulting experience buffer train LoRA adapted version Group Relative Policy Optimization GRPO objective Shao et al method adapts reward normalization GRPO use turn statistics trajectory statistics turns
similar employing variance reducing return baseline estimated parallel rollouts RL policy strategy successfully applied RL literature Kool et al Bakker et al refer Appendix details Reward design reward function sum task reward term format reward term ran experiments different task rewards described Appendix end best performing reward function simply rewards model value task solved current turn solved previous turn format reward term applies penalty model outputs action use correct DSL syntax Additionally model penalized output EXIT action code snippet repaired passes unit tests Experiments code repair benchmark target code repair benchmark consists problems based MBPP dataset largely follow procedure outlined
Ni et al MBPP problem shot prompt base model generate solutions fail solve task taking care repeats proposed solution associated original task description ground truth code snippet defines code repair task generate training evaluation split order increase size training split following Ni et al split MBPP dataset pool training split half test split training split evaluation split use tasks evaluation split MBPP end generate code repair tasks train split evaluation split tasks train split training RLVR stage evaluation split problems validation dataset model selection remaining problems test split final evaluation refer Appendix details dataset report metrics average edit distance snippets
code repaired respective ground truth shot repair performance base training models consider repair benchmark synthetic corruptions outlined Section metrics tasks repair benchmark significantly difficult CoE demonstrations dataset making appropriate test pipeline Additionally ability repair incorrectly LLM generated initial code snippets increasingly relevant LLMs increasingly employed coding agents Gehring et al intention directly use MBPP R benchmark contributed reference find Github dataset release obtain authors decided use method generate new benchmark natural language reasoning baseline CoEs intended constrained agentic counterpart CoTs meant enable SLMs solve code repair tasks step step verbosity CoTs compare training pipeline intended enable SLMs employ CoTs tasks
order separately SFT pre trained models use pipeline K dataset Muennighoff et al consists reasoning problems paired high quality CoTs order facilitate transfer problems code repair design prompt template employ fine tuning K evaluation refer Appendix D details K fine tuning prompt template Training evaluation setups Training SFT RLVR LoRA Hu et al AdamW optimizer Loshchilov Hutter family Llama models sizes employ rank adapters linear layers encoding decoding layers Instruct order facilitate training quantize model SFT CoE demonstrations train standard token prediction objective trace model trained predict actions states directly learn DSL logic RLVR instead train model generated tokens actions
GRPO group size group consists trajectories starting initial prompt sampling experience use temperature perform epoch gradient updates experience discarding leading functionally policy updates batch transitions training fits GPU memory employ gradient accumulation artificially increase batch size SFT RLVR experiments performed single GPU model fully trainable GB GPU memory Appendix C details report rates CoE approach K baseline direct answer ItC base models use greedy decoding compute sample temperature compute evaluation models trained output CoEs coupled text editor state appended model context editing command way model required predict state observes accurate execution feedback step model required solve task fix code snippet
EXIT turns Conversely models trained output CoTs coupled text editor end answer correct snippet Python code pre set delimiters order succeed generating tokens Model selection SFT CoE demonstrations step pipeline evaluate points training steps tasks CoE validation dataset model size select checkpoint solves tasks starting point RLVR training RLVR training step pipeline evaluate models evaluation dataset code repair tasks training iterations saving checkpoint performs par better previous checkpoints runs perform evaluation dataset select checkpoints evaluate test set tasks report score best performing checkpoint natural reasoning baseline evaluate test set epochs training report best performing checkpoint note K paper Muennighoff et
al prescribes training epochs Notably epoch checkpoint performs best model epoch checkpoint performs best model difference negligible model Results report main results comparing RLVR pipeline natural language reasoning trained dataset direct answer Table observe large improvements CoE pipeline models answer baseline metrics performance gap especially notable K Table Main results comparing RLVR pipeline natural reasoning trained different model sizes Instruct model quantized Best performing method model metric highlighted bold CoE SFT CoT Direct answer shot Model Instruct smallest model suggests CoE method allows small language models improve code repair capabilities utilizing turn based structure execution feedback environment interaction environment provides
way small models use additional tokens improve code repair performance possible immediate answering provide instructive example code repair trace model Box Appendix brief discussion Appendix Furthermore fine tuning K fails instill improved code repair capabilities models based natural language thinking instead leading reduced performance metrics models Inspecting model output trained models shows regularly stuck repeating common patterns output degenerates printing consecutive line numbers repeating reasoning sentences condition triangle scalene sides equal paragraphs outputting simulated execution feedback Additionally small models fail use thinking solution delimiters correctly larger model observations reverse Performance shot baseline shoots CoE training hamper model repair performance suggests
model utilize information learned pre training effectively turn turn setting direct answer setting trained model performs better manual inspection model output shows coherent reasoning specific failures initial code snippet instance model encounters MBPP task sum factors given integer initial code snippet correctly performs task pass unit tests reasons backwards unit tests determine actual task find prime factors handily solves task K baseline outperform direct answer baseline metric suggests K LoRA fine tuning adds useful diversity generated outputs models size Conclusions limitations paper proposed alternative way parameterizing thinking tokens tool usage trace natural language reasoning change allows extend RLVR paradigm models
size adapted exclusively LoRA Hu et al resulting agents capable inference time tokens form environment interactions improve performance code repair task results obtained attempting elicit text based reasoning form natural language CoTs model tested result reverses suggesting natural language reasoning supervised fine tuning K feasible size Limitations focused code repair task making established reasoning benchmarks considered range small model sizes confined efforts models Llama family Adapting CoE approach established reasoning function calling benchmarks fixing snippets code sourced coding benchmarks richer MBPP CodeContests Li et al worthwhile direction future research Additionally systematically benchmarking CoE pipeline broad set LMs sizes valuable follow
work References Austin Odena Nye Bosma Michalewski Dohan Jiang Cai Terry Le et al Program synthesis large language models arXiv preprint Bakker van Hoof Welling Experimental design MRI greedy policy search Advances Neural Information Processing Systems Dec Beeching Tunstall Rush Scaling test time compute open models URL https spaces blogpost scaling test time compute Chen Lin Schärli Zhou Teaching large language models self debug arXiv cs CL Chu Zhai Yang Tong Xie Schuurmans Le Levine Ma SFT memorizes RL generalizes comparative study foundation model post training arXiv cs AI Dang Ngo Reinforcement learning reasoning small LLMs works arXiv cs LG
DeepSeek AI DeepSeek Incentivizing reasoning capability LLMs reinforcement learning arXiv cs CL Erdogan Lee Jha Kim Tabrizi Moon Hooper Anumanchipalli Keutzer Gholami TinyAgent Function calling edge arXiv cs CL Feng Huang Qu Zhang Qin Zhong Jiang Chi Zhong ReTool Reinforcement learning strategic tool use LLMs arXiv cs CL Fengli Qianyue Zefang Jingwei Yunke Jingyi Xiaochong Jiahui Tianjian Fanjin Chenyang Yuwei Qinglong Yiwen Sijian Xinyuan Yu Jie Chen Yong large reasoning models survey scaling LLM reasoning capabilities arXiv cs AI Gehring Zheng Copet Mella Cohen Synnaeve RLEF Grounding code LLMs execution feedback reinforcement learning arXiv cs CL Guo Li Liu Ma
Zheng Yu Pan Yizhi Liu Wang Guo Qu Yue Zhang Chen Fu CodeEditorBench Evaluating code editing capability large language models arXiv cs SE Gupta Christensen Chen Song Synthesize execute debug Learning repair neural program synthesis arXiv cs LG July Hao Liu Wang Hu ToolkenGPT Augmenting frozen language models massive tools tool embeddings Adv Neural Inf Process Syst Hu Shen Wallis Allen Zhu Li Wang Wang Chen LoRA Low rank adaptation large language models arXiv cs CL June Hu Zhang Han Jiang Zhang Shum Open reasoner zero open source approach scaling reinforcement learning base model arXiv cs LG Ji Li Ye
Wu Xu Mo Zhang Test time computing thinking thinking arXiv cs AI Jimenez Yang Wettig Yao Pei Ofir Press Narasimhan SWE bench language models resolve real world GitHub issues arXiv cs CL Kalajdzievski Rank Stabilization Scaling Factor Fine Tuning LoRA URL https Kool van Hoof Welling Buy REINFORCE Samples Baseline Free URL Kool van Hoof Welling Estimating gradients discrete random variables sampling replacement International Conference Learning Representations URL net Li Xue Zhang Yang Zhang Wang Yu Hui Lin Liu START Self taught reasoner tools arXiv cs CL Li Choi Chung Kushman Schrittwieser Leblond Eccles Keeling Gimeno Dal Lago Hubert Choy
de Masson Babuschkin Chen Huang Welbl Gowal Cherepanov Molloy Mankowitz Sutherland Robson Kohli de Freitas Kavukcuoglu Vinyals Competition level code generation alphacode arXiv preprint Liu Gao Zhao Zhang Li Qi Ouyang Zhou LLM surpass LLM rethinking compute optimal test time scaling arXiv cs CL Liu Chen Li Qi Pang Du Lee Lin Understanding Zero Like Training Critical Perspective URL Llama Team Llama Herd Models URL Loshchilov Hutter Decoupled weight decay regularization International Conference Learning Representations URL Madaan Tandon Gupta Hallinan Gao Wiegreffe Alon Dziri Prabhumoye Yang Gupta Majumder Hermann Welleck Yazdanbakhsh Clark Self refine Iterative refinement self feedback arXiv cs
CL Mitra Del Corro Mahajan Codas Simoes Agarwal Chen Razdaibiedina Jones Aggarwal Palangi Zheng Rosset Khanpour Awadallah Orca Teaching small language models reason arXiv cs AI Muennighoff Yang Shi Li Fei Fei Hajishirzi Zettlemoyer Liang Candès Hashimoto Simple test time scaling arXiv cs CL Mukherjee Mitra Jawahar Agarwal Palangi Awadallah Orca Progressive learning complex explanation traces arXiv cs CL June Ni Allamanis Cohan Deng Shi Sutton Yin Teaching large language models reason code execution arXiv cs LG OpenAI Learning reason LLMs learning reason Accessed Piterbarg Pinto Fergus Training language models synthetic edit sequences improves code synthesis arXiv cs LG Schick
Dwivedi Yu Dessì Raileanu Lomeli Zettlemoyer Cancedda Scialom Language models teach use tools Adv Neural Inf Process Syst Setlur Rajaraman Levine Kumar Scaling test time compute verification RL suboptimal arXiv cs LG Shao Wang Zhu Xu Song Zhang Li Wu Guo DeepSeekMath Pushing limits mathematical reasoning open language models arXiv cs CL Silver Huang Maddison Guez Sifre van den Driessche Schrittwieser Antonoglou Panneershelvam Lanctot Dieleman Grewe Nham Kalchbrenner Sutskever Lillicrap Leach Kavukcuoglu Graepel Hassabis Mastering game deep neural networks tree search Nature Silver Hubert Schrittwieser Antonoglou Lai Guez Lanctot Sifre Kumaran Graepel Lillicrap Simonyan Hassabis Mastering chess shogi self play
general reinforcement learning algorithm arXiv cs AI Snell Lee Xu Kumar Scaling LLM test time compute optimally effective scaling model parameters arXiv cs LG Wang Li Lu Self training direct preference optimization improves chain thought reasoning arXiv cs CL July Wei Wang Schuurmans Bosma Ichter Xia Chi Le Zhou Chain thought prompting elicits reasoning large language models arXiv cs CL Wei Duchenne Copet Carbonneaux Zhang Fried Synnaeve Singh Wang SWE RL Advancing LLM reasoning reinforcement learning open software evolution arXiv cs SE Wolf Debut Sanh Chaumond Delangue Moi Cistac Rault Louf towicz Davison Shleifer von Platen Ma Jernite Plu Xu
Le Scao Gugger Drame Lhoest Rush Transformers State art natural language processing Liu Schlangen editors Proceedings Conference Empirical Methods Natural Language Processing System Demonstrations Online Association Computational Linguistics doi URL Xinyu Zhang Yifei Ning Youran Yi Fan Mao RStar math Small LLMs master math reasoning self evolved deep thinking arXiv cs CL Yang Prabhakar Narasimhan Yao InterCode Standardizing benchmarking interactive coding execution feedback arXiv cs CL June Yang Jimenez Wettig Lieret Yao Narasimhan Ofir Press SWE agent Agent computer interfaces enable automated software engineering arXiv cs SE Yang Schuurmans Abbeel Nachum Chain thought imitation procedure cloning Adv Neural Inf Process
Syst Ye Huang Xiao Chern Xia Liu LIMO reasoning arXiv cs CL Zelikman Wu Mu Goodman STaR Bootstrapping reasoning reasoning arXiv cs LG Impact statement Recent developments large scale machine learning definitive impact society developments promise positive impact form new helpful technologies AI assistants improvements medical care mitigations climate change powerful technologies inherently dual use scientific community care address risks enabling bad actors AI assisted damage harmful societal consequences large scale misinformation job losses power concentration hands institutions persons sole access best AI systems human obsolescence game theoretic forces AI driven economy takover scenario engage potential negative consequences research field
risks mentioned driven capabilities proliferation powerful frontier models believe current work meaningfully increases risks associated AI technologies exception risks associated capabilities proliferation hope insights improving reasoning small language models help democratize access AI capabilities effect offsets potential risk proliferation bad actors B Data Details CoE demonstrations employ specific string delimiters required scratchpad verifier constructing CoE demonstrations states actions separated delimiter term end output suffix eoos code snippet execution feedback state separated Boxes examples delimiters serve increase human readability demonstrations allow controlling model generation later reinforcement learning stages stop strings Box Example state complete line markers execution feedback L L def
n k L x k L y n k L Arr L Arr L Arr Arr L return Arr Test number successful code failed test assert Test number successful code failed test assert Test number successful code failed test assert Limitations Note demonstrations generate guaranteed shortest possible sequence edits corrupted corrected code snippet sequence instance contain subsequence adds deletes line randomly generated sequence corruptions added line subsequently deleted major issue primary purpose synthetic demonstrations prepare models training RLVR final limitation REPW command replaces instance word Table corruption types automated generation Chain Edits demonstrations corruption type matched DSL command reverse corruption
Corruption type Reversing DSL command Delete line ADDL add line Add line DELL delete line Replace line REPL replace line Add typo word REPW replace word Randomly replace character line REPL replace line line different word strictly reverse typo corruption matched example line typo corrupted command REPW L recovers line original line yields syntax error happens sufficiently rarely automated pipeline simply skip sequences happens Corruption details adding replacing line ADDL REPL randomly sample line code different ground truth code snippet current dataset split training validation insert current snippet Typos recovered REPW command generated randomly uniform selecting word defined unit surrounded
whitespace specified line word replaced string selected uniformly random set strings Levenshtein Damerau distance selected word code repair benchmark outlined main text generated dataset sampling candidate solutions MBPP tasks task solutions sampled sampling temperature ensure sufficient diversity filter perfect duplicates accept order avoid biasing dataset difficult coding tasks MBPP task generate fewer code repair tasks Box shows example resulting code repair task formatted way CoE demonstrations Table Comparison edit distance ground truth MBPP solution datasets repair dataset larger average edit distance variation edit distance tasks dataset Dataset Edit distance mean Edit distance stddev CoE demonstrations Repair dataset code snippets usually
fail task unit tests failing obvious errors syntax variable naming report breakdown type failures undergo Unit test failures Syntax errors Errors Wrong number type arguments errors code repair problems difficult solve tasks appear CoE dataset failures usually consist syntax errors incorrect variable names wrong package imports additional evidence code repair problems harder solve CoE demonstrations report quantitative metrics Table report edit distance initial incorrect solutions test split datasets associated ground truth code snippet MBPP dataset solution solutions task larger edit distance Box Example code repair evaluation prompt task expert Python programmer goal fix mistakes code snippet interact code snippet applying
provided DSL commands Valid DSL command templates DELL delete line specified line number ADDL add line specified line ber specified content REPL replace line specified line number specified content REPW replace specified strings line line number new string task Write function print check triangle scalene code pass tests assert assert assert initial malfunctioning code snippet fix L L def b c L b b c c L return True L L return False Test number successful code failed test assert Test number successful code failed test assert Test number successful code failed test assert implies corrupted initial solution implies task
difficult Second Table report shot direct answer results pre trained Llama models test split datasets repair dataset results main Table Shot performance initial code snippets CoE demonstrations dataset higher indicating far easier repair pre trained models Table Comparison shot evaluation performance pre trained models datasets models higher shot performance repairing initial code snippets CoE demonstrations dataset indicating easier solve CoE demonstrations shot Repair dataset shot Model Instruct Finally provide example successful code repair trace generated trained model Box Note final code snippet passes unit tests considered correct technically fully align MBPP task fails replace commas dots colon input string unit
tests fail cover scenario Box Example short code repair trace generated trained model expert Python programmer goal fix mistakes code snippet interact code snippet applying provided DSL commands Valid DSL command templates DELL delete line specified line number ADDL add line specified line ber specified content REPL replace line specified line number specified content REPW replace specified strings line line number new string task Write function replace occurrences spaces commas dots colon given string regex code pass tests assert Curve Wireless Neckband Boult Curve Wireless Neckband assert Sound Sweatproof Stereo Sound Sweatproof assert Curve Audio Probass Curve Audio initial malfunctioning
code snippet fix L L import L def L s r s L return s Test number successful code failed test assert Curve Wireless Neckband Boult Curve Wireless Neckband Test number successful code failed test assert Sound Sweatproof Stereo Sound Sweatproof Test number successful code failed test assert Curve Audio Probass Curve Audio REPL s L L import L def L s L return s EXIT Table Hyperparameters multi turn supervised fine tuning Model quantization Instruct Yes Quantization parameters Library BitsAndBytes Quantization type quantization type compute type storage type Double quantization Yes LoRA parameters Rank Alpha Effective alpha rank stabilization
Dropout Target modules linear layers encoding decoding Adapter weights type Tokenizers Padding Left Truncation Left C Training setup General design choices section discuss design choices apply experiments use HuggingFace Transformers library Wolf et al load Instruct Llama Team quantize model GPU memory constraints training rank stabilized LoRA Kalajdzievski adapters rank non quantized models train separate adapters stage pipeline merging adapters previous training stages staring round training adapters saved separately flexibility save storage space quantized model naively merging adapters known lead performance drops bit weights LoRA adapters reduced bit weights adaptation stage Instead opt merge adapters quantized model pipeline result continuously
train adapter stages pipeline refer Table detailed breakdown model settings Supervised Fine Tuning CoE demonstrations train rank LoRA standard token prediction objective tokens CoE demonstration Training proceeded remarkably stably function hyperparameters experimented learning rates training batch sizes found clear performance differences average refer Table detailed breakdown hyperparameters final production runs Reinforcement Learning Verifiable Rewards section details reinforcement learning setup Table Hyperparameters multi turn supervised fine tuning General Number training prompts total Number evaluation prompts total Number evaluation prompts training Number evaluation prompts testing Maximum context length training tokens Maximum number environment turns training evaluation Batch size Gradient accumulation steps GPU
memory required GB Optimizer AdamW Learning rate Betas Weight decay GRPO objective train variant GRPO objective Shao et al adapted multi turn scenarios GRPO introduced single turn settings model tasked answering question directly number reasoning tokens setting natural notion turn timestep sensible normalize outcome process reward statistics rewards observed trajectories group computing return sum normalized rewards Shao et al Section multi turn setting instead perform group normalization return turn similar return baseline estimated parallel rollouts RL policy successful strategy RL literature Kool et al Bakker et al baseline uses local turn information global trajectory information improve variance reducing properties Kool
et al opt compute return Rt turn t T trajectory sum rewards turn onward discount factor γ normalize returns turn statistics group early development found change led slightly improved stability RL training particular employ following GRPO objective train policy πθ J GRP G X T X min ˆAi t clip ϵ ˆAi t groups size G query q advantage given ˆAi t return observation oi t given query q previous observation Gt X Gt X Gt contains indices trajectories group terminated turn way normalization uses return values observed group current turn Gt contains trajectory terminated omit normalization step turn simply
use ˆAi t Note Equation employs changes suggested GRPO algorithm Liu et al divide GRPO loss response token length iteration queries training set sample GRPO group size trajectories query πθold scratchpad verifier store trajectories buffer log probabilities associated actions current policy πθold reference model πref perform epoch mini batched updates experience batch consists effectively turns experience gradient accumulation fit batch GPU memory batch perform single averaged update AdamW optimizer Loshchilov Hutter maximize Equation Task reward function describe task reward functions experimented detail OnlyWhenSolved reward task solved current turn solved previous turn UnitTestFraction reward change fraction unit tests solved current turn
turn fraction reward UnitTestFraction EditDistanceBonus reward UnitTestFraction plus bonus penalty current code snippet looks like ground truth snippet previous turn measured change fractional edit distance editdistance Python package reward fluctuate strongly short code snippets multiply term small constant clip reward range best performing RL runs OnlyWhenSolved reward function nation UnitTestFraction EditDistanceBonus ranked second overall UnitTestFraction performed worst Hyperparameters development experimented task reward functions format reward penalties KL divergence weight values GRPO group sizes learning rates experience sampling temperatures experience batch size training batch sizes refer Table detailed breakdown final hyperparameters production RL training runs D Finetuning K Prompt template finetune
model dataset map dataset prompt template box question thinking trajectories attempt respective fields evaluation code repair use template facilitate transfer tasks K code repair benchmark repair task shown box example report box model fine tuned K prompted solve Training evaluation setup fine tune models K standard token prediction objective training sequences formatted demonstrated box length sequences longest longer tokens fix batch size use steps gradient accumulation simulate batch size fine tune epochs following Muennighoff et al report table hyperparameters evaluating finetuned models prompt shown box simply grab verify run agains unit tests model generated BEGIN SOLUTION END SOLUTION delimiters K
Table Hyperparameters RL fine tuning General Number training prompts total Number evaluation prompts total Number test prompts total Number training prompts seen training Maximum context length training tokens Maximum number environment turns training evaluation Maximum number generated tokens turn training evaluation GPU memory training model GB GPU memory training models GB Optimizer AdamW Learning rate Betas Weight decay GRPO Trajectories sampled iteration Sampling batch size Group size Training batch size transitions model Training batch size transitions models Gradient accumulation steps model Gradient accumulation steps models Discount factor γ Clip value ϵ KL divergence weight β Updates experience sample Experience sampling
temperature Temperature computing GRPO log probabilities Group normalization returns turn Subtract group mean return Yes Divide returns group standard deviation opposed Liu et al Yes Average sum log probabilities turn following Liu et al Sum Reward Task reward OnlyWhenSolved Format reward penalty Table Hyperparameters supervised fine tuning General Number training prompts total Number evaluation prompts total Batch size Gradient accumulation steps GPU memory required GB Optimizer AdamW Learning rate Betas Weight decay Box prompt template use SFT K dataset expert conscientious reasoner goal provide detailed answers questions provided question reason step step format reasoning BEGIN THINKING reasoning natural language END
THINKING thinking output answer question formatted BEGIN SOLUTION answer natural language END SOLUTION question answered question BEGIN THINKING thinking trajectories END THINKING BEGIN SOLUTION attempt END SOLUTION E shot evaluation base models code repair shot prompt evaluation base fine tuned models code repair long fit single page report box shot version code repair task box example evaluate base models propt shot prompt simply grab verify run agains unit tests model generated BEGIN SOLUTION END SOLUTION delimiters Box Example code repair evaluation prompt task expert Python programmer goal fix mistakes code snippet follow outline code snippet supposed given natural language description
followed list unit tests supposed pass given broken code snippet Python stack trace generates unit tests run task Write function print check triangle scalene code pass tests assert assert assert provided malfunctioning snippet code python followed resulting python stack trace Reason step step unit tests task stack trace Format reasoning BEGIN THINKING reasoning natural language END THINKING thinking output repaired code snippet formatted BEGIN SOLUTION executable python code END SOLUTION code snippet fixed L L def b c L b b c c L return True L L return False Test number successful code failed test assert Test number successful
code failed test assert Test number successful code failed test assert BEGIN THINKING Box Example shot code repair prompt task expert Python programmer task Write function find minimum cost path reach m n given cost matrix cost position m n cost code pass tests assert assert assert initial malfunctioning code snippet fix L def m n L m n L return L m L return L n L return L return m n m n Test number successful code failed test assert Test number successful code failed test assert Test number successful code failed test assert final correct Python function
BEGIN SOLUTION def m n tc x x j j return END SOLUTION expert Python programmer task Write function print check triangle scalene code pass tests assert assert assert initial malfunctioning code snippet fix L L def b c L b b c c L return True L L return False Test number successful code failed test assert Test number successful code failed test assert Test number successful code failed test assert
Jul Ultra sensitive sizing individual nanoparticles optoﬂuidic microcavity Shalom Larissa Christian Claus David Institut Technologie Physikalisches Institut Wolfgang Gaede Str Karlsruhe Germany Institut Technologie Institut Anorganische Chemie Engesserstr Karlsruhe Germany Institut Technologie Institut QuantenMaterialien und Technologien Hermann von Helmholtz Platz Eggenstein Leopoldshafen Germany Nanoparticles ubiquitous methods reveal insights single particle properties highly desired enable advanced characterization Techniques achieve label free single nanoparticle detection lack bandwidth provide quantitative information present cavity based dispersive sensing method achieves high bandwidth capture relevant timescales translational diﬀusion sensitivity detect size single particles diameters nm develop analytical model describing autocorrelation function particle diﬀusion standing wave sensing
geometry propose method address challenges posed transient nature single particle signals achieve quantitative particle sizing high precision accuracy provide important tool analyze single particle diﬀusion Nanomaterials heterogeneous methods direct access single particle properties open view details distribution reveal features washed ensembles enable studies dynamical behaviour range label free single particle sensing techniques developed example interferometric scattering microscopy iSCAT enabled detection single nanoparticles biomolecules sizes kDa machine learning algorithms tracking sizing single nanoparticles nm diameter approach use nanoplasmonic hot spots tip gold nanorods whispering gallery mode WGM resonators high quality factors microspheres enable real time detection single particles quantitative particle sizing
high sensitivity achievable combining WGM cavities nanoplasmonic hot spots sensors based interaction analyte sensor surface constrain modify diﬀusion preclude precise quantiﬁcation example hydrodynamic radius Open access microcavities avoid limitation larger mode volume compared nanoplasmonic sensors far restricted detection sizing particles nm recently open cavities achieve sensitivities single molecules kDa highly non linear photothermal regime remarkable sensitivity allowed single protein detection extracting information relating size shape approach remained limited statistics obtained detection events addition inﬂuence photothermal response convolves signal makes quantitative unbiased evaluation challenging operate ﬁber Fabry Perot microcavity locked slope resonance linear dispersive regime high ﬁnesse high stability cavity length
ﬂuctuations fm achieve sensitivity capable detecting individual nm gold nanoparticles diﬀuse cavity mode detection bandwidth principle limited decay rate cavity MHz obtain suﬃcient statistics single particle transits cavity mode calculate autocorrelation function ACF analysis analysis established ensemble techniques ﬂuorescence correlation spectroscopy FCS dynamic light scattering DLS scenario studied raises central aspects require careful attention probing volume FCS approximated Gaussian treated analytical model ACF cavity standing wave complex structure requires reformulation model Second extraction diﬀusion time constants hydrodynamic radii ACF relies assumption stationary system satisﬁed ensemble constant average concentration contrast single particle events intrinsically non stationary address aspects propose analytical model
ACF particle diﬀusion standing wave probe volume introduce analysis minimizes undesired inﬂuence transient single particle behavior methodology perform quantitative particle sizing range nominal particle diameters obtain size distributions good agreement reference measurements performed DLS transmission electron microscopy TEM opens way quantitatively assess nanomaterials unlabeled biomolecules native environment allow dynamic AFG PID Cavity Oscilloscope PZT PZT Laser Set point Cavity length Cavity transmission Δν Δy c b FIG Illustration gold nanospheres diﬀusing optoﬂuidic ﬁber Fabry Perot microcavity b optoelectronic setup active cavity stabilization cavity ﬁbers inserted glass ferrule lateral microﬂuidic channel Piezoelectric transducers PZT driven arbitrary function generator AFG proportional integral
derivative PID feedback controller allow tune cavity mirror separation actively stabilize cavity slope resonance respectively detailed description Methods c Detection dispersive resonance shifts induced nanoparticles changes transmission locked cavity processes rotation conformational changes investigated future RESULTS locked optoﬂuidic microcavity Fabry Perot microcavities high ﬁnesse strongly enhance interaction light matter cavity mode volume powerful tools label free detection single nanoparticles aqueous dispersion Similarly previous work use ﬁber based optoﬂuidic microcavity comprising single mode optical ﬁbers inserted ﬁber optic ferrule microﬂuidic channel drilled ferrule perpendicular ﬁbers cavity ﬁber tips immersed water Fig b optical ﬁbers processed laser machining produce concave proﬁles radii
curvature µm µm coated highly reﬂective Bragg mirror having transmission T ppm air implement actively stabilized cavity locked resonance fringe lock FPGA based feedback circuit unity gain bandwidth Hz frequency range covered nanoparticle diﬀusion comparison previously reported length modulated cavities locked cavity oﬀers possibility measurements higher bandwidth limited essentially cavity linewidth cavity lengths experiments µm cavity ﬁnesse water active stabilization microcavity shows root mean square optical length instability fm corresponds linewidth hours cavity locked transmission trace illustrated Fig note passively stable cavity shows high stability short timescales seconds providing low noise background measurements Detection single nanoparticles small particle polarizability α
diﬀusing optical ﬁeld produces relative resonance frequency shift cavity ν value normalized optical ﬁeld distribution position nanoparticle Vm cavity mode volume εm relative electric permittivity surrounding medium spherical particles Rayleigh regime radius R relative permittivity εp polarizability given α Small dispersive shifts produced nanoparticles mode volume translated measurable changes cavity transmission Fig c use microcavity detect single citrate stabilized spherical gold nanoparticles GNP aqueous dispersion speciﬁed diameters ranging nm nm stock sample diluted concentration pM average particle cavity mode time resulting suspension injected microcavity cavity transmission monitored avalanche photodiode bit digital storage oscilloscope Examples single nanoparticle events shown Fig panels
c d calculate ACF measured signal observe characteristic decays correlation separated time scales Fig b diﬀers autocorrelation typically observed methods translational diﬀusion present analyze model behavior detail enable ﬁt data extract physical quantities interest hydrodynamic radius ﬁrst step use autocorrelation enable detection nanoparticles signal noise ratio measured cavity transmission time trace allow direct identiﬁcation nanoparticle events purpose compute autocorrelation short segments data duration b c d e FIG Example transmission locked cavity disturbed diﬀusion nm nanosphere optical mode b autocorrelation nanoparticle signal ﬁt analytical ACF presented work Time traces panel correlation amplitudes middle panel autocorrelograms panel nm nm nanosphere events
respectively e nanoparticle mapped dimensional space nanoparticle diameter autocorrelation amplitude contrast Tseg ms evaluate correlation amplitude δτ shortest temporal delay autocorrelation segment middle panels Fig c d examples nm nm particles respectively lower panel shows merging ACFs segments time resolved autocorrelogram particle present shows value signiﬁcantly larger particle cavity background signal largely uncorrelated use autocorrelation amplitude contrast peak correlation amplitude segment background correlation amplitude measure particle presence statistical distribution GNP events diﬀerent particle sizes shown Fig e roughly exponential correlation particle size observed shows allows diﬀerentiate particles diﬀerent size qualitatively comparably large uncertainty note background value times particle present changes
slightly measurements changing noise levels Supplementary Information background remains factor smaller nm GNPs use signal correlation contrast powerful way detect small particles signal remain masked noise enables highly sensitive particle detection provides qualitative size information require modeling ACF provide quantitative characterization particles proceed develop proper model enable particle sizing autocorrelation function diﬀusion standing wave ﬁeld Autocorrelation based analysis diﬀusional motion widely technique determine size particles suspensions analyzing diﬀusional motion instance FCS accurate determination time constants diﬀusion imperative ﬁrst determine theoretical ACF optical mode geometry involved typical FCS setup Gaussian focal spot approximated normalized intensity proﬁle y z exp exp exp
focus diameter depth focus normalized ACF particles diﬀusivity DT τ τ FCS ﬂuorescence particles diﬀusing focal spot excitation laser measured proportional time dependent concentration ﬂuorophores focal volume average concentration particles k ﬂuctuation signal extracted measured ﬂuorescence steady state average ﬂuorescence signal Fitting autocorrelation Eqn allows determination translational diﬀusion coeﬃcient DT directly yields information average hydrodynamic radius particles Stokes Einstein relation DT η viscosity surrounding ﬂuid T temperature Fabry Perot cavity normalized intensity distribution given slowly varying beam waist y z exp k λ autocorrelation integral analytical solution Instead numerical ACF standing wave ﬁeld calculated analytical function essential ﬁt based analysis
autocorrelation particle diﬀusion propose approximation analytical ACF replacing standing wave antinodes series Gaussians separated standing wave period q X q scaling factor s introduced match levels functions Taking account ﬁnite number antinodes q allows ACF expressed closed form analytical function x X bme x x bm analytically determined constants Fig c compare ACFs arising diﬀerent numbers Gaussian antinodes note q ACF corresponds single dimensional Gaussian focus identical FCS Eqn q second decay appears arises diﬀusion multiple antinodes periodicity ﬁeld seen nanoparticle Additionally ACF converges q k justifying simpliﬁcation obtain Eqn Furthermore transverse dimension cavity mode leads decay time constant shown
grey dotted line Fig overlaps multiple antinode decay cavity geometry resolved b c FIG Calculated normalized intensity cavity mode ﬁeld b standing wave modeled series Gaussian functions separation dashed line shows deviation standing wave weighted normalized intensity c analytical ACF diﬀusion optical ﬁeld varying numbers Gaussians case single Gaussian corresponds ACF FCS Autocorrelation single particle events turn aspect treating transient signal possible contributions lock requires additional considerations signal processing ﬁrst step oﬀset lock set point inﬂuence small external disturbances locking mechanism long particle events removed baseline correcting particle signal Supplementary Information details Second single nanoparticle event occupies typical measurement wish
select segment calculation ACF larger particles region interest identiﬁed monitoring amplitude ﬂuctuations d e f g h FIG Autocorrelations single particle events nanoparticles having nominal diameters nm nm nm respectively averaged curves shown darker theoretical ACFs mean measured sizes shown dotted Particle sizes measured nanoparticles having nominal diameters diameters nm nm nm nm nm respectively comparison size distributions obtained commercial DLS device shown dashed lines Comparison nanoparticle sizing methods microcavity DLS TEM nm nm Note clarity data points diﬀerent measurement methods slightly horizontally oﬀset background noise level smaller particles R nm reliable small signal noise ratio produce calculate introduced use
value measure particle presence trace segment similar Segments low correlation amplitude excluded analysis seen Fig c d segments exhibiting higher correlation background eﬀectively identiﬁed extracted processing method automatic nanoparticle recognition reduces bias larger particles agglomerates compared amplitude based recognition allows extraction particle signals ﬂuctuation amplitudes comparable noise background having identiﬁed nanoparticle event measured signal transient nature single particle event taken account analysis ﬂuctuation signal described Sec C valid stationary system ﬂuctuations particle concentration small compared average concentration observation volume regime single particle measurement average concentration particles observation volume lies depending individual particle trajectory result arbitrarily subtract mean signal level obtain
ﬂuctuation signal Instead note Fig c ACF derived standing wave shows initial decay corresponding diﬀusion single antinode optical ﬁeld second decay corresponding motion antinodes separated point minimum slope ﬁnd height point minimum slope depends cavity geometry sample use point reference level amplitude ﬁrst diﬀusion decay normalized subtracting oﬀset measured signal time trace eﬀectively corresponds Sec Fig measured ACFs single particle events particles nominal diameters nm nm nm respectively ﬁt obtained experimental ACFs analytical model introduced extent ﬁrst diﬀusion decay Fig b second decay useful limited statistics represents use DT ﬁt parameter directly related hydrodynamic radius R Stokes Einstein relation extract
size single particle event Exemplary ﬁts shown average ACFs ﬁt single particle signal shown Fig b manner measured diameters single nanoparticles speciﬁed sizes nm plot obtained sizes histograms Fig verify accuracy method compare results measurements commercial DLS system TEM images excellent agreement shown Fig d e deviations measured sizes nominal values points systematic uncertainties nominal values given manufacturer observed smallest particle sizes cavity measurement reproduces right size trend close nominal diameter DLS shows opposing trend pointing limitation method note intrinsic ambiguity DLS results scattered intensity particle radius R ensemble measurement proportional result directly measured intensity normalized distributions highly vulnerable subpopulations
larger particles agglomerates Instead considered number normalized distributions oﬀering meaningful comparison work calculated assumptions nanoparticle properties dependent priori knowledge nanoparticles susceptible errors provided data Finally contrast DLS measurements microscopic sample volumes microliters ultra low concentration required microcavity experiments II CONCLUSIONS work demonstrated technique measuring analyzing single unlabeled freely diﬀusing nanoparticles diameter nm corresponds gold atoms eﬀective molecular weight kDa note operating far away plasmon resonance wavelength gold nanospheres nm neglect plasmon enhancement GNPs behave essentially like dielectric particles smallest GNPs equivalent silica sphere diameter nm approaches single molecule sensitivity recently reported photothermally enhanced optical microcavity additionally oﬀering capability precise size
determination modiﬁed autocorrelation model described work possible quantitatively extract information single particles autocorrelation event traces need priori calibration measurements Furthermore high measurement bandwidth possible method provides temporal resolution orders magnitude better imaging based nanoscopy techniques enable resolution motion smaller faster particles fast dynamics rotation internal motion conformational dynamics future III METHODS Experimental setup cavity ﬁbers machined house single mode ﬁbers according procedure developed Hunger et al proﬁles radii curvature µm µm mirror coating Laseroptik cavity probed grating stabilized diode laser nm Transmitted light detected avalanche photodetector monitored oscilloscope error signal cavity locking cavity locking implement slow stabilization scheme designed rapid
resonance shifts interaction diﬀusing nanoparticle lie outside locking bandwidth pass cavity unattenuated detected transmission signal Slower acoustic mechanical jitter thermal drifts hand corrected locking system detailed veriﬁcation shown Sec Cavity stabilization carried piezo electric transducer PZT called PZT attached cavity ﬁber driven proportional integral derivative PID feedback signal Fig travel range locking PZT limited PID controller output nm PZT PZT attached cavity ﬁber receives oﬀset arbitrary function generator AFG larger output range bring cavity resonance range lock PZT Reference measurements Reference nanoparticle measurements DLS performed Malvern Instruments Zetasizer Nano ZS equipped nm laser disposable cuvettes TEM images nm nm nanoparticles
acquired FEI Osiris electron microscope kV nanoparticles deposited coated copper grid Size analysis performed Nanoparticle samples gold nanospheres purchased Nanopartz dispersed buﬀer citrate distilled water speciﬁed polydispersity diameter nm nm nm nm nm stock suspension diluted isotonic citrate buﬀer distilled ﬁltered water recommended manufacturer concentration pM particle resides mode volume average time Data processing Data measured oscilloscope smoothed Savinsky Golay ﬁlter resampled kHz nm nm nanospheres MHz nm nanospheres Frequency ﬁltering performed remove sharp peaks arising noise sources data processed described Sec D custom written Python script autocorrelation computed multiple tau algorithm details data processing steps Supplementary Information Acknowledgments acknowledge
Lucas Flatten helpful discussions Jens Treptow performing TEM measurements DH SP acknowledge funding Max Planck School Photonics MPSP Karlsruhe School Optics Photonics KSOP DH SP CR CF acknowledge funding Deutsche Forschungsgemeinschaft DFG German Research Foundation Collaborative Research Centre Future CRC project number project Competing interests authors declare competing interests Author contributions conceived idea prepared experimental set performed microcavity experiments developed performed analysis performed DLS TEM measurements wrote manuscript provided feedback Additional information Supplementary information available paper Correspondence requests materials addressed David Hunger Shalom Palkhivala Reprints permissions information available Richard Taylor Vahid Sandoghdar Interferometric Scattering Microscopy Seeing Single Nanoparticles Molecules Rayleigh
Scattering Nano Letters August Publisher American Chemical Society Naomi Ginsberg Chia Lung Hsieh Philipp Kukura Marek Piliarik Vahid Sandoghdar Interferometric scattering microscopy Nature Reviews Methods Primers April Publisher Nature Publishing Group Marek Piliarik Vahid Sandoghdar Direct optical sensing single unlabelled proteins super resolution imaging binding sites Nature Communications July Publisher Nature Publishing Group Mahyar Dahmardeh Houman Mirzaalian Dasterji Hisham Mazal Harald Vahid Sandoghdar Self supervised machine learning pushes sensitivity limit label free detection single proteins kda Nature Methods Kiarash Kasaian Mahdi Mazaheri Vahid Sandoghdar Long range dimensional tracking nanoparticles interferometric scattering microscopy ACS Nano Anna Kashkanove Martin Blessing Gemeinhardt Didier
Soulat Vahid Sandoghdar Precision size refractive index analysis weakly scattering nanoparticles polydispersions Nature Methods Martin Dieter Baaske Peter Sebastian Neu Michel Orrit Label free plasmonic detection untethered nanometer sized brownian particles ACS Nano Nasrin Asgari Martin Dieter Baaske Michel Orrit Burst Burst Measurement Rotational Diﬀusion Nanosecond Resolution Reveals Hot Brownian Motion Single Chain Binding ACS Nano July Publisher American Chemical Society Vollmer Yang Label free detection high Q microcavities review biosensing mechanisms integrated devices Nanophotonics Matthew Foreman Jon Swaim Frank Vollmer Whispering gallery mode sensors Adv Opt Photon Jun Jiangang Zhu Sahin Kaya Ozdemir Yun Feng Xiao Lin Li Lina
Lan Chen Da Renand Yang chip single nanoparticle detection sizing mode splitting ultrahigh q microresonator Nature Photon Martin Baaske Matthew Foreman Frank Vollmer Single molecule nucleic acid interactions monitored label free microcavity biosensor platform Nature Nanotechnology November Publisher Nature Publishing Group Trichet Dolan James Hughes Vallance Smith Nanoparticle trapping characterization open microcavities Nano Letters Kiana Malmir William Okell Trichet Jason Smith Characterization nanoparticle size distributions microﬂuidic device integrated optical microcavities Lab Chip Larissa Kohler Matthias Mader Christian Kern Martin Wegener David Hunger Tracking brownian motion dimensions characterization individual nanoparticles ﬁber based microcavity Nature Communications Kiana Malmir William Okell Trichet Jason
Smith Characterization nanoparticle size distributions microﬂuidic device integrated optical microcavities Lab Chip September Publisher Royal Society Chemistry Lisa Maria Needham Carlos Saavedra Julia K Rasch Daniel Sole Barber Beau S Schweitzer Alex J Fairhall Cecilia H Vollbrecht Sushu Wan Yulia Podorova Anders J Bergsten Brandon Mehlenbacher Zhao Zhang Lukas Tenbrake Jovanna Saimi Lucy C Kneely Jackson S Kirkwood Hannes Pfeifer Edwin R Chapman Randall H Goldsmith Label free detection proﬁling individual solution phase molecules Nature Thorsten Wohland Radek Machan Sudipta Maiti Introduction Fluorescence Correlation Spectroscopy IOP Publishing David Hunger Tilo Steinmetz Yves Colombe Christian Deutsch Theodor Jakob Reichel ﬁber fabry
perot cavity high ﬁnesse New Journal Physics Jonas Ries Petra Schwille Fluorescence correlation spectroscopy BioEssays Elliot Elson Douglas Magde Fluorescence correlation spectroscopy conceptual basis theory Biopolymers Sergio Dominguez Medina Sishan Chen Jan Blankenburg Pattanawit Swanglap Christy Landes Stephan Link Measuring Hydrodynamic Size Nanoparticles Fluctuation Correlation Spectroscopy Annual Review Physical Chemistry Publisher Annual Reviews Nasrin Asgari Martin Dieter Baaske Jacco Ton Michel Orrit Exploring rotational diﬀusion plasmonic coupling ACS Photonics Paul Python multiple tau algorithm https pypi Supplementary Information Ultra sensitive sizing individual nanoparticles optoﬂuidic microcavity Shalom Larissa Christian Claus David Institut für Technologie Physikalisches Institut Wolfgang Gaede Str Karlsruhe Germany
Institut für Technologie Institut für Anorganische Chemie Engesserstr Karlsruhe Germany Institut für Technologie Institut für QuantenMaterialien und Technologien Hermann von Helmholtz Platz Eggenstein Leopoldshafen Germany SIMULATED CAVITY DISTURBANCE verify nanoparticle signal passes attenuation ﬁltering investigated response lock simulated particle like disturbance cavity signal diﬀusion nanoparticle optical standing wave simulated Python script played PZT AFG leading disturbance physical cavity length simulates dispersive disturbance diﬀusing nanoparticle cavity transmission signal lock signal monitored diﬀerent lock parameters determined conﬁguration nanoparticle disturbance fully measured cavity transmission signal inﬂuence unity gain frequency UGF lock bandwidth cavity transmission lock signal shown Figure y mV b c d
t s mV e t s f t s g t s h Figure Cavity transmission signal virtual nanoparticle modulation unity gain frequencies UGFs Hz Hz kHz kHz respectively lock feedback signal corresponding UGFs Frequency Hz PSD Hz Hz Hz Hz Hz Frequency Hz b b b b Figure Power spectral density PSD cavity transmission virtual nanoparticle modulation varying UGFs PSD nanoparticle signal shown black dotted line b PSD lock feedback signal UGFs shown Fig power spectral density PSD transmission signal lock signal compared diﬀerent lock parameters showing bandwidth nanoparticle signal shown dotted black spectrum passes detected cavity transmission UGF
Hz experiments close agreement spectra disturbance transmission conﬁrm cavity response nanoparticle aﬀected photothermal eﬀects optical power levels higher locking bandwidths lock corrects nanoparticle disturbance seen low frequency attenuation PSD cavity transmission high pass ﬁltering signal lock bandwidth reduced stability cavity decreases making unsuitable nanoparticle detection root mean square stability cavity length fm corresponding frequency jitter MHz underwater cavity experiments II SIGNAL PROCESSING section outline signal processing steps undertaken prior autocorrelation cavity transmission signal processed follows lower envelope mean signal subtracted yield signal centred zero autocorrelation amplitude computed ms long segments signal segments featuring nanoparticle event identiﬁed autocorrelation nanoparticle signal level
point minimum slope characteristic decays computed oﬀset numerically computed minimum slope level autocorrelation matches theoretical ACF calculated cavity geometry main text raw data preprocessed particle size determined ﬁtting analytical ACF experimental autocorrelation Examples measured nanoparticle events shown Figure e autocorrelation note smallest particles measured work correlation approaches background correlation level introducing lower limit size particles currently detected setup III TABLE RESULTS Table III summarize results microcavity measurements comparison measurements performed dynamic light scattering DLS transmission electron microscopy TEM DLS results shown number normalised Sample Speciﬁed Measured diameter nm diameter nm microcavity DLS TEM Table comparison nanosphere diameters measured microcavity commercial
DLS system DLS results number normalised microcavity results represent statistics N single particle measurements sample numbers respectively b c d e f Figure e panel Measured signals separated nanoparticle dark blue background grey segments based autocorrelation amplitude τ δτ ms segments orange panel Autocorrelation nanoparticle event dark blue background section grey ﬁt red ﬁgures correspond particles nominal sizes nm nm nm nm nm f Distribution maximum autocorrelation amplitudes nanoparticle event solid histograms background unﬁlled histograms nm nm nm nm nm GNPs experiments nm nm showed lower background noise
cs RO Jul VerifyLLM LLM Based Pre Execution Task Plan Verification Robots Danil Alexey Aleksandr Abstract field robotics researchers face critical challenge ensuring reliable efficient task planning fying high level task plans execution significantly reduces errors enhance overall performance systems paper propose architecture automatically verifying high level task plans execution lator real world environments Leveraging Large Language Models LLMs approach consists key steps conversion natural language instructions Linear Temporal Logic LTL followed comprehensive analysis action sequences module uses reasoning capabilities LLM evaluate logical coherence identify potential gaps plan Rigorous testing datasets varying complexity demonstrates broad applicability module household tasks contribute
improving reliability efficiency task planning addresses critical need robust pre execution verification autonomous systems code available INTRODUCTION Verifying robot action plans execution remains challenging task robotics Modern planning systems generate action sequences appear correct glance contain hidden errors evident execution example robot attempt pour water upside glass closed container errors stem robot physical limitations action plans fail incorporate common sense fundamental physical constraints humans naturally account Traditional planning systems based PDDL struggle tasks requiring common sense reasoning looking essential preconditions checking container filling fail sider action checking fridge closed interaction Recent approaches evolved classical planning methods learning based techniques better
handle uncertainty complex environments Large language model works shown promising results robotic task ning studies focus primarily generating plans verifying provide verification mechanisms limited scope reliance simple templates Researchers use Linear Temporal Logic LTL powerful formalism specify robotic task requirements ability express temporal relationships logical constraints LTL successfully applied verify system properties Dolgoprudny Russia State University atigorsk Stavropol Krai Russia Moscow Russia Fig VerifyLLM workflow verification task plans system takes generated action plan input analyzes context window window processed LLM identify position errors missing prerequisites redundant actions Based analysis system proposes improvements including reordering adding necessary steps removing duplicates output
refined action plan maintains logical consistency completeness example demonstrates verification tea preparation plan system identifies corrects issues action ordering missing prerequisites robotics methods face challenges natural language understanding address challenge introduce VerifyLLM novel framework integrates LTL LLMs illustrated Fig system translates action plans LTL formulas offering formal representation captures temporal pendencies logical constraints LLM analyzes corresponding action sequences combination leverages contextual understanding common sense reasoning capabilities LLMs detect potential errors execution rigorously assess framework introduce specialized datasets annotated LTL specifications ALFRED LTL VirtualHome LTL ALFRED LTL derived ALFRED dataset consists hold task instructions executed simulated environment VirtualHome LTL adapted VirtualHome
dataset containing human like activities modeled virtual environment main contributions include development VerifyLLM novel verification framework combines formal LTL representations LLM based reasoning systematically identify inconsistencies robot action plans sive evaluation demonstrates significant improvements plan reliability diverse household scenarios creation specialized datasets LTL annotations ALFRED LTL VirtualHome LTL II RELATED WORKS Plan Verification Traditional plan verification approaches typically relied model checking theorem proving formal methods applied PDDL representations classical techniques verify plans domain cations systematically exploring state spaces ensure preconditions effects consistent struggle scaling complex domains lack ability incorporate common sense knowledge explicitly encoded domain definitions Recent research increasingly explored
use LLMs robotic task planning works focus plan generation relatively address critical challenge plan ification Current approaches primarily use LLMs generate task plans natural language instructions translate instructions formal specifications Works like attempt verify plans detecting recovering execution failures showing improved success rates scale tasks methods rely simple plates describing failures object X blocking struggle complex scenarios involving temporal dependencies safety constraints Approaches focus feasibility checking generated plans limited specific domains like motion planning researchers introduced LLatrieval system LLMs verify iteratively improve retrieval results focused text generation verification approach parallels plan verification incremental checks identify correct errors researchers proposed collaborative
verification combining Chain Thought Program Thought methods demonstrating different solution representations improve verification reliability specialized approaches emerged focuses error recovery continuous plan toring addresses common sense knowledge integration robotics methods address specific verification challenges tend operate limited contexts fully integrate temporal reasoning safety preconditions physical world constraints simultaneously approach aims bridge gap contextual analysis plan segments gap plan generation verification remains significant robotics VerifyLLM framework addresses systematically identifying critical types plan inconsistencies position errors missing prerequisites redundant actions Unlike approaches focus specifically robotic task execution safety actionable plan corrections maintain commonsense sistency Linear Temporal Logic Linear Temporal Logic LTL emerged
erful formalism specifying robotic task requirements ability express temporal relationships logical constraints LTL suited representing structure dependencies complex task plans ally researchers apply LTL model checking theorem proving verify system properties methods face challenges addressing nuances natural language understanding real world complexity seminal work laid foundation LTL system verification approach later adapted robotics Subsequent advancements introduced robust temporal logics enable realistic modeling continuous systems Recent research begun explore integration formal methods based approaches instance proposed frameworks verifiable reinforcement learning incorporate safety constraints introduced methods sense verification LLMs hybrid strategies aim leverage strengths formal representations natural language processing albeit specific domains
simplified scenarios mapping natural language lifted LTL representations investigated improve generalization domains work highlights potential LTL intermediate representation captures key temporal logical relationships facilitating transferability deeper understanding complex tasks Similarly grounding language non Markovian tasks LTL explored focus learning strict formal verification Overall research landscape indicates need proaches combine expressive power LTL senting temporal logical dependencies language understanding capabilities LLMs traditional formal methods emphasize strict verification VerifyLLM approach employs LTL intermediate representation enhance interpretability refinement generated task plans III PRELIMINARIES use Linear Temporal Logic formal guage specifying robotic task requirements LTL formulas follow grammar ϕ p p represents atomic propositions
describing robot environment possible states ϕ denotes task specification LTL formulas basic logical operators include negation conjunction tion temporal operators serve distinct purposes globally operator G specifies property hold future time points operator U indicates hold continuously true eventually operator F requires ϕ true future time point IV FORMAL TASK DEFINITION classical planning problem typically defined tuple P P S T G O denotes set objects environment P represents properties objects characteristics affordances set available actions S set possible states T S state transition function initial state G set goal states τ task description provided natural language Eq Eq work
focuses quality generated plans introduce concept plan π ordered sequence actions π ai denote Π set admissible plans given plan π set defined ai n ensure correctness transitions Eq actions respect task τ extend formulation include verification procedure extended planning problem defined P P S T G τ Π V verification procedure V designed evaluate refine plan based problem description τ Formally verification function defined V Π τ takes input initial plan πin task description τ returns refined plan πout validity transitions checked common sense LLM reasoning combined formal specification ϕ Eq Eq METHOD Plan verification robots remains underexplored lenge
existing research focuses plan generation current verification methods significant limitations mal approaches struggle natural language understanding learning based methods require extensive training data address gaps propose VerifyLLM framework combining LLM LTL comprehensive pre execution verification robot task plans VerifyLLM Architecture framework illustrated Figure generate action plans verifies pre generated sequences plans produced external planner approach focuses ensuring logical correctness tion VerifyLLM enables systematic analysis task plans addressing logical consistency contextual aspects execution VerifyLLM architecture sented Algorithm comprises main components Algorithm VerifyLLM Plan Verification Input Task description τ action sequence π Output Verificated sequence begin E examples return E end begin
ϕ ctx w Extract window props propositions switch props case remove Remove redundant action end case Fix position error end case augment Add missing prerequisite end end end return end ϕ Convert task LTL ϕ Initial verification ϕ end return Translation Module converts action plans Linear Temporal Logic LTL formulas Verification ule analyzes plans LLM based reasoning enhanced LTL formalism process begins action plan π natural language task description τ shown lines Algorithm Translation Module converts τ LTL formula formula provides formal representation task temporal logical constraints Verification Module lines uses formula analyze action plan sliding window approach identifying correcting
types issues incorrectly positioned actions missing prerequisites Fig proposed LLM based pre execution validation framework system processes natural language plan main stages translation formal temporal logic representations LTL b context common sense enhancement redundant steps Consider following example plan making tea check timer heat water prepare cup pour tea add sugar stir tea pour tea serve VerifyLLM system identify issues plan Redundancy Action pour tea duplicates action violating non redundancy principle Missing prerequisite action adding tea bag tea leaves pouring necessary prerequisite Position error preparation sequence atic need add tea pouring water verification VerifyLLM produce corrected plan check timer heat
water prepare cup add tea bag pour hot water add sugar stir tea serve example demonstrates VerifyLLM identifies corrects logical inconsistencies robot action plans leveraging LLM reasoning capabilities guided formal LTL constraints Translation LTL Translation Module converts task descriptions LTL formulas capture temporal dependencies logical constraints module uses LLM shot ing perform translation shown lines Algorithm given task description τ module generates LTL formula ϕ following process ϕ E E ek ϕk E represents set example pairs ei task description ϕi corresponding LTL formula prompt structure designed guide LLM extracting key propositions temporal relationships task description simplified tea making example
Heat water add tea serve shown Fig module generate formula ϕ water tea verify formula correctness Spot library converts LTL formulas automata syntax validation errors detected module employs reprompting strategy refine formula times Verification LLM verification Module detailed lines rithm analyzes action sequences sliding window approach window size w typically set based empirical findings Table IV examines consecutive action subsequences original plan π action ai module examines surrounding context w ai w w w represent w previous upcoming actions window lines Algorithm context includes relevant atomic propositions extracted LTL formula ϕ line Algorithm example Fig demonstrates system analyzes action sequences
making scenario LLM identifies issues incorrect action ordering redundant steps analysis evaluates key aspects position optimality ai appears correct sequence necessity ai essential task completion compatibility ai aligns propositions analysis module generates verification decision lines Algorithm decision action remains unchanged remove augment decisions module performs corresponding modification plan lines Algorithm Prompt Engineering Plan Optimization verification process relies structured prompts main components shown Fig input context provides task description atomic propositions derived LTL formulas action window previous actions current action actions sliding window Fig Prompt guiding LLM based reasoning plan validation analysis criteria guide LLM systematic ification position correctness prerequisite completeness
action necessity tea example system detect missing prerequisite adding tea pouring water identify redundant second pour tea action suggest reordering steps logical sequence prompt enforces tured JSON response containing verification decisions detailed reasoning suggested modifications illustrated Response Format component Fig Based analysis system employs tions correct invalid plans removal redundant actions line Algorithm π ai aj addition necessary preconditions line anew ai anew reordering optimize sequences line Optimization continues iteratively plan end lines Algorithm system maintains interpretability preserving original intent ensuring executability modifications VI EXPERIMENTAL SETUP RESULTS experimental evaluation designed address key aspects VerifyLLM framework tifying common error patterns LLM
generated plans measuring impact verification approach plan quality analyzing contribution individual components ablation studies experiments VirtualHome dataset ZeroShot planner VirtualHome comprehensive benchmark simulates household activities realistic virtual environment provides diverse set natural language task instructions corresponding action sequences everyday tasks cooking cleaning organizing dataset richly annotated details object interactions action dependencies making ideal testbed evaluating plan verification optimization methods Prior evaluation preprocess action sequences ing actions converting lowercase removing prepositions ensure consistent comparison metrics comprehensive evaluation approach allows assess effectiveness approach identify areas improvement Evaluation Metrics evaluation metrics plan quality focus overall sequence similarity specific error types key metrics LCS
Similarity LCS metric quantifies overall similarity original optimized plan sequences defined LCS denotes length longest mon subsequence original plan timized plan value indicates identical sequences indicates commonality addition introduce error metrics detailed analysis plan quality Missing Actions counts number required actions present reference plan absent generated plan Formally Aref set normalized actions reference plan Agen set generated plan number Missing Actions given Missing Actions Extra Actions measures number unnecessary redundant actions included generated plan appear reference plan defined Extra Actions Order Errors quantify discrepancies sequential ordering actions Let Lref aref aref aref n Lgen agen agen agen m ordered lists
normalized tions reference generated plans respectively number Order Errors computed Order Errors m X aref m indicator function equal condition true Results Analysis Common Types Errors Generated Plans understand typical failure modes language generated plans analyzed instructions work evaluated plans generated different models ranging parameters shown Table analysis revealed consistent patterns errors models regardless size findings identified consistent types plan errors tested models Missing Actions Eq TABLE ANALYSIS ERROR TYPES DIFFERENT LANGUAGE MODELS GENERATING TASK PLANS Model LCS Missing Extra Order Llama Llama Qwen Qwen Kanana Nano major issue models consistently omitted sary actions plan average including safety checks preparatory
steps environmental preconditions mans naturally consider second type Extra Actions Eq plans contained superfluous tions average added unnecessary complexity potentially interfere successful task completion Order Errors Eq emerged prevalent issue ordering mistakes plan average indicating significant challenge understanding temporal cies Longest Common Subsequence LCS score demonstrates substantial divergence optimal plans findings directly motivated design verification system particularly inclusion mechanisms detecting missing prerequisites identifying redundant actions optimizing action ordering Impact Verification Module verification different types baselines evaluate effectiveness approach allowed systematically assess different verification methods form implemented underlying language model compared VerifyLLM approach ternative verification methods establish comprehensive baseline Baseline Optimizer
examines adjacent action pairs suggests insertions based direct analysis utive steps straightforward prompt structure Chain Thought CoT Optimizer employs explicit step reasoning analyze transitions actions prompting LLM consider action goals identify ical gaps reason prerequisites based Optimizer extends CoT incorporating contextual information analyzing actions sliding window consider broader context suggesting modifications Table II presents comprehensive comparison approaches alongside VerifyLLM method Claude underlying models results reveal important insights different ification approaches Overall baseline methods showed limited effectiveness improving plan quality reduced percentage missing actions pared original results degraded performance key metrics baseline methods CoT Optimization achieved highest LCS score represents poor
sequence similarity TABLE II COMPREHENSIVE COMPARISON DIFFERENT VERIFICATION APPROACHES Method LCS Missing Extra Order Baseline Opt CoT Opt Window Opt VerifyLLM Llama VerifyLLM Claude TABLE III ABLATION STUDY RESULTS SHOWING IMPACT DIFFERENT COMPONENTS Configuration LCS Sim Missing Extra Order System LTL Translation LLM Verification ground truth baseline methods maintained identical missing action scores suggesting fundamental itation identifying addressing missing prerequisites baseline approaches performed particularly poorly terms extra actions ordering errors Window Optimization significantly worsened plan structure introducing excessive extra actions substantially increasing ordering errors compared original plan generation best performing baseline CoT Optimization introduced unnecessary actions ordering errors acceptable reliable
robotic task execution contrast VerifyLLM approach especially implemented Claude demonstrated substantial ments metrics Claude based implementation achieved higher LCS similarity reduced missing actions significantly decreased extra actions notably reduced ordering errors nearly compared best baseline method results clearly demonstrate combining LLMs formal logical guidance LTL significantly enhances cation quality Ablation Studies understand contribution component system conducted comprehensive ablation studies systematically removed modified key components evaluated impact system formance Table III presents results ablation experiments ablation studies provided important insights Removing LTL translation module resulted modest degradation performance LCS similarity creased slightly ordering errors increased indicates LTL module contributes capturing temporal
relationships actions overall impact relatively limited contrast eliminating LLM based verification nent dramatic effect plan quality Specifically removal led increase ordering errors TABLE IV MODULE PERFORMANCE DIFFERENT WINDOW SIZES Dataset Window Size score Zero Shot Zero Shot Zero Shot decrease LCS similarity findings confirm formal logical guidance provided LTL module robust reasoning capabilities LLM critical effective plan verification system combination components offers complementary strengths address different aspects verification process Window Size Analysis experimented ent sliding window sizes determine optimal context length processing plans Table IV presents scores achieved window sizes actions Zero Shot dataset results highlight window size yields best
score outperforming smaller larger windows suggests intermediate window size provides optimal balance capturing sufficient contextual information avoiding excessive complexity dilution relevant details window small size lacks adequate context complex dependencies larger window size introduces noise irrelevant information confuse model Based findings selected window size subsequent experiments Motivation Transitioning Larger Models experimental results strongly motivate transition smaller larger language models plan verification Smaller models struggle intricate nested vocabulary Fig required comprehensive plan analysis fail capture long range dependencies nuanced tual relationships leading suboptimal handling layered prompts contrast larger models parameters Claude variant demonstrate enhanced capabilities understanding complex temporal dependencies logical relationships
shown Table II VerifyLLM system implemented larger models achieves significantly higher LCS larity scores lower ordering errors resulting plans closely resemble optimal structure formance gain attributable larger models superior ability process understand multi level nested prompts making better suited tasks requiring detailed hierarchical plan analysis VII CONCLUSION FUTURE WORK VerifyLLM combines Large Language Models cation methods address critical plan inconsistencies position errors missing prerequisites redundant actions key contributions include framework translating instructions Linear Temporal Logic formulas textual analysis approach validating action sequences specialized datasets LTL annotations Experimental results demonstrate significant improvements plan quality verification accuracy household domains consistent error patterns
observed different language models Despite promising results acknowledge tions including challenges complex temporal cies parallel task sequences system needs better mechanisms mapping robot capabilities generated plans Future work focus handling complex temporal relationships improving parallel task processing expanding household domains industrial healthcare outdoor environments REFERENCES Taioli Rosa Castellini Natale Del Bue Farinelli Cristani Wang Mind error detection localization instruction errors vision language navigation IEEE RSJ International Conference Intelligent Robots Systems IROS IEEE pp Carta Romac Wolf Lamprier Sigaud Oudeyer Grounding large language models interactive ments online reinforcement learning International Conference Machine Learning PMLR pp Fox Long extension pddl expressing temporal
planning domains Journal artificial intelligence research vol pp Sharma Sundaralingam Blukis Paxton Hermans Torralba Andreas Fox Correcting robot plans natural language feedback arXiv preprint Ren Chalvatzaki Peters Extended tree search robot task motion planning IEEE RSJ International Conference Intelligent Robots Systems IROS IEEE pp Huang Abbeel Pathak Mordatch Language models zero shot planners Extracting actionable knowledge embodied agents International conference machine learning PMLR pp Kovalev Panov Application pretrained large language models embodied artificial intelligence Doklady Mathematics vol Suppl Springer pp Sarkisyan Korchemnyi Kovalev Panov Evaluation pretrained large language models embodied planning tasks International Conference Artificial General Intelligence Springer pp Ni
Deng Tai Zhu Xie Huang Wu Zeng Grid Scene graph based instruction driven robotic task planning IEEE RSJ International Conference Intelligent Robots Systems IROS IEEE pp Onishchenko Kovalev Panov Lookplangraph Embodied instruction following method VLM graph augmentation Workshop Reasoning Planning Large Language Models Online Available Ahn Brohan Brown Chebotar Cortes David Finn Fu Gopalakrishnan Hausman et al guage models learn explanations context arXiv preprint Grigorev Kovalev Panov Common sense plan verification large language models International Conference Hybrid Artificial Intelligence Systems pp Kress Gazit Fainekos Pappas Temporal based reactive mission motion planning IEEE transactions robotics vol pp Lahijanian Maly Fried Kavraki
Gazit Vardi Iterative temporal planning uncertain environments partial satisfaction guarantees IEEE Transactions Robotics vol pp Aksaray Vasile Belta Distributed multi robot ordination time scale temporal logic tasks IEEE International Conference Robotics Automation pp Song Wu Washington Sadler Chao Su Llm planner shot grounded planning embodied agents large language models Proceedings IEEE CVF International Conference Computer Vision pp Shridhar Thomason Gordon Bisk Han Mottaghi Zettlemoyer Fox ALFRED Benchmark Interpreting Grounded Instructions Everyday Tasks IEEE Conference Computer Vision Pattern Recognition CVPR Online Available Puig Ra Boben Li Wang Fidler Torralba Virtualhome Simulating household activities programs ceedings IEEE Conference Computer Vision Pattern Recognition
pp Cimatti Roveri Susi Tonetta informal safety critical requirements property driven formal validation Proceedings Sixth NASA Langley Formal Methods Workshop Howey Long Fox Val Automatic plan validation continuous effects mixed initiative planning pddl IEEE International Conference Tools Artificial Intelligence IEEE pp Yang Garrett Fox Lozano Kaelbling Guiding long horizon task motion planning vision language models arXiv preprint Zhang Ding Amiri Yang Kaminski Esselink Zhang Grounding classical task planners vision language models arXiv preprint Lin Agia Migimatsu Pavone Bohg natural language instructions feasible plans Autonomous Robots vol pp Li Zhu Li Yin Sun Qiu LLatrieval LLM verified retrieval verifiable generation Proceedings
Conference North American Chapter Association Computational Linguistics Human Language Technologies Volume Long Papers Duh Gomez Bethard Eds Mexico City Mexico Association Computational Linguistics June pp Online Available Liang Liu Niu Zhang Zhou Yavuz Improving llm reasoning scaling inference computation tive verification arXiv preprint Skreta Zhou Yuan Darvish Aspuru Guzik Garg Replan Robotic replanning perception language models arXiv preprint Pnueli temporal logic programs annual sium foundations computer science sfcs ieee pp Pnueli Manna temporal logic reactive rent systems Springer vol Fainekos Girard Kress Gazit Pappas Temporal logic motion planning dynamic robots Automatica vol pp Desai Ghosh Seshia Shankar Tiwari Soter runtime
assurance framework programming safe robotics systems IEEE IFIP International Conference Dependable Systems Networks pp Hsiung Mehta Chu Liu Patel Tellex Konidaris Generalizing new domains mapping natural language lifted ltl International Conference Robotics Automation ICRA IEEE pp Patel Pavlick Tellex Grounding language markovian tasks supervision task specifications Robotics Science Systems vol
cs CV Jul VOTE Vision Language Action Optimization Trajectory Ensemble Voting Juyi Amir Arash Arman Lei Guangyu Taskin Xiaomeng Weiwei Yiqian Xue David Pu Yanzhi University Inc Abstract Recent large scale Vision Language Action VLA models shown superior performance robotic manipulation tasks guided natural language generalization remains limited applied novel objects unfamiliar vironments lie outside training distribution address existing approaches integrate additional components depth estimation tion diffusion improve generalization cost adding significant computation overhead resulting low efficiency motivates exploration efficient action prediction methods independent additional high level visual representations diffusion techniques work propose VOTE efficient general framework optimization acceleration VLA
models details propose novel tokenizer free fine tuning approach parallel accurate action prediction reduces computational overhead accelerates inference speed Additionally adopt ensemble voting strategy action sampling significantly improves model performance enhances generalization Experimental results method achieves state art performance faster inference Hz throughput code available URL Introduction Building general purpose robotic policies capable handling diverse tasks embodiments world interactions central challenge robotics research Recent studies leverage Vision Language Action VLA models problem solving demonstrating excellent accuracy robotic simulation benchmarks VLA models enable robots perform complex tasks natural language instructions achieving outstanding performance familiar objects environments training distribution VLA models mainly
developed based Vision Language Models VLMs continuous training finetuning diverse robot data success paradigm lies leveraging generalization capabilities VLMs diverse robotic manipulation tasks alongside architectural designs effectively integrate VLM backbone robot action output head Pioneering efforts integrate VLMs robotic action prediction introducing specially designed action tokenizers fine tuning robotic datasets enable language guided manipulation generalization methods limited evidenced poor performance unseen Author Preprint review robotic benchmarks involving novel objects unfamiliar environments limited generalization capability motivates efforts develop methods enhance robustness diverse objects environments CogACT designs specialized action module conditioned VLM output imports diffusion optimization CogACT demonstrates improved generalization capability scalability
practical applicability limited high computational cost training inference action diffusion hand SpatialVLA attributes poor generalization traditional VLA models insufficient access high level visual cues structure depth information arguing lack information limits generalization performance address SpatialVLA extends VLA architecture incorporating additional visual modules position encoding conducts large scale training cross embodiment dataset yielding superior evaluation results reliance additional visual features introduces significant pre processing overhead increased number visual tokens results longer input sequences impacting inference speed high training cost diverse robotic datasets combined added latency action sampling limits scalability practical deployment VLA models real world scenarios motivates investigate fast acting sampling
methods reduce training overhead work propose VOTE shown Figure lightweight VLA model leveraging ensemble voting strategy optimized trajectory facilitate higher throughput faster inference action sampling deliberately exclude additional visual modules extra visual information diffusion based techniques model built solely VLM backbone propose remove action tokenizer instead introduce direct action prediction head end end action generation detail introduce special token ACT represent entire action chunks significantly reducing number generated tokens strategy avoids multiple sequential decoder passes tokenization greatly enhancing efficiency Consequently enables faster inference substantially reduces training cost making rapid fine tuning practical fewer required input output tokens simpler decoding processes
propose novel action sampling technique ensemble voting improve model performance test time Specifically construct action sampling committee incorporating actions predicted previous steps determine current action based voting mechanism weighted accumulated tickets prior predictions sampling technique mitigates errors encountered VLA models relying solely recent multimodal inputs reducing likelihood incorrect action predictions Experimental results demonstrate method achieves state art performance excellent generalization delivering higher throughput faster inference improves average success rates OpenVLA LIBERO task suites surpass average success rate CogACT SimplerEnv WidowX Robot accelerates action generation throughput edge device NVIDIA Jetson Orin contributions summarized follows propose VOTE lightweight VLA model eliminates need
action tokenizer significantly reduces training cost achieving improved generalization capabilities minimal training data VOTE enables rapid adaptation new tasks embodiments propose novel action sampling technique constructs action sampling committee action selection enabling implicit error correction action prediction Experimental results method achieves high success rate introducing lower training costs achieving higher throughput Related Work Vision Language Action Models Bridging gap seeing understanding ing Vision Language Action VLA models represent significant leap robotics object manipulation VLMs excel visual language understanding ently capable generating actions robotic embodiments recently ies present new ways building general robot policies fine tuning pretrained VLMs robot data offering
ability directly generate robot actions X pioneering model proposes VLA model pretrained Open X Embodiment OXE dataset discretized actions OpenVLA proposes fine tune prismatic VLM OXE dataset CogACT employs diffusion transformer based action Image Camera Language Instruction Vision Encoder Tokenize LLM Parallel Decoding Hidden States n tokens Action Head Current Past Committee Selected Action Vote Embedding Figure VOTE pipeline adopt parallel decoding following n actions adopt ensemble voting strategy accurate current action prediction module enhance generalization adaptability robotic tasks fine tunes PaliGemma VLM introduces novel flow matching action head enables zero shot fine tuned robotic control diverse manipulation tasks SpatialVLA
introduces Position Encoding inject information input observations VLA representing spatial robot movement actions Adaptive Action Grids RoboVLM systematically forms VLMs VLAs exploring key design choices backbone selection policy architecture cross embodiment data integration Acceleration Vision Language Action Models significant inference latency intensive computations limits VLA models wide deployments popular edge devices real world robotic embodiments real time responsiveness critical developing acceleration techniques essential advancing field innovative approaches developed VLA models DeeR VLA introduces dynamic early exit framework backbone VLA models enabling model adaptively determine computations required based task complexity VLA Cache presents token caching mechanism adaptively identify reuse unchanged visual
tokens sequential inputs reduce redundant computations TinyVLA FAST focus training smaller models scratch applying new tokenization schemes enhance training time VLA models Recently optimized fine tuning recipe presented OpenVLA OFT accelerate inference speed integrating parallel decoding action chunking continuous action representation Overhead Analysis Latency Profiling primary computational overhead current VLA models lies VLM backbone VLA architecture latency profile SpatialVLA CogACT OpenVLA left Figure decoding VLM dominates latency action prediction particular CogACT suffers significantly higher latency diffusion process action decoding VLM output SpatialVLA relies multimodal high level visual representations information results feeding larger number input tokens VLM significantly increasing inference latency
compared methods additional visual inputs high latency VLM inference action decoding motivates exploration optimized action prediction method VLA models avoids reliance diffusion additional visual inputs Training Cost VLA models normally adopt finetuning data new tasks embodiments improve performance new environments plot training cost function number trajectories SpatialVLA CogACT OpenVLA right Figure observe existing methods rely large scale pretraining data adapt VLM backbones robotic action prediction tasks Furthermore methods additional data Fractal fine tuning improves generalization capabilities SpatialVLA CogACT require significantly additional data OpenVLA VLM Prefill VLM decode Action head Diffusion Total Latency ms SpatialVLA CogACT OpenVLA Fractal Bridge Pretrain Trajectories
k SpatialVLA CogACT OpenVLA Figure Latency training costs SpatialVLA CogACT OpenVLA incorporation extra modules VLA model architectures increased data requirements generalization leads substantially higher computational costs fine tuning motivating development new VLA models robotic action prediction minimizing training overhead enabling rapid adaptation new tasks embodiments limited data Method section briefly provide preliminaries model architecture problem statement highlighting challenges limitations previous VLA methods Section Section elaborates innovative training method detailing introduction special token ACT optimizes action prediction accuracy computational efficiency Section introduce novel vote based adaptive action ensemble strategy designed enhance stability robustness action execution dynamically selecting relevant actions based cosine
similarity Preliminaries Problem statement model generates action based image language instruction action prediction time step t utilize model π predict temporal action sequence executing desired task π l describe robot actions different control modes end effectors Following strategy described previous work use degrees freedom DoF express end effector pose robot arm g relative translation offsets end effector denote rotation changes g indicates gripper open close state action space enables continuous control robot arm motion end effector behavior Training Framework Overview training introduce special token ACT tokenizer LLM explicitly signal action prediction task Specifically append ACT token end language instruction sequence
target token label denoted ytoken LLM performs single forward pass generate single token final layer hidden state corresponding ACT passed action head predict continuous action values ˆa Different original OpenVLA generate large number tokens multiple actions train model output single special token ACT reducing output token number nificantly acceleration Furthermore instead employing tokenizer obtain action tokens OpenVLA SpatialVLA methods directly utilize action head map hidden states special token ACT normalized continuous actions enabling efficient parallel computation specify details training framework Action Generation Given language instruction l corresponding image model generates multiple consecutive action predictions detailed process discussed input data processed
VLA model obtain hidden states h h B batch size L sequence length H hidden dimension hidden states corresponding special action token ACT extracted h ACT ACT h ACT Note model needs generate single token ACT instead multiple tokens multi dimensional actions need convert hidden states ACT actual action predictions achieved action head Specifically obtained hidden states h ACT passed MLP Action Head predict action chunk multiple consecutive actions ˆa ACT ˆa N chunk size number actions chunk dimensionality action MLP module architecture following h ACT ReLU ReLU ReLU ˆa ReLU actions obtained MLP action head efficient parallel computing tokenizer
computation intensive decoding traditional VLA models Training Objectives training objective incorporates token level action level pervision use loss measure discrepancy predicted actions ground truth actions denoted Laction losses combined weighted total loss balances semantic understanding language modeling accurate action generation Ltotal λtokenLtoken λactionLaction Laction represents loss comparing predicted actions ˆa ground truth actions action dimensions Ltoken cross entropy loss calculated based prediction ACT token instruction tokens Specifically Ltoken ytoken Laction BNA B X N X n ˆytoken predicted token distribution ytoken ground truth token label ˆa denote predicted ground truth action vectors respectively Advantages Training Inference method condenses entire action
chunk compact high level representation single ACT token hidden states token passed action head directly predict action chunks significantly reduces number tokens required leading improved efficiency training inference Specifically chunk size N time steps N consecutive action predictions action dimensionality D method generates hidden states single token instead ND hidden states OpenVLA OFT Furthermore generating actions hidden states action Selected Ensembled Output Vote Committee Input spoon towel Image Image Image Image Figure Vote Action Ensemble Illustration action ensemble strategy K historical action predictions example Historical predictions current prediction form voting committee jointly determine final action execute head require single forward
pass instead ND sequential decoder forward passes OpenVLA addition adopting method significantly reduced output token number need padding ND action embeddings input OpenVLA OFT training Robotic model normally requires fine tuning new data generalize new objects environments training cost traditional VLAs typically high method quickly fine tune satisfied model training cost reduced number training input output tokens faster decoding Ensemble Voting inference VLA model predicts sequence actions multiple time steps Typically robots directly execute action current time step based current observation discarding historical action predictions previous time steps approach fails fully utilize historical visual information model predictions leading stable trajectory
potential performance degradation address degradation performance propose voting based adaptive ensemble strategy action aggregation selects frequent prediction higher chance correct prediction list adjacent time steps Specifically given observation ot let denote predicted action current time step model provides list consecutive action predictions time step current time step t past action predictions previous time steps available represented H ensemble action ˆat executed time step t computed averaging selected actions based cosine similarity current prediction ˆat X k X k M τ k K N k K cosine similarity τ threshold empirically set number elements set shown compute similarity prior action current
newest prediction Based similarities action set H split subsets M high similarity N low similarity Following voting rule select set votes compute average actions selected subset final action time step Advantages voting based ensemble method adaptive higher chance correct Unlike straightforward average static weighted aggregation method method adaptive automatically select voted actions likely correct ii naive average method compute average actions H straightforward incorrect predictions considerations performance degradations Different average method voting ensemble effectively filters unreasonable inconsistent predictions enhancing reliability robustness final aggregated action iii method pays attention gives credit current newest action prediction similarities computed reference current action definite
vote high similarity reasonable current observation provides critical real time information iv experiments demonstrate method outperform straightforward average static weighted aggregation method Experimental Results Experimental Setup Baselines evaluate model LIBERO SimplerEnv simulation benchmarks comprise diverse set robotic manipulation tasks simulated environments simulated evaluations conducted NVIDIA RTX GPUs fine tune OpenVLA model AdamW learning rate Fine tuning employs Low Rank Adaptation LoRA rank r α Appendix details Training Details LIBERO LIBERO train GPUs global batch size default model finetuned output token ACT chunk size action chunk size predict ACT tokens token decodes actions Training Details SimplerEnv WidowX robot simulations SimplerEnv fine
tune K trajectories dataset global batch size K steps NVIDIA GPUs action chunk size N ensemble horizon K including current prediction set consistent OpenVLA OFT configuration Google robot simulations SimplerEnv fine tune K trajectories Fractal dataset global batch size K steps NVIDIA GPUs action chunk size N ensemble horizon K set consistent SpatialVLA Baselines compare model state art manipulation policies including X X Octo OpenVLA HPT TraceVLA RoboVLM Dita X X Octo OpenVLA HPT TraceVLA Dita RoboVLM trained mixtures OXE dataset SpatialVLA pre trained dataset mixture consisting subset OXE T finetuned Fractal dataset SimplerEnv evaluation Evaluation Results LIBERO evaluation results
LIBERO shown Table observed method chunk size performs best sub tasks LIBERO benchmark results demonstrate method improves success rate Evaluation Results SimplerEnv SimplerEnv Evaluation Table summarizes results different manipulation policies WidowX setup SimplerEnv task repeats trails SimplerEnv simulation method baselines predict action chunk image instead executing entire sequence action chunks prediction model surpasses state art methods CogACT SpatialVLA average success rate report results Google Robot SimplerEnv Table Appendix Table demonstrates latency speedup SimplerEnv Compared frameworks higher latency method achieves speedup OpenVLA Table LIBERO benchmark results Success rates SR LIBERO benchmark task suites model achieves highest average SR Method Spatial SR
Object SR Goal SR Long SR Average OpenVLA Diffusion Policy Octo TraceVLA Dita SpatialVLA OpenVLA OFT Table Evaluation results WidowX robot SimplerEnv Visual Matching setting Stack Block refers Stack Green Block Yellow Block task Eggplant Eggplant Yellow Basket task Carrot Carrot Plate task Spoon Spoon Towel task zero shot fine tuning results denote performance models pretrained OXE dataset models fine tuned BridgeData respectively Latency tested GPU Method Stack Avg Latency Speed Spoon Carrot Block Eggplant X Octo Base Octo Small OpenVLA RoboVLM zero shot RoboVLM fine tuned SpatialVLA zero shot SpatialVLA fine tuned CogACT Cross Platform Inference Evaluation investigate efficiency
VOTE measured average latency time generate action chunk throughput number actions generated second querying model times distinct platforms query processes image sample language instruction action robot pick cup test inference latency GPU shown Table VOTE achieves throughput SpatialVLA despite larger LLM model versus SpatialVLA chunk VOTE deliver speed compared OpenVLA outperforming baselines difference speed compared Table arises SimplerEnv action ensemble predicts action chunk timestep predicting action chunk actions chunk executed modern edge computing platforms NVIDIA AGX Orin preferred real time robotic control enabling real time robot inference low latency platforms struggle faced heavy demands VLA models limited heterogeneous computing resources
assess performance edge platform compare proposed approach existing methods OpenVLA SpatialVLA CogACT OFT shown Table VOTE chunk size achieves Hz throughput speedup OpenVLA imposing negligible memory overhead compared memory cost OpenVLA OFT CogACT fails execute Memory OOM results highlight superior speed efficiency making approach suited edge deployment Orin specifications found Appendix Table Cross Platform Inference Evaluation Peak VRAM represents maximum GPU memory inference Speedup Memory Efficiency reported relative OpenVLA baseline Note higher Memory Efficiency value indicates higher memory usage Model Chunk Size Platform Latency ms Peak VRAM GB Throughput Speed Memory OpenVLA SpatialVLA OpenVLA OFT CogACT OpenVLA Orin SpatialVLA Orin
OpenVLA OFT Orin CogACT Orin OOM Orin Orin Orin Throughput Hz Orin Speed Success rate Action Action Figure Latency speedup Orin token indicates generating chunk actions single ACT token tokens indicates generating ACT tokens responsible chunk actions resulting total chunk size success rate averaged LIBERO tasks evaluated GPU Ablation Study perform comprehensive study experiment different chunk sizes different numbers ACT tokens Specifically generate ACT tokens chunk size total chunk size refer tokens variant shown Figure default setting ACT token chunk size leads slightly higher accuracy compared tokens variant observe larger chunk sizes typically lead higher throughput task success rates reported
Table Appendix Section present action ensemble strategy termed Vote Ensemble formulated Eq ablate effects Vote Ensemble Figure observed Vote Ensemble effectively improve evaluation results compared case Vote Ensemble executes action chunk predictions directly Conclusion presented lightweight VLA model enhances efficiency predicting actions hidden latent space approach leverages novel tokenizer free training methodology simultaneously predicts multiple actions significantly reducing computational requirements training inference Additionally method maintains compatibility emerging powerful Vision Language Model VLM backbones Furthermore propose straightforward effective action ensemble algorithm optimizes action sampling Extensive experimental results confirm GR VM GR VA WR VM Success Ensemble Vote Ensemble Figure Ablation study
proposed VOTE action ensemble strategy SimplerEnv lation GR denotes SimplerEnv Google Robot WR denotes WidowX Robot VM denotes SimplerEnv Visual Matching VA denotes visual variant model achieves fastest training inference speeds exhibiting exceptional generative performance References Lucas Beyer Andreas Steiner André Susano Pinto Alexander Kolesnikov Xiao Wang Daniel Salz Maxim Neumann Ibrahim Alabdulmohsin Michael Tschannen Emanuele Bugliarello et al Paligemma versatile vlm transfer arXiv preprint Kevin Black Noah Brown Danny Driess Adnan Esmail Michael Equi Chelsea Finn Niccolo Fusai Lachy Groom Karol Hausman Brian Ichter et al vision language action flow model general robot control arXiv preprint Anthony Brohan Noah
Brown Justice Carbajal Yevgen Chebotar Joseph Dabis Chelsea Finn Keerthana Gopalakrishnan Karol Hausman Alex Herzog Jasmine Hsu et al Robotics transformer real world control scale arXiv preprint Anthony Brohan Noah Brown Justice Carbajal Yevgen Chebotar Xi Chen Krzysztof Choromanski Tianli Ding Danny Driess Avinava Dubey Chelsea Finn Pete Florence Chuyuan Fu Montse Gonzalez Arenas Keerthana Gopalakrishnan Kehang Han Karol Hausman Alexander Herzog Jasmine Hsu Brian Ichter Alex Irpan Nikhil Joshi Ryan Julian Dmitry Kalashnikov Yuheng Kuang Isabel Leal Lisa Lee Tsang Wei Edward Lee Sergey Levine Yao Lu Henryk Michalewski Igor Mordatch Karl Pertsch Kanishka Rao Krista Reymann Michael Ryoo
Grecia Salazar Pannag Sanketi Pierre Sermanet Jaspiar Singh Anikait Singh Radu Soricut Huong Tran Vincent Vanhoucke Quan Vuong Ayzaan Wahid Stefan Welker Paul Wohlhart Jialin Wu Fei Xia Ted Xiao Peng Xu Sichun Xu Tianhe Yu Brianna Zitkovich Vision language action models transfer web knowledge robotic control URL Xi Chen Josip Djolonga Piotr Padlewski Basil Mustafa Soravit Changpinyo Jialin Wu Carlos Riquelme Ruiz Sebastian Goodman Xiao Wang Yi Tay et al Pali x scaling multilingual vision language model arXiv preprint Danny Driess Fei Xia Mehdi SM Sajjadi Corey Lynch Aakanksha Chowdhery Ayzaan Wahid Jonathan Tompson Quan Vuong Tianhe Yu Wenlong
Huang et al Palm e embodied multimodal language model Hao Shu Fang Hongjie Fang Zhenyu Tang Jirong Liu Chenxi Wang Junbo Wang Haoyi Zhu Cewu Lu t comprehensive robotic dataset learning diverse skills shot IEEE International Conference Robotics Automation ICRA pages IEEE Zhi Hou Tianyi Zhang Yuwen Xiong Haonan Duan Hengjun Pu Ronglei Tong Chengyang Zhao Xizhou Zhu Yu Qiao Jifeng Dai et al Dita Scaling diffusion transformer generalist vision language action policy arXiv preprint Edward J Hu Yelong Shen Phillip Wallis Zeyuan Allen Zhu Yuanzhi Li Shean Wang Lu Wang Weizhu Chen et al Lora Low rank adaptation large
language models ICLR Huang Huang Fangchen Liu Letian Fu Tingfan Wu Mustafa Mukadam Jitendra Malik Ken Goldberg Pieter Abbeel Otter vision language action model text aware visual feature extraction arXiv preprint Siddharth Karamcheti Suraj Nair Ashwin Balakrishna Percy Liang Thomas Kollar Dorsa Sadigh Prismatic vlms Investigating design space visually conditioned language models International Conference Machine Learning Moo Jin Kim Karl Pertsch Siddharth Karamcheti Ted Xiao Ashwin Balakrishna Suraj Nair Rafael Rafailov Ethan Foster Grace Lam Pannag Sanketi et al Openvla open source vision language action model arXiv preprint Moo Jin Kim Chelsea Finn Percy Liang Fine tuning vision language action
models Optimizing speed success arXiv preprint S Karumbunathan Leela NVIDIA Jetson AGX Orin Series Technical Brief giant leap forward robotics edge AI applications URL Solutions jetson orin nvidia jetson agx orin technical Qixiu Li Yaobo Liang Zeyu Wang Lin Luo Xi Chen Mozheng Liao Fangyun Wei Yu Deng Sicheng Xu Yizhong Zhang et al Cogact foundational vision language action model synergizing cognition action robotic manipulation arXiv preprint Xinghang Li Minghuan Liu Hanbo Zhang Cunjun Yu Jie Xu Hongtao Wu Chilam Cheang Ya Jing Weinan Zhang Huaping Liu Hang Li Tao Kong Vision language foundation models effective robot imitators arXiv preprint
Xinghang Li Peiyan Li Minghuan Liu Dong Wang Jirong Liu Bingyi Kang Xiao Ma Tao Kong Hanbo Zhang Huaping Liu generalist robot policies matters building vision action models arXiv preprint Xuanlin Li Kyle Hsu Jiayuan Gu Karl Pertsch Oier Mees Homer Rich Walke Chuyuan Fu Ishikaa nawat Isabel Sieh Sean Kirmani Sergey Levine Jiajun Wu Chelsea Finn Hao Su Quan Vuong Ted Xiao Evaluating real world robot manipulation policies simulation arXiv preprint Yi Li Yuquan Deng Jesse Zhang Joel Jang Marius Memmel Raymond Yu Caelan Reed Garrett Fabio Ramos Dieter Fox Anqi Li et al Hamster Hierarchical action models open
world robot manipulation arXiv preprint Bo Liu Yifeng Zhu Chongkai Gao Yihao Feng Qiang Liu Yuke Zhu Peter Stone Libero Benchmarking knowledge transfer lifelong robot learning Advances Neural Information Processing Systems Abby Abdul Rehman Abhiram Maddukuri Abhishek Gupta Abhishek Padalkar Abraham Lee Acorn Pooley Agrim Gupta Ajay Mandlekar Ajinkya Jain et al Open x embodiment Robotic learning datasets rt x models Open x embodiment collaboration IEEE International Conference Robotics Automation ICRA pages IEEE Karl Pertsch Kyle Stachowicz Brian Ichter Danny Driess Suraj Nair Quan Vuong Oier Mees Chelsea Finn Sergey Levine Fast Efficient action tokenization vision language action models arXiv
preprint Delin Qu Haoming Song Qizhi Chen Yuanqi Yao Xinyi Ye Yan Ding Zhigang Wang JiaYuan Gu Bin Zhao Dong Wang et al Spatialvla Exploring spatial representations visual language action model arXiv preprint Xuan Shen Chenxia Han Yufa Zhou Yanyue Xie Yifan Gong Quanyi Wang Yiwei Wang Yanzhi Wang Pu Zhao Jiuxiang Gu Draftattention Fast video diffusion low resolution attention guidance arXiv preprint Xuan Shen Weize Ma Jing Liu et al Quartdepth Post training quantization real time depth estimation edge CVPR Xuan Shen Weize Ma Yufa Zhou Enhao Tang Yanyue Xie Zhengang Li Yifan Gong Quanyi Wang Henghui Ding Yiwei
Wang et al Fastcar Cache attentive replay fast auto regressive video generation edge arXiv preprint Xuan Shen Zhao Song Yufa Zhou Bo Chen Yanyu Li Yifan Gong Kai Zhang Hao Tan Jason Kuen Henghui Ding et al Lazydit Lazy learning acceleration diffusion transformers AAAI Xuan Shen Zhao Song Yufa Zhou Bo Chen Jing Liu Ruiyi Zhang Ryan Rossi Hao Tan Tong Yu Xiang Chen et al Numerical pruning efficient autoregressive models AAAI Xuan Shen Yizhou Wang Xiangxi Shi Yanzhi Wang Pu Zhao Jiuxiang Gu Efficient reasoning hidden thinking arXiv preprint Xuan Shen Hangyu Zheng Yifan Gong Zhenglun Kong Changdi Yang
Zheng Zhan Yushu Wu Xue Lin Yanzhi Wang Pu Zhao Wei Niu Sparse learning state space models mobile Thirteenth International Conference Learning Representations URL id Lucy Xiaoyang Shi Brian Ichter Michael Equi Liyiming Ke Karl Pertsch Quan Vuong James Tanner Anna Walling Haohuan Wang Niccolo Fusai et al Hi robot Open ended instruction following hierarchical vision language action models arXiv preprint Octo Model Team Dibya Ghosh Homer Walke Karl Pertsch Kevin Black Oier Mees Sudeep Dasari Joey Hejna Tobias Kreiman Charles Xu et al Octo open source generalist robot policy arXiv preprint Homer Rich Walke Kevin Black Tony Z Zhao
Quan Vuong Chongyi Zheng Philippe Hansen Estruch Andre Wang Vivek Myers Moo Jin Kim Max Du et al Bridgedata dataset robot learning scale Conference Robot Learning pages PMLR Lirui Wang Xinlei Chen Jialiang Zhao Kaiming Scaling proprioceptive visual learning heterogeneous pre trained transformers Advances Neural Information Processing Systems Junjie Wen Yichen Zhu Jinming Li Minjie Zhu Kun Wu Zhiyuan Xu Ning Liu Ran Cheng Chaomin Shen Yaxin Peng Feifei Feng Jian Tang Tinyvla fast data efficient vision language action models robotic manipulation URL Siyu Xu Yunke Wang Chenghao Xia Dihao Zhu Tao Huang Chang Xu Vla cache efficient vision language
action model adaptive token caching robotic manipulation arXiv preprint Changdi Yang Pu Zhao Yanyu Li et al Pruning parameterization bi level optimization efficient semantic segmentation edge CVPR Yang Yue Yulin Wang Bingyi Kang Yizeng Han Shenzhi Wang Shiji Song Jiashi Feng Gao Huang Deer vla Dynamic inference multimodal large language models efficient robot execution Advances Neural Information Processing Systems Zheng Zhan Zhenglun Kong Yifan Gong Yushu Wu Zichong Meng Hangyu Zheng Xuan Shen Stratis Ioannidis Wei Niu Pu Zhao Yanzhi Wang Exploring token pruning vision state space models NeurIPS Zheng Zhan Yushu Wu Yifan Gong et al Fast memory efficient
video diffusion streamlined inference NeurIPS URL Zheng Zhan Yushu Wu Zhenglun Kong Changdi Yang Yifan Gong Xuan Shen Xue Lin Pu Zhao Yanzhi Wang Rethinking token reduction state space models EMNLP pages Miami Florida USA nov ACL URL Pu Zhao Fei Sun Xuan Shen Pinrui Yu Zhenglun Kong Yanzhi Wang Xue Lin Pruning foundation models high accuracy retraining Findings EMNLP pages ACL November doi URL findings Ruijie Zheng Yongyuan Liang Shuaiyi Huang Jianfeng Gao Hal Daumé III Andrey Kolobov Furong Huang Jianwei Yang Tracevla Visual trace prompting enhances spatial temporal awareness generalist robotic policies arXiv preprint Minjie Zhu Yichen Zhu
Jinming Li Zhongyi Zhou Junjie Wen Xiaoyu Liu Chaomin Shen Yaxin Peng Feifei Feng Objectvla End end open world object manipulation demonstration arXiv preprint Brianna Zitkovich Tianhe Yu Sichun Xu Peng Xu Ted Xiao Fei Xia Jialin Wu Paul Wohlhart Stefan Welker Ayzaan Wahid et al Vision language action models transfer web knowledge robotic control Conference Robot Learning pages PMLR Appendix Environments Software Environment model implemented PyTorch TorchVision Transformers SimplerEnv benchmark key software dependencies follows TensorFlow NumPy Edge Computing Environment specifications shown Table B Training Details LIBERO Training runs NVIDIA RTX GPUs GB VRAM GB system RAM Training converges hours
Fractal Training runs NVIDIA GPUs GB VRAM GB RAM set shuffle buffer K samples Training converges hours Bridge Training runs NVIDIA GPUs GB VRAM GB RAM set shuffle buffer K samples Training converges hours total experiment hours x Hyperparameter token loss require model correctly predict ACT token token loss weight small model struggles recognize attend ACT weight large loss overly dominated token prediction undermining accuracy convergence action prediction set action loss weight token loss weight libero benchmark train mean loss predicted ground truth normalized actions loss facilitate faster convergence follow learning rate schedule OpenVLA OFT decaying learning rate achieve k
steps models converge k steps learning rate decay applied libero long tasks chunk size detailed parameters libero benchmark shown Table compare OpenVLA OFT Table Table NVIDIA AGX Orin Specifications GPU NVIDIA Ampere architecture GPCs TPCs SMs CUDA cores SM Tensor Cores KB cache SM MB cache CPU core Arm Cortex bit organized clusters KB core MB core MB cluster MB system cache Memory Unified GB bit GB s bandwidth Storage TB NVMe SSD GB eMMC Power Computation Saving OpenVLA parameter manipulation policy created fine tuning Prismatic VLM M episodes Open X Embodiment dataset OpenVLA original training lation adopts autoregressive strategy
predict DoF discrete robot action tokens timestep Specifically generates tokens auto regressively action token corresponding single result chunk size K timesteps action dimensionality D original OpenVLA formulation requires KD sequential decoder forward passes K fold increase decoding latency makes action chunking impractical high frequency robotic control address latency overhead OpenVLA OFT proposes parallel decoding approach generates tokens simultaneously tokens processed language model produce hidden states subsequently passed MLP action head predict continuous actions approach yields KD hidden states receives action embeddings input training parallel decoding theoretically expressive autoregressive methods work demonstrate generating KD hidden states unnecessary effective chunked action prediction
replace original Action Head concatenates action token representations passes dimension hidden size action dimension MLP head resulting excessive model parameters Instead action head processes action token independently MLP input dimension d produces multiple actions token design reduces parameter count approximately thirds M M preserving ability model sequential actions improvement reduces total parameter count M M shown Table achieving reduction approximately output dimension increases predicting multiple actions token minor impact total parameter count contrast dominant contributor parameter size input dimension design significantly reduces distinction OpenVLA OFT concatenating action tokens input dimension output dimension majority parameters come linear layer M parameters input
dimension output dimension increase x M parameters result compact architecture action head mitigates risk overfitting reducing model capacity improves generalization compromising prediction accuracy Table Hyperparameters LIBERO experiments Hyperparameter Value GPUs NVIDIA GB VRAM Learning rate LR Total batch size GPU Train steps K object K goal K spatial K long K steps Input images person camera image Input image size px Use observation history use single step inputs LoRA rank Action chunk size steps Trainable parameters M total M LoRA adapter M action head C Evaluation Details LIBERO Evaluation Detail LIBERO task suites suites evaluate model understanding spatial relationships LIBERO
Spatial object types LIBERO Object task oriented behaviors LIBERO Goal ability generalize long horizon tasks diverse objects layouts goals Long experiment conduct evaluations task suites containing tasks task repeated times resulting total trials suite LIBERO Performance comparison different tasks model variants reported Table Table OpenVLA OFT hyperparameters Hyperparameter Value GPUs NVIDIA GB VRAM Learning rate LR Total batch size GPU Train steps K LIBERO Spatial K steps K LIBERO Object K steps K LIBERO Goal K LIBERO Long K steps Input images person camera image Input image size px Use observation history use single step inputs LoRA rank Action chunk
size steps Trainable parameters M total M LoRA adapter M action head Table LIBERO Performance comparison different tasks model variants Method Spatial SR Object SR Goal SR Long SR Average chunk chunk tokens compare training efficiency OpenVLA OFT OpenVLA OFT finetuned VLA model k training steps GPUs GPU batch size training speeds vary different hardware configurations number training samples offers fair basis comparison define total training effort product training steps global batch size million samples uses training effort easy tasks like goal object results shown Table Table Relative training effort compared OpenVLA OFT defined M training samples Chunk Size Goal
Object Long SimplerEnv Evaluation Detail SimplerEnv Simulation Environment offers evaluation settings Visual Matching closely replicates real world tasks minimizing discrepancies simulated real environments Variant Aggregations introduces variations Visual Matching modifying elements background lighting distractors table texture Models evaluated k steps action loss fully indicative performance report results Google Robot SimplerEnv Table Task Definition task variants provided SimplerEnv evaluation Google robot setup included following tasks Pick coke close Open Close middle drawer Open drawer place apple drawer WidowX robot setup included Place spoon towel Place carrot plate Stack green block yellow block Place eggplant yellow basket Google robot setup provided evaluation
methods Visual Matching VM Variant Aggregations VA WidowX robot setup Visual Matching VM evaluation provided Table Comparison approach existing VLA models Google robot tasks SimplerEnv settings OpenVLA success rate reported CogACT shot fine tuning results denote performance OXE pre trained models Fractal fine tuned models respectively Google Robot Method Pick Open Close Avg Latency Speedup Coke Near Drawer ms SimplerEnv Visual Matching X X Octo Base OpenVLA HPT TraceVLA RoboVLM zero shot RoboVLM fine tuned Dita SpatialVLA zero shot SpatialVLA fine tuned CogACT SimplerEnv Variant Aggregation X X Octo Base OpenVLA TraceVLA RoboVLM zero shot RoboVLM fine tuned Dita SpatialVLA
zero shot SpatialVLA fine tuned CogACT Table Performance comparison SimplerEnv WidowX Robot tasks task includes separate success rates grasping placing actions Method Spoon Carrot Stack Block Eggplant Average Grasp Success Grasp Success Grasp Success Grasp Success OpenVLA OFT Table Comparison approach OPENVLA OFT models Google robot tasks SimplerEnv settings Google Robot Method Pick Open Close Avg Coke Near Drawer SimplerEnv Visual Matching OpenVLA OFT SimplerEnv Variant Aggregation OpenVLA OFT Ablation Study OpenVLA OFT SimplerEnv OpenVLA OFT report results SimplerEnv fine tune open source implementation settings NVIDIA RTX GPUs learning rate Google robot use global batch size GPU chunk size WidowX
robot use chunk size global batch size GPU Results WidowX shown Table results Google robot Table